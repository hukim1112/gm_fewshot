{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_path = os.path.abspath('.').split('jupyters')[0]\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_size = (10, 10)\n",
    "#py_func = partial(data_process, map_size = map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = np.zeros(map_size, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corr_map(mp, map_size):\n",
    "    #mp = mp.numpy()\n",
    "    correlations = np.zeros(map_size, np.float32)\n",
    "    for i in range(map_size[0]):\n",
    "        for j in range(map_size[1]):\n",
    "            if i+mp == j:\n",
    "                correlations[i,j] += 1\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "correlations = make_corr_map(0, map_size)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "correlations = make_corr_map(1, map_size)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "correlations = make_corr_map(2, map_size)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "correlations = make_corr_map(-1, map_size)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "correlations = make_corr_map(-2, map_size)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_integer():\n",
    "    while True:\n",
    "        yield tf.random.uniform(shape=[1], minval = -10, maxval=11, dtype=tf.int32, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(mp, map_size):\n",
    "    mp = mp.numpy()\n",
    "    correlations = np.zeros(map_size, np.float32)\n",
    "    for i in range(map_size[0]):\n",
    "        for j in range(map_size[1]):\n",
    "            if i+mp == j:\n",
    "                correlations[i,j] += 1\n",
    "    return mp, correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_func = partial(data_process, map_size = map_size)\n",
    "def map_function(motion_paramters):\n",
    "    return tf.py_function(py_func, [motion_paramters], [tf.float32, tf.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(gen_integer, output_types=tf.int32)\n",
    "ds = ds.map(map_function)\n",
    "ds = ds.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  2.]\n",
      " [-10.]\n",
      " [ -6.]\n",
      " [ -3.]\n",
      " [  7.]\n",
      " [  6.]\n",
      " [ -5.]\n",
      " [ -5.]\n",
      " [  3.]\n",
      " [ -1.]\n",
      " [ -2.]\n",
      " [  6.]\n",
      " [ -7.]\n",
      " [ 10.]\n",
      " [  0.]\n",
      " [ -4.]\n",
      " [  7.]\n",
      " [ -1.]\n",
      " [  2.]\n",
      " [  0.]\n",
      " [  9.]\n",
      " [ -4.]\n",
      " [  9.]\n",
      " [  1.]\n",
      " [ -6.]\n",
      " [ 10.]\n",
      " [  7.]\n",
      " [  7.]\n",
      " [ 10.]\n",
      " [ -6.]\n",
      " [  8.]\n",
      " [ -2.]\n",
      " [ -2.]\n",
      " [  0.]\n",
      " [  1.]\n",
      " [ -6.]\n",
      " [-10.]\n",
      " [  9.]\n",
      " [ -5.]\n",
      " [ -8.]\n",
      " [-10.]\n",
      " [  7.]\n",
      " [ -6.]\n",
      " [  5.]\n",
      " [  4.]\n",
      " [ -3.]\n",
      " [  7.]\n",
      " [ 10.]\n",
      " [ -8.]\n",
      " [  3.]\n",
      " [  2.]\n",
      " [ -3.]\n",
      " [ -1.]\n",
      " [ -8.]\n",
      " [ -8.]\n",
      " [ 10.]\n",
      " [ -6.]\n",
      " [  2.]\n",
      " [  2.]\n",
      " [ -3.]\n",
      " [  6.]\n",
      " [ -9.]\n",
      " [ 10.]\n",
      " [  9.]\n",
      " [  9.]\n",
      " [ -1.]\n",
      " [  0.]\n",
      " [  0.]\n",
      " [  8.]\n",
      " [ -4.]\n",
      " [ -7.]\n",
      " [ -2.]\n",
      " [ -8.]\n",
      " [  4.]\n",
      " [ -8.]\n",
      " [  9.]\n",
      " [  0.]\n",
      " [ -7.]\n",
      " [ -5.]\n",
      " [  1.]\n",
      " [  5.]\n",
      " [  0.]\n",
      " [  5.]\n",
      " [  4.]\n",
      " [ -5.]\n",
      " [ -6.]\n",
      " [ -5.]\n",
      " [  0.]\n",
      " [ -7.]\n",
      " [  7.]\n",
      " [ -5.]\n",
      " [  9.]\n",
      " [ -1.]\n",
      " [ -9.]\n",
      " [  6.]\n",
      " [  2.]\n",
      " [ -6.]\n",
      " [ -4.]\n",
      " [  9.]\n",
      " [ 10.]], shape=(100, 1), dtype=float32)\n",
      "tf.Tensor([-10.], shape=(1,), dtype=float32) tf.Tensor([10.], shape=(1,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]], shape=(100, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for mp, corr in ds.take(1):\n",
    "    print(mp)\n",
    "    print(min(mp), max(mp))\n",
    "    print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = tf.keras.layers\n",
    "#dir(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, label):\n",
    "    loss = tf.sqrt(tf.pow(pred - label, 2))\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv1d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              multiple                  93        \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  30        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  19        \n",
      "=================================================================\n",
      "Total params: 142\n",
      "Trainable params: 142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 iter loss :  5.6745405\n",
      "20 iter loss :  4.6946363\n",
      "30 iter loss :  4.963554\n",
      "40 iter loss :  4.890676\n",
      "50 iter loss :  4.3484406\n",
      "60 iter loss :  4.373718\n",
      "70 iter loss :  3.85291\n",
      "80 iter loss :  4.273892\n",
      "90 iter loss :  3.5273328\n",
      "100 iter loss :  2.7849991\n",
      "110 iter loss :  3.8273058\n",
      "120 iter loss :  3.8628597\n",
      "130 iter loss :  2.9293232\n",
      "140 iter loss :  2.8420591\n",
      "150 iter loss :  2.970322\n",
      "160 iter loss :  2.813371\n",
      "170 iter loss :  1.6468602\n",
      "180 iter loss :  3.747525\n",
      "190 iter loss :  2.090798\n",
      "200 iter loss :  2.063028\n",
      "210 iter loss :  2.351018\n",
      "220 iter loss :  3.1256166\n",
      "230 iter loss :  2.561669\n",
      "240 iter loss :  2.276454\n",
      "250 iter loss :  1.7719779\n",
      "260 iter loss :  3.067497\n",
      "270 iter loss :  2.3159254\n",
      "280 iter loss :  2.8614156\n",
      "290 iter loss :  2.3703542\n",
      "300 iter loss :  1.6972584\n",
      "310 iter loss :  2.2609735\n",
      "320 iter loss :  1.9154271\n",
      "330 iter loss :  2.1579554\n",
      "340 iter loss :  1.5670377\n",
      "350 iter loss :  1.535007\n",
      "360 iter loss :  2.3173459\n",
      "370 iter loss :  2.25002\n",
      "380 iter loss :  2.1812525\n",
      "390 iter loss :  2.1300068\n",
      "400 iter loss :  1.6035223\n",
      "410 iter loss :  1.1361405\n",
      "420 iter loss :  1.696811\n",
      "430 iter loss :  1.523417\n",
      "440 iter loss :  2.0136285\n",
      "450 iter loss :  1.6888013\n",
      "460 iter loss :  1.4170969\n",
      "470 iter loss :  1.5808014\n",
      "480 iter loss :  1.5702344\n",
      "490 iter loss :  1.208233\n",
      "500 iter loss :  1.1407824\n",
      "510 iter loss :  1.6811421\n",
      "520 iter loss :  1.5207617\n",
      "530 iter loss :  1.5921437\n",
      "540 iter loss :  1.5140129\n",
      "550 iter loss :  1.5765014\n",
      "560 iter loss :  1.3753301\n",
      "570 iter loss :  1.8003316\n",
      "580 iter loss :  0.9966071\n",
      "590 iter loss :  1.8710487\n",
      "600 iter loss :  1.9189839\n",
      "610 iter loss :  2.0293732\n",
      "620 iter loss :  1.4952631\n",
      "630 iter loss :  1.2834195\n",
      "640 iter loss :  1.5671479\n",
      "650 iter loss :  2.6082644\n",
      "660 iter loss :  1.7223947\n",
      "670 iter loss :  1.437772\n",
      "680 iter loss :  1.3476815\n",
      "690 iter loss :  0.9718612\n",
      "700 iter loss :  1.1542627\n",
      "710 iter loss :  0.9268635\n",
      "720 iter loss :  1.738267\n",
      "730 iter loss :  1.190327\n",
      "740 iter loss :  1.8679487\n",
      "750 iter loss :  1.5685952\n",
      "760 iter loss :  1.1766183\n",
      "770 iter loss :  1.629377\n",
      "780 iter loss :  1.4584798\n",
      "790 iter loss :  1.522963\n",
      "800 iter loss :  1.2636473\n",
      "810 iter loss :  1.387491\n",
      "820 iter loss :  1.8397223\n",
      "830 iter loss :  1.6844052\n",
      "840 iter loss :  1.5363839\n",
      "850 iter loss :  1.7129349\n",
      "860 iter loss :  1.3742752\n",
      "870 iter loss :  1.5336313\n",
      "880 iter loss :  1.493229\n",
      "890 iter loss :  0.9877162\n",
      "900 iter loss :  2.1308718\n",
      "910 iter loss :  2.3046007\n",
      "920 iter loss :  1.9431429\n",
      "930 iter loss :  1.1895666\n",
      "940 iter loss :  0.9859613\n",
      "950 iter loss :  1.1474056\n",
      "960 iter loss :  0.66913575\n",
      "970 iter loss :  1.2607968\n",
      "980 iter loss :  1.1125946\n",
      "990 iter loss :  1.4637539\n",
      "1000 iter loss :  1.3635372\n",
      "1010 iter loss :  1.5637102\n",
      "1020 iter loss :  1.5290393\n",
      "1030 iter loss :  1.5761348\n",
      "1040 iter loss :  1.3946887\n",
      "1050 iter loss :  0.83308434\n",
      "1060 iter loss :  1.4009248\n",
      "1070 iter loss :  1.2407252\n",
      "1080 iter loss :  0.9063178\n",
      "1090 iter loss :  1.7437615\n",
      "1100 iter loss :  0.8972752\n",
      "1110 iter loss :  1.7126878\n",
      "1120 iter loss :  0.84361476\n",
      "1130 iter loss :  1.3498845\n",
      "1140 iter loss :  1.5213802\n",
      "1150 iter loss :  1.119565\n",
      "1160 iter loss :  1.0798292\n",
      "1170 iter loss :  1.8584322\n",
      "1180 iter loss :  1.2418327\n",
      "1190 iter loss :  1.263919\n",
      "1200 iter loss :  1.0846747\n",
      "1210 iter loss :  1.0686897\n",
      "1220 iter loss :  1.1609825\n",
      "1230 iter loss :  1.0343271\n",
      "1240 iter loss :  1.2213104\n",
      "1250 iter loss :  1.702301\n",
      "1260 iter loss :  1.6886342\n",
      "1270 iter loss :  2.1888566\n",
      "1280 iter loss :  1.3229976\n",
      "1290 iter loss :  1.0804231\n",
      "1300 iter loss :  1.4677277\n",
      "1310 iter loss :  0.9628691\n",
      "1320 iter loss :  1.1423622\n",
      "1330 iter loss :  1.2292094\n",
      "1340 iter loss :  1.33941\n",
      "1350 iter loss :  0.8720195\n",
      "1360 iter loss :  1.5559169\n",
      "1370 iter loss :  1.2769539\n",
      "1380 iter loss :  1.9683816\n",
      "1390 iter loss :  0.885457\n",
      "1400 iter loss :  1.4277705\n",
      "1410 iter loss :  1.1723765\n",
      "1420 iter loss :  1.214397\n",
      "1430 iter loss :  1.313938\n",
      "1440 iter loss :  1.0306108\n",
      "1450 iter loss :  1.6490679\n",
      "1460 iter loss :  1.9214445\n",
      "1470 iter loss :  2.3313794\n",
      "1480 iter loss :  1.0397391\n",
      "1490 iter loss :  1.6642512\n",
      "1500 iter loss :  1.1965446\n",
      "1510 iter loss :  1.1193062\n",
      "1520 iter loss :  0.7185903\n",
      "1530 iter loss :  1.7652175\n",
      "1540 iter loss :  1.7366948\n",
      "1550 iter loss :  0.6513452\n",
      "1560 iter loss :  1.0042301\n",
      "1570 iter loss :  1.1254846\n",
      "1580 iter loss :  1.3174713\n",
      "1590 iter loss :  1.0329813\n",
      "1600 iter loss :  1.6099056\n",
      "1610 iter loss :  1.4434438\n",
      "1620 iter loss :  1.5189087\n",
      "1630 iter loss :  0.6463224\n",
      "1640 iter loss :  1.3230706\n",
      "1650 iter loss :  1.1446104\n",
      "1660 iter loss :  1.0245461\n",
      "1670 iter loss :  1.2706933\n",
      "1680 iter loss :  1.4861282\n",
      "1690 iter loss :  0.9264035\n",
      "1700 iter loss :  1.1215091\n",
      "1710 iter loss :  1.0784978\n",
      "1720 iter loss :  0.8843523\n",
      "1730 iter loss :  1.3469739\n",
      "1740 iter loss :  2.3141916\n",
      "1750 iter loss :  1.8920649\n",
      "1760 iter loss :  1.0416362\n",
      "1770 iter loss :  1.0210189\n",
      "1780 iter loss :  1.1245835\n",
      "1790 iter loss :  0.7764799\n",
      "1800 iter loss :  1.3539138\n",
      "1810 iter loss :  1.5387039\n",
      "1820 iter loss :  1.2635757\n",
      "1830 iter loss :  1.4917206\n",
      "1840 iter loss :  1.0905101\n",
      "1850 iter loss :  1.4353176\n",
      "1860 iter loss :  1.3944371\n",
      "1870 iter loss :  1.6345642\n",
      "1880 iter loss :  1.1792663\n",
      "1890 iter loss :  1.0527654\n",
      "1900 iter loss :  1.2098336\n",
      "1910 iter loss :  1.6059517\n",
      "1920 iter loss :  1.0902219\n",
      "1930 iter loss :  1.6446846\n",
      "1940 iter loss :  1.2828543\n",
      "1950 iter loss :  1.7812959\n",
      "1960 iter loss :  1.0548794\n",
      "1970 iter loss :  1.0883967\n",
      "1980 iter loss :  0.8400846\n",
      "1990 iter loss :  0.8164498\n",
      "2000 iter loss :  0.9461286\n",
      "2010 iter loss :  1.2196269\n",
      "2020 iter loss :  1.0562966\n",
      "2030 iter loss :  1.3795737\n",
      "2040 iter loss :  1.3167728\n",
      "2050 iter loss :  0.8924212\n",
      "2060 iter loss :  1.4064769\n",
      "2070 iter loss :  0.8301896\n",
      "2080 iter loss :  0.98825943\n",
      "2090 iter loss :  0.95767695\n",
      "2100 iter loss :  1.1625518\n",
      "2110 iter loss :  1.100292\n",
      "2120 iter loss :  1.0446811\n",
      "2130 iter loss :  1.2489086\n",
      "2140 iter loss :  1.3817573\n",
      "2150 iter loss :  1.1646452\n",
      "2160 iter loss :  1.0136834\n",
      "2170 iter loss :  1.5769784\n",
      "2180 iter loss :  1.2121558\n",
      "2190 iter loss :  1.0999722\n",
      "2200 iter loss :  0.95520484\n",
      "2210 iter loss :  1.0905488\n",
      "2220 iter loss :  1.5536239\n",
      "2230 iter loss :  1.1414807\n",
      "2240 iter loss :  1.3352057\n",
      "2250 iter loss :  0.91672456\n",
      "2260 iter loss :  0.7504029\n",
      "2270 iter loss :  1.2803172\n",
      "2280 iter loss :  1.0966501\n",
      "2290 iter loss :  1.0483073\n",
      "2300 iter loss :  1.5483527\n",
      "2310 iter loss :  1.0590546\n",
      "2320 iter loss :  1.2553403\n",
      "2330 iter loss :  1.015665\n",
      "2340 iter loss :  1.0595009\n",
      "2350 iter loss :  1.160231\n",
      "2360 iter loss :  1.2494683\n",
      "2370 iter loss :  1.1080276\n",
      "2380 iter loss :  1.467876\n",
      "2390 iter loss :  1.1515273\n",
      "2400 iter loss :  1.3779762\n",
      "2410 iter loss :  0.9735342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2420 iter loss :  1.1817261\n",
      "2430 iter loss :  1.6653708\n",
      "2440 iter loss :  0.9867492\n",
      "2450 iter loss :  1.1893284\n",
      "2460 iter loss :  1.0412598\n",
      "2470 iter loss :  1.0500995\n",
      "2480 iter loss :  0.6134098\n",
      "2490 iter loss :  1.2944516\n",
      "2500 iter loss :  1.0083756\n",
      "2510 iter loss :  1.1965901\n",
      "2520 iter loss :  1.1261816\n",
      "2530 iter loss :  1.4677978\n",
      "2540 iter loss :  1.5881397\n",
      "2550 iter loss :  1.2386605\n",
      "2560 iter loss :  1.3636844\n",
      "2570 iter loss :  0.85446286\n",
      "2580 iter loss :  1.0803494\n",
      "2590 iter loss :  1.2980953\n",
      "2600 iter loss :  1.8345194\n",
      "2610 iter loss :  1.0737702\n",
      "2620 iter loss :  1.3893772\n",
      "2630 iter loss :  1.4651\n",
      "2640 iter loss :  1.2454342\n",
      "2650 iter loss :  1.2154254\n",
      "2660 iter loss :  1.4736372\n",
      "2670 iter loss :  1.6611444\n",
      "2680 iter loss :  1.7089283\n",
      "2690 iter loss :  1.1692591\n",
      "2700 iter loss :  0.9537831\n",
      "2710 iter loss :  0.9321528\n",
      "2720 iter loss :  1.0528625\n",
      "2730 iter loss :  0.7808732\n",
      "2740 iter loss :  1.1681587\n",
      "2750 iter loss :  1.0323162\n",
      "2760 iter loss :  0.93869\n",
      "2770 iter loss :  1.4460303\n",
      "2780 iter loss :  1.0943382\n",
      "2790 iter loss :  1.5354577\n",
      "2800 iter loss :  1.0361005\n",
      "2810 iter loss :  1.3736784\n",
      "2820 iter loss :  0.99911416\n",
      "2830 iter loss :  1.4143779\n",
      "2840 iter loss :  1.6627319\n",
      "2850 iter loss :  1.218557\n",
      "2860 iter loss :  1.4307046\n",
      "2870 iter loss :  1.5285568\n",
      "2880 iter loss :  1.5042632\n",
      "2890 iter loss :  1.3989888\n",
      "2900 iter loss :  1.3518556\n",
      "2910 iter loss :  1.3260279\n",
      "2920 iter loss :  1.1878005\n",
      "2930 iter loss :  0.98813766\n",
      "2940 iter loss :  1.5921375\n",
      "2950 iter loss :  1.9305097\n",
      "2960 iter loss :  1.0320137\n",
      "2970 iter loss :  1.0883788\n",
      "2980 iter loss :  1.1754636\n",
      "2990 iter loss :  0.9634007\n",
      "3000 iter loss :  1.127714\n",
      "3010 iter loss :  1.4519017\n",
      "3020 iter loss :  1.4405824\n",
      "3030 iter loss :  0.80516136\n",
      "3040 iter loss :  1.186557\n",
      "3050 iter loss :  1.1546378\n",
      "3060 iter loss :  1.638801\n",
      "3070 iter loss :  0.617754\n",
      "3080 iter loss :  1.8982518\n",
      "3090 iter loss :  1.4507608\n",
      "3100 iter loss :  0.865582\n",
      "3110 iter loss :  0.91191316\n",
      "3120 iter loss :  1.3269213\n",
      "3130 iter loss :  1.0820867\n",
      "3140 iter loss :  1.4902105\n",
      "3150 iter loss :  1.2285262\n",
      "3160 iter loss :  1.0094583\n",
      "3170 iter loss :  1.3749547\n",
      "3180 iter loss :  1.608981\n",
      "3190 iter loss :  1.3514872\n",
      "3200 iter loss :  1.131829\n",
      "3210 iter loss :  0.77694964\n",
      "3220 iter loss :  0.81972474\n",
      "3230 iter loss :  1.1577337\n",
      "3240 iter loss :  0.89743453\n",
      "3250 iter loss :  1.7377368\n",
      "3260 iter loss :  1.3926986\n",
      "3270 iter loss :  1.0998443\n",
      "3280 iter loss :  1.2619995\n",
      "3290 iter loss :  1.3560382\n",
      "3300 iter loss :  1.2581105\n",
      "3310 iter loss :  0.8777533\n",
      "3320 iter loss :  1.3416357\n",
      "3330 iter loss :  0.8629802\n",
      "3340 iter loss :  0.7206205\n",
      "3350 iter loss :  1.3283195\n",
      "3360 iter loss :  0.91015136\n",
      "3370 iter loss :  1.3373477\n",
      "3380 iter loss :  1.734116\n",
      "3390 iter loss :  0.7880074\n",
      "3400 iter loss :  1.8169101\n",
      "3410 iter loss :  0.60477865\n",
      "3420 iter loss :  1.2862031\n",
      "3430 iter loss :  1.5022349\n",
      "3440 iter loss :  1.7287201\n",
      "3450 iter loss :  0.8223945\n",
      "3460 iter loss :  1.2345624\n",
      "3470 iter loss :  1.2042606\n",
      "3480 iter loss :  1.0705343\n",
      "3490 iter loss :  1.5499104\n",
      "3500 iter loss :  0.99749565\n",
      "3510 iter loss :  1.30815\n",
      "3520 iter loss :  1.1912293\n",
      "3530 iter loss :  0.73586357\n",
      "3540 iter loss :  1.5863963\n",
      "3550 iter loss :  0.8126615\n",
      "3560 iter loss :  1.119916\n",
      "3570 iter loss :  0.8150685\n",
      "3580 iter loss :  1.2858468\n",
      "3590 iter loss :  1.2092136\n",
      "3600 iter loss :  1.0947324\n",
      "3610 iter loss :  1.4385979\n",
      "3620 iter loss :  1.2452952\n",
      "3630 iter loss :  1.2061245\n",
      "3640 iter loss :  1.0107018\n",
      "3650 iter loss :  0.96710557\n",
      "3660 iter loss :  1.2689875\n",
      "3670 iter loss :  1.3344681\n",
      "3680 iter loss :  1.2224773\n",
      "3690 iter loss :  0.8695675\n",
      "3700 iter loss :  1.3715003\n",
      "3710 iter loss :  0.84439653\n",
      "3720 iter loss :  0.88222224\n",
      "3730 iter loss :  1.5537152\n",
      "3740 iter loss :  0.7562588\n",
      "3750 iter loss :  0.9976791\n",
      "3760 iter loss :  0.9738623\n",
      "3770 iter loss :  1.092457\n",
      "3780 iter loss :  1.3045051\n",
      "3790 iter loss :  1.1109775\n",
      "3800 iter loss :  1.3051143\n",
      "3810 iter loss :  1.1771282\n",
      "3820 iter loss :  1.1967933\n",
      "3830 iter loss :  1.2840542\n",
      "3840 iter loss :  1.3023927\n",
      "3850 iter loss :  1.3484801\n",
      "3860 iter loss :  1.8359733\n",
      "3870 iter loss :  1.1909428\n",
      "3880 iter loss :  1.0721122\n",
      "3890 iter loss :  1.478491\n",
      "3900 iter loss :  1.1498592\n",
      "3910 iter loss :  0.8597315\n",
      "3920 iter loss :  1.7557929\n",
      "3930 iter loss :  1.12777\n",
      "3940 iter loss :  1.7463355\n",
      "3950 iter loss :  1.2358186\n",
      "3960 iter loss :  1.3047556\n",
      "3970 iter loss :  1.3584408\n",
      "3980 iter loss :  1.036758\n",
      "3990 iter loss :  1.2351909\n",
      "4000 iter loss :  1.3939687\n"
     ]
    }
   ],
   "source": [
    "regressor = tf.keras.Sequential([layers.Conv1D(3, 3),\n",
    "                             layers.Conv1D(3, 3),\n",
    "                             layers.Flatten(),\n",
    "                             layers.Dense(1)])\n",
    "pred = regressor(np.ones([16,map_size[0],map_size[1]]))\n",
    "pred = tf.reshape(pred, [-1, 1])\n",
    "regressor.summary()\n",
    "\n",
    "i = 0\n",
    "x = []\n",
    "x_loss1 = []\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "for epoch in range(4000):\n",
    "    i+= 1\n",
    "    x.append(i)\n",
    "    for mp, correlations in ds.take(1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = regressor(correlations)\n",
    "            pred = tf.reshape(pred, [-1, 1])\n",
    "            loss = loss_fn(pred, mp)\n",
    "            x_loss1.append(loss)\n",
    "        gradients = tape.gradient(loss, regressor.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, regressor.trainable_variables)) \n",
    "        if i % 10 == 0:\n",
    "            print(i, \"iter loss : \", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv1d_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            multiple                  155       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            multiple                  48        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  19        \n",
      "=================================================================\n",
      "Total params: 222\n",
      "Trainable params: 222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 iter loss :  5.3148375\n",
      "20 iter loss :  5.372183\n",
      "30 iter loss :  5.4789605\n",
      "40 iter loss :  5.15216\n",
      "50 iter loss :  5.381894\n",
      "60 iter loss :  5.322453\n",
      "70 iter loss :  4.9644017\n",
      "80 iter loss :  4.5684524\n",
      "90 iter loss :  5.0929027\n",
      "100 iter loss :  5.3676577\n",
      "110 iter loss :  4.579795\n",
      "120 iter loss :  5.266763\n",
      "130 iter loss :  4.4468837\n",
      "140 iter loss :  5.558283\n",
      "150 iter loss :  5.016864\n",
      "160 iter loss :  4.730456\n",
      "170 iter loss :  4.557724\n",
      "180 iter loss :  4.080969\n",
      "190 iter loss :  4.4666758\n",
      "200 iter loss :  3.6741052\n",
      "210 iter loss :  4.5838456\n",
      "220 iter loss :  4.0666213\n",
      "230 iter loss :  3.824787\n",
      "240 iter loss :  4.140275\n",
      "250 iter loss :  3.8812888\n",
      "260 iter loss :  3.3027682\n",
      "270 iter loss :  3.0591516\n",
      "280 iter loss :  3.2231376\n",
      "290 iter loss :  3.6306446\n",
      "300 iter loss :  3.129075\n",
      "310 iter loss :  3.1299453\n",
      "320 iter loss :  3.2743366\n",
      "330 iter loss :  3.1855955\n",
      "340 iter loss :  3.062807\n",
      "350 iter loss :  3.0079763\n",
      "360 iter loss :  2.7366755\n",
      "370 iter loss :  2.816866\n",
      "380 iter loss :  3.3648999\n",
      "390 iter loss :  2.8594604\n",
      "400 iter loss :  3.1050138\n",
      "410 iter loss :  2.5304534\n",
      "420 iter loss :  2.3866236\n",
      "430 iter loss :  2.3939044\n",
      "440 iter loss :  3.0402002\n",
      "450 iter loss :  2.7005281\n",
      "460 iter loss :  3.071749\n",
      "470 iter loss :  2.946704\n",
      "480 iter loss :  2.868311\n",
      "490 iter loss :  2.6782358\n",
      "500 iter loss :  2.1130586\n",
      "510 iter loss :  2.2847722\n",
      "520 iter loss :  2.250411\n",
      "530 iter loss :  3.2437682\n",
      "540 iter loss :  2.6738212\n",
      "550 iter loss :  2.573035\n",
      "560 iter loss :  2.3041725\n",
      "570 iter loss :  2.205645\n",
      "580 iter loss :  3.1914742\n",
      "590 iter loss :  2.6627398\n",
      "600 iter loss :  3.053484\n",
      "610 iter loss :  2.76336\n",
      "620 iter loss :  2.6158874\n",
      "630 iter loss :  2.1829875\n",
      "640 iter loss :  2.932845\n",
      "650 iter loss :  2.9031348\n",
      "660 iter loss :  2.691746\n",
      "670 iter loss :  1.9199566\n",
      "680 iter loss :  2.6135747\n",
      "690 iter loss :  2.1506922\n",
      "700 iter loss :  2.572467\n",
      "710 iter loss :  2.523275\n",
      "720 iter loss :  3.316891\n",
      "730 iter loss :  1.8377106\n",
      "740 iter loss :  2.5059893\n",
      "750 iter loss :  2.1509283\n",
      "760 iter loss :  1.9462917\n",
      "770 iter loss :  2.5935488\n",
      "780 iter loss :  2.3963642\n",
      "790 iter loss :  2.6439147\n",
      "800 iter loss :  3.05797\n",
      "810 iter loss :  2.454917\n",
      "820 iter loss :  2.3802328\n",
      "830 iter loss :  2.3139284\n",
      "840 iter loss :  2.248327\n",
      "850 iter loss :  2.1963837\n",
      "860 iter loss :  3.0226214\n",
      "870 iter loss :  2.2605944\n",
      "880 iter loss :  2.2859452\n",
      "890 iter loss :  2.116378\n",
      "900 iter loss :  2.3376596\n",
      "910 iter loss :  1.5004622\n",
      "920 iter loss :  1.4142605\n",
      "930 iter loss :  2.2180474\n",
      "940 iter loss :  2.5990477\n",
      "950 iter loss :  1.3928987\n",
      "960 iter loss :  1.808411\n",
      "970 iter loss :  2.175845\n",
      "980 iter loss :  1.5982907\n",
      "990 iter loss :  2.129438\n",
      "1000 iter loss :  1.840927\n",
      "1010 iter loss :  1.5060703\n",
      "1020 iter loss :  2.225658\n",
      "1030 iter loss :  2.3046877\n",
      "1040 iter loss :  2.1447341\n",
      "1050 iter loss :  1.3586661\n",
      "1060 iter loss :  1.9437927\n",
      "1070 iter loss :  2.1509514\n",
      "1080 iter loss :  2.2122297\n",
      "1090 iter loss :  2.2316067\n",
      "1100 iter loss :  2.2572763\n",
      "1110 iter loss :  2.4530253\n",
      "1120 iter loss :  1.8062352\n",
      "1130 iter loss :  2.469902\n",
      "1140 iter loss :  1.1914762\n",
      "1150 iter loss :  1.7220492\n",
      "1160 iter loss :  2.1324975\n",
      "1170 iter loss :  2.7413213\n",
      "1180 iter loss :  2.00358\n",
      "1190 iter loss :  2.3974555\n",
      "1200 iter loss :  1.3637227\n",
      "1210 iter loss :  1.2093946\n",
      "1220 iter loss :  1.2523949\n",
      "1230 iter loss :  1.5824646\n",
      "1240 iter loss :  1.8392922\n",
      "1250 iter loss :  1.5948653\n",
      "1260 iter loss :  1.3906288\n",
      "1270 iter loss :  1.873804\n",
      "1280 iter loss :  2.1693707\n",
      "1290 iter loss :  2.1872811\n",
      "1300 iter loss :  1.6383814\n",
      "1310 iter loss :  2.2377245\n",
      "1320 iter loss :  1.7037784\n",
      "1330 iter loss :  1.3907391\n",
      "1340 iter loss :  1.4863223\n",
      "1350 iter loss :  2.2297094\n",
      "1360 iter loss :  2.3750384\n",
      "1370 iter loss :  1.3458217\n",
      "1380 iter loss :  1.2650679\n",
      "1390 iter loss :  1.5120654\n",
      "1400 iter loss :  1.6785529\n",
      "1410 iter loss :  1.9641103\n",
      "1420 iter loss :  2.0715823\n",
      "1430 iter loss :  2.0453522\n",
      "1440 iter loss :  1.9961118\n",
      "1450 iter loss :  1.0264065\n",
      "1460 iter loss :  2.1113365\n",
      "1470 iter loss :  1.5908378\n",
      "1480 iter loss :  1.3438122\n",
      "1490 iter loss :  1.1873363\n",
      "1500 iter loss :  1.2814395\n",
      "1510 iter loss :  1.6191031\n",
      "1520 iter loss :  1.7666769\n",
      "1530 iter loss :  1.5781163\n",
      "1540 iter loss :  2.4388926\n",
      "1550 iter loss :  1.7736777\n",
      "1560 iter loss :  1.6180397\n",
      "1570 iter loss :  1.657158\n",
      "1580 iter loss :  1.5112288\n",
      "1590 iter loss :  1.5972995\n",
      "1600 iter loss :  2.0552175\n",
      "1610 iter loss :  1.9482745\n",
      "1620 iter loss :  1.9955155\n",
      "1630 iter loss :  2.1352448\n",
      "1640 iter loss :  1.1152964\n",
      "1650 iter loss :  1.6814243\n",
      "1660 iter loss :  1.2448399\n",
      "1670 iter loss :  0.5713449\n",
      "1680 iter loss :  1.5642744\n",
      "1690 iter loss :  2.108925\n",
      "1700 iter loss :  1.2408638\n",
      "1710 iter loss :  1.3042908\n",
      "1720 iter loss :  0.8756338\n",
      "1730 iter loss :  1.1608561\n",
      "1740 iter loss :  1.4936938\n",
      "1750 iter loss :  1.4008193\n",
      "1760 iter loss :  0.6577126\n",
      "1770 iter loss :  1.6880748\n",
      "1780 iter loss :  1.9028213\n",
      "1790 iter loss :  1.2172782\n",
      "1800 iter loss :  1.8155133\n",
      "1810 iter loss :  1.8284683\n",
      "1820 iter loss :  1.6235889\n",
      "1830 iter loss :  0.8896534\n",
      "1840 iter loss :  1.7698638\n",
      "1850 iter loss :  1.1032029\n",
      "1860 iter loss :  1.2673419\n",
      "1870 iter loss :  1.1908399\n",
      "1880 iter loss :  1.1570262\n",
      "1890 iter loss :  0.8935402\n",
      "1900 iter loss :  1.6235344\n",
      "1910 iter loss :  1.3924309\n",
      "1920 iter loss :  1.0630131\n",
      "1930 iter loss :  0.939119\n",
      "1940 iter loss :  1.8357458\n",
      "1950 iter loss :  1.6720816\n",
      "1960 iter loss :  0.5454942\n",
      "1970 iter loss :  1.1205006\n",
      "1980 iter loss :  1.6508539\n",
      "1990 iter loss :  1.2567602\n",
      "2000 iter loss :  1.3528322\n",
      "2010 iter loss :  1.9846547\n",
      "2020 iter loss :  1.4576402\n",
      "2030 iter loss :  1.2639867\n",
      "2040 iter loss :  1.1782498\n",
      "2050 iter loss :  1.1040856\n",
      "2060 iter loss :  1.4584384\n",
      "2070 iter loss :  1.9551308\n",
      "2080 iter loss :  1.1703272\n",
      "2090 iter loss :  0.8239074\n",
      "2100 iter loss :  1.2151827\n",
      "2110 iter loss :  1.8569117\n",
      "2120 iter loss :  1.4061079\n",
      "2130 iter loss :  1.2496551\n",
      "2140 iter loss :  1.2565339\n",
      "2150 iter loss :  1.2851763\n",
      "2160 iter loss :  2.436241\n",
      "2170 iter loss :  2.2026207\n",
      "2180 iter loss :  0.81858903\n",
      "2190 iter loss :  1.1154522\n",
      "2200 iter loss :  2.2947237\n",
      "2210 iter loss :  1.1977868\n",
      "2220 iter loss :  1.0643383\n",
      "2230 iter loss :  1.1155285\n",
      "2240 iter loss :  1.7063931\n",
      "2250 iter loss :  1.8621641\n",
      "2260 iter loss :  1.6953503\n",
      "2270 iter loss :  2.1340263\n",
      "2280 iter loss :  0.6200998\n",
      "2290 iter loss :  0.7273988\n",
      "2300 iter loss :  1.4311806\n",
      "2310 iter loss :  1.4459981\n",
      "2320 iter loss :  0.98592734\n",
      "2330 iter loss :  0.89321345\n",
      "2340 iter loss :  0.83967453\n",
      "2350 iter loss :  1.5219334\n",
      "2360 iter loss :  2.2761886\n",
      "2370 iter loss :  2.0915391\n",
      "2380 iter loss :  0.98000056\n",
      "2390 iter loss :  1.6746424\n",
      "2400 iter loss :  1.8391359\n",
      "2410 iter loss :  1.5120528\n",
      "2420 iter loss :  1.6084462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2430 iter loss :  1.5873691\n",
      "2440 iter loss :  1.1619891\n",
      "2450 iter loss :  0.96331495\n",
      "2460 iter loss :  0.88348794\n",
      "2470 iter loss :  1.0697777\n",
      "2480 iter loss :  1.1705784\n",
      "2490 iter loss :  1.396633\n",
      "2500 iter loss :  1.5097536\n",
      "2510 iter loss :  1.3093026\n",
      "2520 iter loss :  1.8867427\n",
      "2530 iter loss :  0.78969896\n",
      "2540 iter loss :  1.1416714\n",
      "2550 iter loss :  1.374677\n",
      "2560 iter loss :  1.5765022\n",
      "2570 iter loss :  1.5363476\n",
      "2580 iter loss :  1.7444876\n",
      "2590 iter loss :  0.99906296\n",
      "2600 iter loss :  1.9450787\n",
      "2610 iter loss :  1.6758478\n",
      "2620 iter loss :  0.7824514\n",
      "2630 iter loss :  0.8399876\n",
      "2640 iter loss :  2.0388403\n",
      "2650 iter loss :  1.3762045\n",
      "2660 iter loss :  1.3005525\n",
      "2670 iter loss :  1.1100721\n",
      "2680 iter loss :  1.1931622\n",
      "2690 iter loss :  1.1422616\n",
      "2700 iter loss :  1.907272\n",
      "2710 iter loss :  1.2940474\n",
      "2720 iter loss :  1.190643\n",
      "2730 iter loss :  1.0375917\n",
      "2740 iter loss :  1.6575687\n",
      "2750 iter loss :  0.8613736\n",
      "2760 iter loss :  1.2708064\n",
      "2770 iter loss :  1.9771564\n",
      "2780 iter loss :  1.1359092\n",
      "2790 iter loss :  1.1087207\n",
      "2800 iter loss :  0.53488374\n",
      "2810 iter loss :  1.4784573\n",
      "2820 iter loss :  1.202777\n",
      "2830 iter loss :  1.7120723\n",
      "2840 iter loss :  1.3079628\n",
      "2850 iter loss :  1.4771583\n",
      "2860 iter loss :  1.6205052\n",
      "2870 iter loss :  0.84704596\n",
      "2880 iter loss :  1.7496489\n",
      "2890 iter loss :  0.8005651\n",
      "2900 iter loss :  1.4135747\n",
      "2910 iter loss :  0.6741437\n",
      "2920 iter loss :  1.6312453\n",
      "2930 iter loss :  0.96184593\n",
      "2940 iter loss :  0.8856749\n",
      "2950 iter loss :  0.73036456\n",
      "2960 iter loss :  1.0243012\n",
      "2970 iter loss :  1.2023259\n",
      "2980 iter loss :  0.48788482\n",
      "2990 iter loss :  1.1167542\n",
      "3000 iter loss :  1.7670975\n",
      "3010 iter loss :  0.7478703\n",
      "3020 iter loss :  0.7012078\n",
      "3030 iter loss :  0.5930508\n",
      "3040 iter loss :  0.81350917\n",
      "3050 iter loss :  1.0121859\n",
      "3060 iter loss :  0.9465438\n",
      "3070 iter loss :  1.5167015\n",
      "3080 iter loss :  0.7919728\n",
      "3090 iter loss :  1.0622821\n",
      "3100 iter loss :  0.8896395\n",
      "3110 iter loss :  1.4692172\n",
      "3120 iter loss :  0.7224476\n",
      "3130 iter loss :  0.7361407\n",
      "3140 iter loss :  1.8396637\n",
      "3150 iter loss :  0.81517327\n",
      "3160 iter loss :  1.3261476\n",
      "3170 iter loss :  1.4248734\n",
      "3180 iter loss :  1.442562\n",
      "3190 iter loss :  2.2438478\n",
      "3200 iter loss :  2.0638175\n",
      "3210 iter loss :  1.6103244\n",
      "3220 iter loss :  2.165162\n",
      "3230 iter loss :  1.234752\n",
      "3240 iter loss :  0.7389138\n",
      "3250 iter loss :  1.4225994\n",
      "3260 iter loss :  0.85253286\n",
      "3270 iter loss :  0.99986005\n",
      "3280 iter loss :  1.4536933\n",
      "3290 iter loss :  1.0416712\n",
      "3300 iter loss :  1.2407756\n",
      "3310 iter loss :  0.68159753\n",
      "3320 iter loss :  1.2939508\n",
      "3330 iter loss :  0.5612025\n",
      "3340 iter loss :  1.2261082\n",
      "3350 iter loss :  1.1524262\n",
      "3360 iter loss :  0.81875765\n",
      "3370 iter loss :  1.1029923\n",
      "3380 iter loss :  1.5182648\n",
      "3390 iter loss :  1.0824744\n",
      "3400 iter loss :  1.173033\n",
      "3410 iter loss :  1.4815972\n",
      "3420 iter loss :  1.0745149\n",
      "3430 iter loss :  1.2776966\n",
      "3440 iter loss :  1.4077318\n",
      "3450 iter loss :  1.1931604\n",
      "3460 iter loss :  0.9720884\n",
      "3470 iter loss :  1.704048\n",
      "3480 iter loss :  0.83362997\n",
      "3490 iter loss :  1.3838115\n",
      "3500 iter loss :  1.3684293\n",
      "3510 iter loss :  1.427716\n",
      "3520 iter loss :  1.2146327\n",
      "3530 iter loss :  1.8278968\n",
      "3540 iter loss :  0.6262503\n",
      "3550 iter loss :  1.3797541\n",
      "3560 iter loss :  0.635512\n",
      "3570 iter loss :  1.3080603\n",
      "3580 iter loss :  1.8772974\n",
      "3590 iter loss :  1.2412121\n",
      "3600 iter loss :  1.5847625\n",
      "3610 iter loss :  2.0274017\n",
      "3620 iter loss :  1.1334203\n",
      "3630 iter loss :  1.5386738\n",
      "3640 iter loss :  1.2780246\n",
      "3650 iter loss :  1.4773058\n",
      "3660 iter loss :  1.2205693\n",
      "3670 iter loss :  0.64382905\n",
      "3680 iter loss :  1.0087715\n",
      "3690 iter loss :  0.46945658\n",
      "3700 iter loss :  0.4184746\n",
      "3710 iter loss :  1.8612252\n",
      "3720 iter loss :  1.5564265\n",
      "3730 iter loss :  1.216922\n",
      "3740 iter loss :  2.0514567\n",
      "3750 iter loss :  0.5997021\n",
      "3760 iter loss :  0.9728859\n",
      "3770 iter loss :  1.5577674\n",
      "3780 iter loss :  1.0672984\n",
      "3790 iter loss :  1.8223555\n",
      "3800 iter loss :  1.9221054\n",
      "3810 iter loss :  1.2893323\n",
      "3820 iter loss :  1.4134904\n",
      "3830 iter loss :  1.4178603\n",
      "3840 iter loss :  0.9371978\n",
      "3850 iter loss :  1.3189502\n",
      "3860 iter loss :  1.7370083\n",
      "3870 iter loss :  1.1212037\n",
      "3880 iter loss :  0.7032689\n",
      "3890 iter loss :  0.5627977\n",
      "3900 iter loss :  2.0754821\n",
      "3910 iter loss :  1.7286813\n",
      "3920 iter loss :  1.1168358\n",
      "3930 iter loss :  1.8350973\n",
      "3940 iter loss :  1.0096138\n",
      "3950 iter loss :  1.1797098\n",
      "3960 iter loss :  0.99915713\n",
      "3970 iter loss :  1.7222981\n",
      "3980 iter loss :  1.0550324\n",
      "3990 iter loss :  1.249759\n",
      "4000 iter loss :  0.8872141\n"
     ]
    }
   ],
   "source": [
    "regressor = tf.keras.Sequential([layers.Conv1D(5, 3),\n",
    "                             layers.Conv1D(3, 3),\n",
    "                             layers.Flatten(),\n",
    "                             layers.Dense(1)])\n",
    "pred = regressor(np.ones([16,10,10]))\n",
    "pred = tf.reshape(pred, [-1, 1])\n",
    "regressor.summary()\n",
    "\n",
    "i = 0\n",
    "x = []\n",
    "x_loss2 = []\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for epoch in range(4000):\n",
    "    i+= 1\n",
    "    x.append(i)\n",
    "    for mp, correlations in ds.take(1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = regressor(correlations)\n",
    "            pred = tf.reshape(pred, [-1, 1])\n",
    "            loss = loss_fn(pred, mp)\n",
    "            x_loss2.append(loss)\n",
    "        gradients = tape.gradient(loss, regressor.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, regressor.trainable_variables)) \n",
    "        if i % 10 == 0:\n",
    "            print(i, \"iter loss : \", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv1d_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            multiple                  93        \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            multiple                  50        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  31        \n",
      "=================================================================\n",
      "Total params: 174\n",
      "Trainable params: 174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 iter loss :  4.7327485\n",
      "20 iter loss :  5.2069483\n",
      "30 iter loss :  5.1500707\n",
      "40 iter loss :  5.3561525\n",
      "50 iter loss :  5.079407\n",
      "60 iter loss :  5.351317\n",
      "70 iter loss :  5.004444\n",
      "80 iter loss :  5.175328\n",
      "90 iter loss :  4.600278\n",
      "100 iter loss :  4.2169237\n",
      "110 iter loss :  4.528116\n",
      "120 iter loss :  5.3363695\n",
      "130 iter loss :  5.205307\n",
      "140 iter loss :  5.238996\n",
      "150 iter loss :  3.9916308\n",
      "160 iter loss :  4.29803\n",
      "170 iter loss :  4.333738\n",
      "180 iter loss :  4.6781764\n",
      "190 iter loss :  4.1141253\n",
      "200 iter loss :  4.011742\n",
      "210 iter loss :  3.9581907\n",
      "220 iter loss :  3.8933387\n",
      "230 iter loss :  4.1758523\n",
      "240 iter loss :  3.8571572\n",
      "250 iter loss :  4.0875645\n",
      "260 iter loss :  3.7009559\n",
      "270 iter loss :  3.6810386\n",
      "280 iter loss :  3.9065053\n",
      "290 iter loss :  3.1103153\n",
      "300 iter loss :  3.5855393\n",
      "310 iter loss :  3.5527086\n",
      "320 iter loss :  3.1842034\n",
      "330 iter loss :  2.7925346\n",
      "340 iter loss :  3.4983578\n",
      "350 iter loss :  3.6231718\n",
      "360 iter loss :  2.940938\n",
      "370 iter loss :  3.4704685\n",
      "380 iter loss :  2.8194003\n",
      "390 iter loss :  2.8679523\n",
      "400 iter loss :  2.038089\n",
      "410 iter loss :  3.433014\n",
      "420 iter loss :  2.9768488\n",
      "430 iter loss :  2.5586202\n",
      "440 iter loss :  2.515627\n",
      "450 iter loss :  2.2948458\n",
      "460 iter loss :  2.8420932\n",
      "470 iter loss :  2.6577747\n",
      "480 iter loss :  2.213812\n",
      "490 iter loss :  2.4380448\n",
      "500 iter loss :  2.7430778\n",
      "510 iter loss :  1.8953578\n",
      "520 iter loss :  2.3357344\n",
      "530 iter loss :  2.3996716\n",
      "540 iter loss :  2.3374248\n",
      "550 iter loss :  2.4068918\n",
      "560 iter loss :  2.3461945\n",
      "570 iter loss :  2.5745544\n",
      "580 iter loss :  1.9210974\n",
      "590 iter loss :  2.5975142\n",
      "600 iter loss :  2.7434008\n",
      "610 iter loss :  2.8279188\n",
      "620 iter loss :  2.2472448\n",
      "630 iter loss :  2.2234719\n",
      "640 iter loss :  2.1981215\n",
      "650 iter loss :  1.9508897\n",
      "660 iter loss :  1.9416995\n",
      "670 iter loss :  1.8232243\n",
      "680 iter loss :  2.1622875\n",
      "690 iter loss :  1.5372367\n",
      "700 iter loss :  1.950232\n",
      "710 iter loss :  1.846954\n",
      "720 iter loss :  1.8569937\n",
      "730 iter loss :  2.1299741\n",
      "740 iter loss :  1.5710535\n",
      "750 iter loss :  1.7519803\n",
      "760 iter loss :  1.9623268\n",
      "770 iter loss :  1.4051763\n",
      "780 iter loss :  1.8065414\n",
      "790 iter loss :  1.7056952\n",
      "800 iter loss :  1.3411622\n",
      "810 iter loss :  1.8577763\n",
      "820 iter loss :  2.0238357\n",
      "830 iter loss :  2.2734177\n",
      "840 iter loss :  1.6459796\n",
      "850 iter loss :  1.7843194\n",
      "860 iter loss :  1.480846\n",
      "870 iter loss :  1.7413538\n",
      "880 iter loss :  1.938435\n",
      "890 iter loss :  2.1437178\n",
      "900 iter loss :  1.8892822\n",
      "910 iter loss :  1.8294237\n",
      "920 iter loss :  1.5202966\n",
      "930 iter loss :  1.8317218\n",
      "940 iter loss :  1.5992584\n",
      "950 iter loss :  1.5661992\n",
      "960 iter loss :  1.5150334\n",
      "970 iter loss :  1.7539486\n",
      "980 iter loss :  1.6992203\n",
      "990 iter loss :  1.6528903\n",
      "1000 iter loss :  1.0781819\n",
      "1010 iter loss :  2.0976696\n",
      "1020 iter loss :  1.5029379\n",
      "1030 iter loss :  1.355009\n",
      "1040 iter loss :  1.6296886\n",
      "1050 iter loss :  1.2965108\n",
      "1060 iter loss :  1.7310067\n",
      "1070 iter loss :  1.3944584\n",
      "1080 iter loss :  1.0239152\n",
      "1090 iter loss :  1.7260535\n",
      "1100 iter loss :  1.6873904\n",
      "1110 iter loss :  2.2044716\n",
      "1120 iter loss :  1.314335\n",
      "1130 iter loss :  1.3882647\n",
      "1140 iter loss :  1.6769751\n",
      "1150 iter loss :  1.1330981\n",
      "1160 iter loss :  1.5822093\n",
      "1170 iter loss :  1.4168838\n",
      "1180 iter loss :  1.449114\n",
      "1190 iter loss :  1.7041429\n",
      "1200 iter loss :  1.2595977\n",
      "1210 iter loss :  1.6609061\n",
      "1220 iter loss :  1.0465392\n",
      "1230 iter loss :  1.1302024\n",
      "1240 iter loss :  1.4258325\n",
      "1250 iter loss :  1.3595918\n",
      "1260 iter loss :  1.4592022\n",
      "1270 iter loss :  1.3663014\n",
      "1280 iter loss :  1.6774004\n",
      "1290 iter loss :  1.2611606\n",
      "1300 iter loss :  1.3588758\n",
      "1310 iter loss :  1.3815329\n",
      "1320 iter loss :  0.9506501\n",
      "1330 iter loss :  1.3439362\n",
      "1340 iter loss :  0.93384796\n",
      "1350 iter loss :  0.9783905\n",
      "1360 iter loss :  1.0443398\n",
      "1370 iter loss :  1.1907107\n",
      "1380 iter loss :  0.8023286\n",
      "1390 iter loss :  0.7933134\n",
      "1400 iter loss :  1.3248665\n",
      "1410 iter loss :  1.2890171\n",
      "1420 iter loss :  1.5897282\n",
      "1430 iter loss :  1.328692\n",
      "1440 iter loss :  1.0513387\n",
      "1450 iter loss :  1.7579241\n",
      "1460 iter loss :  1.3544048\n",
      "1470 iter loss :  1.5840907\n",
      "1480 iter loss :  1.0236297\n",
      "1490 iter loss :  1.6239846\n",
      "1500 iter loss :  1.3497705\n",
      "1510 iter loss :  1.5189976\n",
      "1520 iter loss :  1.1053784\n",
      "1530 iter loss :  0.8946435\n",
      "1540 iter loss :  1.1497356\n",
      "1550 iter loss :  0.79534525\n",
      "1560 iter loss :  1.2321596\n",
      "1570 iter loss :  1.0940006\n",
      "1580 iter loss :  1.2481301\n",
      "1590 iter loss :  0.5399784\n",
      "1600 iter loss :  1.1984575\n",
      "1610 iter loss :  1.0264659\n",
      "1620 iter loss :  1.4851674\n",
      "1630 iter loss :  1.4609424\n",
      "1640 iter loss :  1.5805538\n",
      "1650 iter loss :  0.95300895\n",
      "1660 iter loss :  1.5627652\n",
      "1670 iter loss :  1.2530127\n",
      "1680 iter loss :  1.3028466\n",
      "1690 iter loss :  1.3429545\n",
      "1700 iter loss :  1.2151442\n",
      "1710 iter loss :  0.62229466\n",
      "1720 iter loss :  1.1095823\n",
      "1730 iter loss :  1.0665234\n",
      "1740 iter loss :  0.94234496\n",
      "1750 iter loss :  0.9911349\n",
      "1760 iter loss :  1.3966212\n",
      "1770 iter loss :  1.1849372\n",
      "1780 iter loss :  1.2302855\n",
      "1790 iter loss :  1.5351688\n",
      "1800 iter loss :  1.185856\n",
      "1810 iter loss :  1.2331345\n",
      "1820 iter loss :  0.8564044\n",
      "1830 iter loss :  0.946819\n",
      "1840 iter loss :  1.291095\n",
      "1850 iter loss :  1.1879086\n",
      "1860 iter loss :  1.0119008\n",
      "1870 iter loss :  0.8045041\n",
      "1880 iter loss :  1.4039655\n",
      "1890 iter loss :  0.95293707\n",
      "1900 iter loss :  1.1751331\n",
      "1910 iter loss :  0.99447954\n",
      "1920 iter loss :  1.0366411\n",
      "1930 iter loss :  0.79696196\n",
      "1940 iter loss :  0.9491763\n",
      "1950 iter loss :  1.2845733\n",
      "1960 iter loss :  1.0747563\n",
      "1970 iter loss :  1.1306615\n",
      "1980 iter loss :  0.8819826\n",
      "1990 iter loss :  1.1821996\n",
      "2000 iter loss :  0.93485457\n",
      "2010 iter loss :  0.9877008\n",
      "2020 iter loss :  1.0889736\n",
      "2030 iter loss :  1.1081445\n",
      "2040 iter loss :  1.1586131\n",
      "2050 iter loss :  0.9105127\n",
      "2060 iter loss :  1.055328\n",
      "2070 iter loss :  1.1205539\n",
      "2080 iter loss :  2.154428\n",
      "2090 iter loss :  1.0722432\n",
      "2100 iter loss :  1.167742\n",
      "2110 iter loss :  0.7395786\n",
      "2120 iter loss :  1.1307365\n",
      "2130 iter loss :  1.0242809\n",
      "2140 iter loss :  1.3469309\n",
      "2150 iter loss :  0.8732671\n",
      "2160 iter loss :  0.902153\n",
      "2170 iter loss :  1.3857532\n",
      "2180 iter loss :  1.2774988\n",
      "2190 iter loss :  1.0653007\n",
      "2200 iter loss :  0.42319778\n",
      "2210 iter loss :  0.7371469\n",
      "2220 iter loss :  1.5691699\n",
      "2230 iter loss :  1.1609751\n",
      "2240 iter loss :  1.4695839\n",
      "2250 iter loss :  1.2383828\n",
      "2260 iter loss :  1.130875\n",
      "2270 iter loss :  1.1787326\n",
      "2280 iter loss :  0.76785934\n",
      "2290 iter loss :  1.2360903\n",
      "2300 iter loss :  1.1477153\n",
      "2310 iter loss :  1.0269746\n",
      "2320 iter loss :  1.015635\n",
      "2330 iter loss :  1.4083072\n",
      "2340 iter loss :  1.1884356\n",
      "2350 iter loss :  1.4263337\n",
      "2360 iter loss :  1.0466528\n",
      "2370 iter loss :  1.0268431\n",
      "2380 iter loss :  0.8546246\n",
      "2390 iter loss :  0.51634616\n",
      "2400 iter loss :  0.9264212\n",
      "2410 iter loss :  1.5317181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2420 iter loss :  1.006172\n",
      "2430 iter loss :  1.1286206\n",
      "2440 iter loss :  0.6397642\n",
      "2450 iter loss :  0.9031079\n",
      "2460 iter loss :  0.7576168\n",
      "2470 iter loss :  1.7474341\n",
      "2480 iter loss :  0.7777622\n",
      "2490 iter loss :  0.9896\n",
      "2500 iter loss :  0.7737493\n",
      "2510 iter loss :  0.9543729\n",
      "2520 iter loss :  0.93614054\n",
      "2530 iter loss :  0.8054418\n",
      "2540 iter loss :  0.8994037\n",
      "2550 iter loss :  0.8314912\n",
      "2560 iter loss :  1.0987699\n",
      "2570 iter loss :  0.70166814\n",
      "2580 iter loss :  0.92813283\n",
      "2590 iter loss :  1.6429927\n",
      "2600 iter loss :  1.278004\n",
      "2610 iter loss :  1.4033552\n",
      "2620 iter loss :  1.1438513\n",
      "2630 iter loss :  1.0734961\n",
      "2640 iter loss :  1.0176395\n",
      "2650 iter loss :  0.97344214\n",
      "2660 iter loss :  1.0420171\n",
      "2670 iter loss :  0.889834\n",
      "2680 iter loss :  1.3078065\n",
      "2690 iter loss :  1.1952194\n",
      "2700 iter loss :  0.9392435\n",
      "2710 iter loss :  1.0852532\n",
      "2720 iter loss :  1.1126533\n",
      "2730 iter loss :  0.6162038\n",
      "2740 iter loss :  0.8492031\n",
      "2750 iter loss :  0.83486646\n",
      "2760 iter loss :  1.2959404\n",
      "2770 iter loss :  1.0496187\n",
      "2780 iter loss :  0.932018\n",
      "2790 iter loss :  1.4640623\n",
      "2800 iter loss :  1.8255081\n",
      "2810 iter loss :  1.2196951\n",
      "2820 iter loss :  1.1958477\n",
      "2830 iter loss :  0.47693902\n",
      "2840 iter loss :  1.0935526\n",
      "2850 iter loss :  0.6456337\n",
      "2860 iter loss :  1.2061353\n",
      "2870 iter loss :  1.2039005\n",
      "2880 iter loss :  0.99937224\n",
      "2890 iter loss :  1.4051499\n",
      "2900 iter loss :  1.3145804\n",
      "2910 iter loss :  1.2043362\n",
      "2920 iter loss :  0.6006409\n",
      "2930 iter loss :  0.87024534\n",
      "2940 iter loss :  1.3881668\n",
      "2950 iter loss :  1.4192613\n",
      "2960 iter loss :  1.1949543\n",
      "2970 iter loss :  1.3268487\n",
      "2980 iter loss :  1.0242585\n",
      "2990 iter loss :  1.0461934\n",
      "3000 iter loss :  1.0504005\n",
      "3010 iter loss :  1.032051\n",
      "3020 iter loss :  0.9304821\n",
      "3030 iter loss :  0.90913826\n",
      "3040 iter loss :  1.2515116\n",
      "3050 iter loss :  0.78148943\n",
      "3060 iter loss :  0.89142\n",
      "3070 iter loss :  0.40977234\n",
      "3080 iter loss :  1.2297674\n",
      "3090 iter loss :  1.3168292\n",
      "3100 iter loss :  1.2548753\n",
      "3110 iter loss :  1.2071564\n",
      "3120 iter loss :  0.5155917\n",
      "3130 iter loss :  0.89212936\n",
      "3140 iter loss :  1.1916842\n",
      "3150 iter loss :  0.5883606\n",
      "3160 iter loss :  0.8303936\n",
      "3170 iter loss :  1.4298961\n",
      "3180 iter loss :  0.6714011\n",
      "3190 iter loss :  0.71982276\n",
      "3200 iter loss :  1.0079088\n",
      "3210 iter loss :  0.904538\n",
      "3220 iter loss :  0.971719\n",
      "3230 iter loss :  0.6549118\n",
      "3240 iter loss :  0.8164273\n",
      "3250 iter loss :  1.0126262\n",
      "3260 iter loss :  1.1747804\n",
      "3270 iter loss :  1.3831105\n",
      "3280 iter loss :  1.4525025\n",
      "3290 iter loss :  0.77601045\n",
      "3300 iter loss :  0.8893677\n",
      "3310 iter loss :  0.68366915\n",
      "3320 iter loss :  1.0323725\n",
      "3330 iter loss :  0.57711923\n",
      "3340 iter loss :  0.91964304\n",
      "3350 iter loss :  1.4328157\n",
      "3360 iter loss :  1.0312259\n",
      "3370 iter loss :  1.8135092\n",
      "3380 iter loss :  1.2932239\n",
      "3390 iter loss :  0.9316475\n",
      "3400 iter loss :  0.95495605\n",
      "3410 iter loss :  0.60797334\n",
      "3420 iter loss :  1.1828693\n",
      "3430 iter loss :  0.9980411\n",
      "3440 iter loss :  1.5835229\n",
      "3450 iter loss :  0.8048432\n",
      "3460 iter loss :  0.8929226\n",
      "3470 iter loss :  0.904068\n",
      "3480 iter loss :  1.2753291\n",
      "3490 iter loss :  1.0004044\n",
      "3500 iter loss :  1.241806\n",
      "3510 iter loss :  1.2130364\n",
      "3520 iter loss :  0.91162753\n",
      "3530 iter loss :  1.0116599\n",
      "3540 iter loss :  1.2712191\n",
      "3550 iter loss :  1.4072108\n",
      "3560 iter loss :  1.0371593\n",
      "3570 iter loss :  0.90279144\n",
      "3580 iter loss :  1.7998385\n",
      "3590 iter loss :  0.894216\n",
      "3600 iter loss :  0.7708021\n",
      "3610 iter loss :  1.3979082\n",
      "3620 iter loss :  0.95485747\n",
      "3630 iter loss :  1.0747818\n",
      "3640 iter loss :  1.0394284\n",
      "3650 iter loss :  0.8912988\n",
      "3660 iter loss :  0.82202303\n",
      "3670 iter loss :  1.4066356\n",
      "3680 iter loss :  1.4076794\n",
      "3690 iter loss :  0.9909158\n",
      "3700 iter loss :  1.1265037\n",
      "3710 iter loss :  1.0342395\n",
      "3720 iter loss :  0.9991286\n",
      "3730 iter loss :  1.129076\n",
      "3740 iter loss :  1.1127237\n",
      "3750 iter loss :  0.8413598\n",
      "3760 iter loss :  1.3845677\n",
      "3770 iter loss :  1.0850617\n",
      "3780 iter loss :  1.6848603\n",
      "3790 iter loss :  1.4914873\n",
      "3800 iter loss :  1.1152321\n",
      "3810 iter loss :  1.8024428\n",
      "3820 iter loss :  1.5611764\n",
      "3830 iter loss :  0.6443022\n",
      "3840 iter loss :  1.5197083\n",
      "3850 iter loss :  0.8539781\n",
      "3860 iter loss :  1.1632825\n",
      "3870 iter loss :  0.8107664\n",
      "3880 iter loss :  0.75585634\n",
      "3890 iter loss :  0.7289949\n",
      "3900 iter loss :  1.2824347\n",
      "3910 iter loss :  1.2502083\n",
      "3920 iter loss :  0.94688416\n",
      "3930 iter loss :  0.873002\n",
      "3940 iter loss :  1.2923532\n",
      "3950 iter loss :  0.88344324\n",
      "3960 iter loss :  1.1329587\n",
      "3970 iter loss :  0.792494\n",
      "3980 iter loss :  0.9576017\n",
      "3990 iter loss :  1.211578\n",
      "4000 iter loss :  0.7364482\n"
     ]
    }
   ],
   "source": [
    "regressor = tf.keras.Sequential([layers.Conv1D(3, 3),\n",
    "                             layers.Conv1D(5, 3),\n",
    "                             layers.Flatten(),\n",
    "                             layers.Dense(1)])\n",
    "pred = regressor(np.ones([16,10,10]))\n",
    "pred = tf.reshape(pred, [-1, 1])\n",
    "regressor.summary()\n",
    "\n",
    "i = 0\n",
    "x = []\n",
    "x_loss3 = []\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for epoch in range(4000):\n",
    "    i+= 1\n",
    "    x.append(i)\n",
    "    for mp, correlations in ds.take(1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = regressor(correlations)\n",
    "            pred = tf.reshape(pred, [-1, 1])\n",
    "            loss = loss_fn(pred, mp)\n",
    "            x_loss3.append(loss)\n",
    "        gradients = tape.gradient(loss, regressor.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, regressor.trainable_variables)) \n",
    "        if i % 10 == 0:\n",
    "            print(i, \"iter loss : \", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4898dc4f28>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax1.title.set_text(\"training loss\")\n",
    "ax1.set_xlabel(\"epoch\")\n",
    "ax1.set_ylabel(\"training loss\")\n",
    "\n",
    "ax2.title.set_text(\"training loss\")\n",
    "ax2.set_xlabel(\"epoch\")\n",
    "ax2.set_ylabel(\"training loss\")\n",
    "\n",
    "\n",
    "ax3.title.set_text(\"training loss\")\n",
    "ax3.set_xlabel(\"epoch\")\n",
    "ax3.set_ylabel(\"training loss\")\n",
    "\n",
    "\n",
    "ax1.plot(np.array(x), np.array(x_loss1), label=\"3,3\")\n",
    "ax2.plot(np.array(x), np.array(x_loss2), label=\"5,3\")\n",
    "ax3.plot(np.array(x), np.array(x_loss3), label=\"3,5\")\n",
    "#plt.legend(loc=2)\n",
    "\n",
    "# ax2.title.set_text(\"std-dev of correlations\")\n",
    "# ax2.set_xlabel(\"epoch\")\n",
    "# ax2.set_ylabel(\"std-dev\")\n",
    "# ax2.plot(np.array(x), np.array(y_score_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv1d_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            multiple                  93        \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            multiple                  50        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  31        \n",
      "=================================================================\n",
      "Total params: 174\n",
      "Trainable params: 174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 iter loss :  4.926101\n",
      "20 iter loss :  5.3488827\n",
      "30 iter loss :  5.096687\n",
      "40 iter loss :  5.465713\n",
      "50 iter loss :  5.301909\n",
      "60 iter loss :  4.5854306\n",
      "70 iter loss :  5.9159117\n",
      "80 iter loss :  5.174085\n",
      "90 iter loss :  4.8335934\n",
      "100 iter loss :  5.0927553\n",
      "110 iter loss :  5.4272933\n",
      "120 iter loss :  5.754984\n",
      "130 iter loss :  5.3942604\n",
      "140 iter loss :  5.27126\n",
      "150 iter loss :  5.158186\n",
      "160 iter loss :  5.379984\n",
      "170 iter loss :  5.2169957\n",
      "180 iter loss :  4.9737687\n",
      "190 iter loss :  5.232418\n",
      "200 iter loss :  5.4033136\n",
      "210 iter loss :  5.3630414\n",
      "220 iter loss :  5.0098205\n",
      "230 iter loss :  5.171976\n",
      "240 iter loss :  5.1512084\n",
      "250 iter loss :  5.3249507\n",
      "260 iter loss :  5.246518\n",
      "270 iter loss :  5.096181\n",
      "280 iter loss :  5.0438685\n",
      "290 iter loss :  5.0205455\n",
      "300 iter loss :  5.196509\n",
      "310 iter loss :  4.773305\n",
      "320 iter loss :  5.363623\n",
      "330 iter loss :  4.9327617\n",
      "340 iter loss :  5.6637926\n",
      "350 iter loss :  4.995264\n",
      "360 iter loss :  4.5576215\n",
      "370 iter loss :  4.7829957\n",
      "380 iter loss :  5.5630646\n",
      "390 iter loss :  5.190864\n",
      "400 iter loss :  5.398042\n",
      "410 iter loss :  5.094673\n",
      "420 iter loss :  4.8754444\n",
      "430 iter loss :  4.690927\n",
      "440 iter loss :  4.627589\n",
      "450 iter loss :  5.02369\n",
      "460 iter loss :  4.6614575\n",
      "470 iter loss :  4.695206\n",
      "480 iter loss :  4.8789783\n",
      "490 iter loss :  5.009166\n",
      "500 iter loss :  4.055493\n",
      "510 iter loss :  4.8203263\n",
      "520 iter loss :  5.0480776\n",
      "530 iter loss :  4.381694\n",
      "540 iter loss :  3.9596941\n",
      "550 iter loss :  3.9181025\n",
      "560 iter loss :  4.1271944\n",
      "570 iter loss :  3.8503375\n",
      "580 iter loss :  4.0272775\n",
      "590 iter loss :  3.453679\n",
      "600 iter loss :  4.300546\n",
      "610 iter loss :  4.2951283\n",
      "620 iter loss :  2.9615355\n",
      "630 iter loss :  3.4098234\n",
      "640 iter loss :  3.6565769\n",
      "650 iter loss :  2.5963697\n",
      "660 iter loss :  3.5650976\n",
      "670 iter loss :  2.8172114\n",
      "680 iter loss :  3.4109075\n",
      "690 iter loss :  3.1612122\n",
      "700 iter loss :  3.150885\n",
      "710 iter loss :  3.3132775\n",
      "720 iter loss :  2.3336549\n",
      "730 iter loss :  3.2025602\n",
      "740 iter loss :  2.463084\n",
      "750 iter loss :  2.6391056\n",
      "760 iter loss :  3.1720386\n",
      "770 iter loss :  2.786403\n",
      "780 iter loss :  2.5569391\n",
      "790 iter loss :  2.547853\n",
      "800 iter loss :  2.656889\n",
      "810 iter loss :  2.7858143\n",
      "820 iter loss :  3.3059478\n",
      "830 iter loss :  2.9465673\n",
      "840 iter loss :  2.3805926\n",
      "850 iter loss :  2.9034376\n",
      "860 iter loss :  2.1088407\n",
      "870 iter loss :  1.695642\n",
      "880 iter loss :  2.7036343\n",
      "890 iter loss :  2.7171853\n",
      "900 iter loss :  1.8484371\n",
      "910 iter loss :  2.5197172\n",
      "920 iter loss :  2.9261515\n",
      "930 iter loss :  2.222196\n",
      "940 iter loss :  1.4360113\n",
      "950 iter loss :  2.665732\n",
      "960 iter loss :  1.8872082\n",
      "970 iter loss :  1.8532233\n",
      "980 iter loss :  2.4646769\n",
      "990 iter loss :  1.8098859\n",
      "1000 iter loss :  2.3027053\n",
      "1010 iter loss :  2.427609\n",
      "1020 iter loss :  2.2732768\n",
      "1030 iter loss :  2.4514627\n",
      "1040 iter loss :  1.9256697\n",
      "1050 iter loss :  2.4126585\n",
      "1060 iter loss :  2.84826\n",
      "1070 iter loss :  2.221426\n",
      "1080 iter loss :  1.9041197\n",
      "1090 iter loss :  2.5214415\n",
      "1100 iter loss :  2.1569161\n",
      "1110 iter loss :  2.0983787\n",
      "1120 iter loss :  2.183801\n",
      "1130 iter loss :  2.124734\n",
      "1140 iter loss :  1.8418823\n",
      "1150 iter loss :  1.9112891\n",
      "1160 iter loss :  1.4959385\n",
      "1170 iter loss :  1.4749391\n",
      "1180 iter loss :  2.2754166\n",
      "1190 iter loss :  2.1903596\n",
      "1200 iter loss :  2.5547953\n",
      "1210 iter loss :  1.9851712\n",
      "1220 iter loss :  2.3275788\n",
      "1230 iter loss :  1.5564919\n",
      "1240 iter loss :  1.7368841\n",
      "1250 iter loss :  2.3560526\n",
      "1260 iter loss :  1.9913919\n",
      "1270 iter loss :  1.6529335\n",
      "1280 iter loss :  1.8599755\n",
      "1290 iter loss :  1.5628587\n",
      "1300 iter loss :  2.2548776\n",
      "1310 iter loss :  1.4500047\n",
      "1320 iter loss :  1.8964435\n",
      "1330 iter loss :  1.9920601\n",
      "1340 iter loss :  2.2570639\n",
      "1350 iter loss :  1.9731114\n",
      "1360 iter loss :  1.5952008\n",
      "1370 iter loss :  1.5239804\n",
      "1380 iter loss :  1.1076189\n",
      "1390 iter loss :  0.96569324\n",
      "1400 iter loss :  1.4091365\n",
      "1410 iter loss :  1.340678\n",
      "1420 iter loss :  1.3576422\n",
      "1430 iter loss :  2.0182009\n",
      "1440 iter loss :  1.7042856\n",
      "1450 iter loss :  1.7002052\n",
      "1460 iter loss :  1.415517\n",
      "1470 iter loss :  1.7238852\n",
      "1480 iter loss :  2.2566547\n",
      "1490 iter loss :  1.8841614\n",
      "1500 iter loss :  1.6570401\n",
      "1510 iter loss :  1.4797014\n",
      "1520 iter loss :  1.4921243\n",
      "1530 iter loss :  1.9240001\n",
      "1540 iter loss :  1.9555737\n",
      "1550 iter loss :  2.3459187\n",
      "1560 iter loss :  0.71790075\n",
      "1570 iter loss :  1.5296338\n",
      "1580 iter loss :  1.9647332\n",
      "1590 iter loss :  1.0403491\n",
      "1600 iter loss :  1.6579101\n",
      "1610 iter loss :  1.4330254\n",
      "1620 iter loss :  0.8426623\n",
      "1630 iter loss :  1.604671\n",
      "1640 iter loss :  1.4705528\n",
      "1650 iter loss :  1.6235234\n",
      "1660 iter loss :  1.7674947\n",
      "1670 iter loss :  2.1150708\n",
      "1680 iter loss :  1.2545112\n",
      "1690 iter loss :  2.2149024\n",
      "1700 iter loss :  1.1215348\n",
      "1710 iter loss :  2.0488036\n",
      "1720 iter loss :  2.1830175\n",
      "1730 iter loss :  1.3970001\n",
      "1740 iter loss :  1.0166267\n",
      "1750 iter loss :  1.1918758\n",
      "1760 iter loss :  1.8808277\n",
      "1770 iter loss :  1.8238416\n",
      "1780 iter loss :  0.7815739\n",
      "1790 iter loss :  1.2635198\n",
      "1800 iter loss :  1.0484506\n",
      "1810 iter loss :  2.2107623\n",
      "1820 iter loss :  1.0568557\n",
      "1830 iter loss :  1.5340755\n",
      "1840 iter loss :  0.6742985\n",
      "1850 iter loss :  1.1929448\n",
      "1860 iter loss :  0.7122751\n",
      "1870 iter loss :  1.0566655\n",
      "1880 iter loss :  0.8513706\n",
      "1890 iter loss :  1.3158066\n",
      "1900 iter loss :  1.085566\n",
      "1910 iter loss :  1.8608179\n",
      "1920 iter loss :  1.0008429\n",
      "1930 iter loss :  1.212833\n",
      "1940 iter loss :  1.8458133\n",
      "1950 iter loss :  1.1630654\n",
      "1960 iter loss :  1.6205102\n",
      "1970 iter loss :  1.4597604\n",
      "1980 iter loss :  2.3279812\n",
      "1990 iter loss :  1.0250268\n",
      "2000 iter loss :  1.2191815\n",
      "2010 iter loss :  1.0667081\n",
      "2020 iter loss :  1.1424011\n",
      "2030 iter loss :  1.4519401\n",
      "2040 iter loss :  1.4097368\n",
      "2050 iter loss :  0.6505242\n",
      "2060 iter loss :  1.0004245\n",
      "2070 iter loss :  1.7683612\n",
      "2080 iter loss :  1.2927281\n",
      "2090 iter loss :  2.254961\n",
      "2100 iter loss :  0.72479516\n",
      "2110 iter loss :  1.4656261\n",
      "2120 iter loss :  0.9289639\n",
      "2130 iter loss :  1.6854819\n",
      "2140 iter loss :  1.1987106\n",
      "2150 iter loss :  1.3805166\n",
      "2160 iter loss :  2.1472132\n",
      "2170 iter loss :  0.9789124\n",
      "2180 iter loss :  1.0355687\n",
      "2190 iter loss :  1.0513991\n",
      "2200 iter loss :  1.04029\n",
      "2210 iter loss :  0.99236304\n",
      "2220 iter loss :  1.8754135\n",
      "2230 iter loss :  1.0122998\n",
      "2240 iter loss :  1.71065\n",
      "2250 iter loss :  0.7607436\n",
      "2260 iter loss :  0.93539834\n",
      "2270 iter loss :  1.5284764\n",
      "2280 iter loss :  1.4128348\n",
      "2290 iter loss :  1.329254\n",
      "2300 iter loss :  1.1994833\n",
      "2310 iter loss :  1.0768216\n",
      "2320 iter loss :  1.1186163\n",
      "2330 iter loss :  0.97283745\n",
      "2340 iter loss :  1.1648082\n",
      "2350 iter loss :  1.7833924\n",
      "2360 iter loss :  1.2147207\n",
      "2370 iter loss :  1.7971951\n",
      "2380 iter loss :  0.7242447\n",
      "2390 iter loss :  1.7119275\n",
      "2400 iter loss :  1.2570976\n",
      "2410 iter loss :  0.8887694\n",
      "2420 iter loss :  0.80532265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2430 iter loss :  1.5630606\n",
      "2440 iter loss :  1.396775\n",
      "2450 iter loss :  2.4913926\n",
      "2460 iter loss :  0.6846614\n",
      "2470 iter loss :  1.4511609\n",
      "2480 iter loss :  1.9629126\n",
      "2490 iter loss :  1.7684239\n",
      "2500 iter loss :  1.3885624\n",
      "2510 iter loss :  1.0402818\n",
      "2520 iter loss :  1.110211\n",
      "2530 iter loss :  1.4109339\n",
      "2540 iter loss :  1.2799869\n",
      "2550 iter loss :  1.1016707\n",
      "2560 iter loss :  1.5886412\n",
      "2570 iter loss :  0.8048508\n",
      "2580 iter loss :  1.8231605\n",
      "2590 iter loss :  1.91296\n",
      "2600 iter loss :  0.9948175\n",
      "2610 iter loss :  1.5390863\n",
      "2620 iter loss :  1.2357341\n",
      "2630 iter loss :  2.0804698\n",
      "2640 iter loss :  0.99066406\n",
      "2650 iter loss :  0.7511893\n",
      "2660 iter loss :  1.1690177\n",
      "2670 iter loss :  2.0108263\n",
      "2680 iter loss :  1.2518437\n",
      "2690 iter loss :  1.490603\n",
      "2700 iter loss :  0.93164223\n",
      "2710 iter loss :  0.8927809\n",
      "2720 iter loss :  1.2463949\n",
      "2730 iter loss :  1.0950305\n",
      "2740 iter loss :  0.7936402\n",
      "2750 iter loss :  1.2042717\n",
      "2760 iter loss :  2.1561658\n",
      "2770 iter loss :  0.94421494\n",
      "2780 iter loss :  0.5429085\n",
      "2790 iter loss :  1.5149089\n",
      "2800 iter loss :  1.4133708\n",
      "2810 iter loss :  1.279236\n",
      "2820 iter loss :  1.1895432\n",
      "2830 iter loss :  1.4680803\n",
      "2840 iter loss :  1.2875813\n",
      "2850 iter loss :  1.360823\n",
      "2860 iter loss :  1.0990584\n",
      "2870 iter loss :  0.8976534\n",
      "2880 iter loss :  1.095312\n",
      "2890 iter loss :  1.4257617\n",
      "2900 iter loss :  1.7295293\n",
      "2910 iter loss :  1.7228487\n",
      "2920 iter loss :  1.2962244\n",
      "2930 iter loss :  1.9057641\n",
      "2940 iter loss :  1.199253\n",
      "2950 iter loss :  1.0774347\n",
      "2960 iter loss :  1.1813854\n",
      "2970 iter loss :  1.0816609\n",
      "2980 iter loss :  0.8608545\n",
      "2990 iter loss :  1.5151193\n",
      "3000 iter loss :  1.5507152\n",
      "3010 iter loss :  1.123885\n",
      "3020 iter loss :  0.93221223\n",
      "3030 iter loss :  1.44116\n",
      "3040 iter loss :  1.3526458\n",
      "3050 iter loss :  0.7464679\n",
      "3060 iter loss :  0.8046738\n",
      "3070 iter loss :  1.4850141\n",
      "3080 iter loss :  1.1749384\n",
      "3090 iter loss :  0.9413153\n",
      "3100 iter loss :  1.7124923\n",
      "3110 iter loss :  1.0480952\n",
      "3120 iter loss :  1.5686547\n",
      "3130 iter loss :  0.8030153\n",
      "3140 iter loss :  0.736466\n",
      "3150 iter loss :  0.689802\n",
      "3160 iter loss :  1.3222066\n",
      "3170 iter loss :  1.119479\n",
      "3180 iter loss :  1.2321645\n",
      "3190 iter loss :  0.93522996\n",
      "3200 iter loss :  1.480956\n",
      "3210 iter loss :  0.928987\n",
      "3220 iter loss :  1.6096364\n",
      "3230 iter loss :  1.0302761\n",
      "3240 iter loss :  0.66637146\n",
      "3250 iter loss :  0.9510342\n",
      "3260 iter loss :  1.7666442\n",
      "3270 iter loss :  1.2005265\n",
      "3280 iter loss :  0.9682312\n",
      "3290 iter loss :  1.6129566\n",
      "3300 iter loss :  0.90670425\n",
      "3310 iter loss :  0.44847205\n",
      "3320 iter loss :  0.7636367\n",
      "3330 iter loss :  0.70522684\n",
      "3340 iter loss :  0.8477717\n",
      "3350 iter loss :  0.8115094\n",
      "3360 iter loss :  1.4576269\n",
      "3370 iter loss :  1.0900809\n",
      "3380 iter loss :  0.4321224\n",
      "3390 iter loss :  1.2867676\n",
      "3400 iter loss :  1.0496211\n",
      "3410 iter loss :  1.4463489\n",
      "3420 iter loss :  0.85374504\n",
      "3430 iter loss :  0.7156135\n",
      "3440 iter loss :  0.75029296\n",
      "3450 iter loss :  1.9426084\n",
      "3460 iter loss :  0.98975813\n",
      "3470 iter loss :  1.6174823\n",
      "3480 iter loss :  1.7530091\n",
      "3490 iter loss :  0.9122369\n",
      "3500 iter loss :  1.5546662\n",
      "3510 iter loss :  0.99739623\n",
      "3520 iter loss :  1.2314005\n",
      "3530 iter loss :  0.85656565\n",
      "3540 iter loss :  0.86062104\n",
      "3550 iter loss :  1.3326881\n",
      "3560 iter loss :  1.5624176\n",
      "3570 iter loss :  1.7155886\n",
      "3580 iter loss :  0.8946855\n",
      "3590 iter loss :  0.6592709\n",
      "3600 iter loss :  1.9090624\n",
      "3610 iter loss :  1.5430452\n",
      "3620 iter loss :  0.8219657\n",
      "3630 iter loss :  1.3511224\n",
      "3640 iter loss :  0.71644735\n",
      "3650 iter loss :  0.7782746\n",
      "3660 iter loss :  0.94753325\n",
      "3670 iter loss :  1.1542578\n",
      "3680 iter loss :  1.0648667\n",
      "3690 iter loss :  0.5772123\n",
      "3700 iter loss :  1.6181766\n",
      "3710 iter loss :  1.5390885\n",
      "3720 iter loss :  1.4736458\n",
      "3730 iter loss :  1.2480743\n",
      "3740 iter loss :  1.5060047\n",
      "3750 iter loss :  1.6529701\n",
      "3760 iter loss :  0.84813166\n",
      "3770 iter loss :  1.7013837\n",
      "3780 iter loss :  1.3853362\n",
      "3790 iter loss :  1.1498232\n",
      "3800 iter loss :  1.2300129\n",
      "3810 iter loss :  1.2996609\n",
      "3820 iter loss :  1.8784541\n",
      "3830 iter loss :  1.3359114\n",
      "3840 iter loss :  1.0537502\n",
      "3850 iter loss :  0.93746734\n",
      "3860 iter loss :  0.77682906\n",
      "3870 iter loss :  1.0577078\n",
      "3880 iter loss :  1.8362513\n",
      "3890 iter loss :  1.2279991\n",
      "3900 iter loss :  0.89258456\n",
      "3910 iter loss :  0.7478235\n",
      "3920 iter loss :  0.6170174\n",
      "3930 iter loss :  1.2096505\n",
      "3940 iter loss :  1.3565136\n",
      "3950 iter loss :  1.1371412\n",
      "3960 iter loss :  0.74343604\n",
      "3970 iter loss :  1.3403417\n",
      "3980 iter loss :  1.0314538\n",
      "3990 iter loss :  2.0785217\n",
      "4000 iter loss :  1.3514469\n"
     ]
    }
   ],
   "source": [
    "regressor = tf.keras.Sequential([layers.Conv1D(3, 3, activation='relu'),\n",
    "                             layers.Conv1D(5, 3, activation='relu'),\n",
    "                             layers.Flatten(),\n",
    "                             layers.Dense(1)])\n",
    "pred = regressor(np.ones([16,10,10]))\n",
    "pred = tf.reshape(pred, [-1, 1])\n",
    "regressor.summary()\n",
    "\n",
    "i = 0\n",
    "x = []\n",
    "x_loss3 = []\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for epoch in range(4000):\n",
    "    i+= 1\n",
    "    x.append(i)\n",
    "    for mp, correlations in ds.take(1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = regressor(correlations)\n",
    "            pred = tf.reshape(pred, [-1, 1])\n",
    "            loss = loss_fn(pred, mp)\n",
    "            x_loss3.append(loss)\n",
    "        gradients = tape.gradient(loss, regressor.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, regressor.trainable_variables)) \n",
    "        if i % 10 == 0:\n",
    "            print(i, \"iter loss : \", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f489934b828>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax1.title.set_text(\"training loss\")\n",
    "ax1.set_xlabel(\"epoch\")\n",
    "ax1.set_ylabel(\"training loss\")\n",
    "\n",
    "ax2.title.set_text(\"training loss\")\n",
    "ax2.set_xlabel(\"epoch\")\n",
    "ax2.set_ylabel(\"training loss\")\n",
    "\n",
    "\n",
    "ax3.title.set_text(\"training loss\")\n",
    "ax3.set_xlabel(\"epoch\")\n",
    "ax3.set_ylabel(\"training loss\")\n",
    "\n",
    "\n",
    "ax1.plot(np.array(x), np.array(x_loss1), label=\"3,3\")\n",
    "ax2.plot(np.array(x), np.array(x_loss2), label=\"5,3\")\n",
    "ax3.plot(np.array(x), np.array(x_loss3), label=\"3,5\")\n",
    "#plt.legend(loc=2)\n",
    "\n",
    "# ax2.title.set_text(\"std-dev of correlations\")\n",
    "# ax2.set_xlabel(\"epoch\")\n",
    "# ax2.set_ylabel(\"std-dev\")\n",
    "# ax2.plot(np.array(x), np.array(y_score_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "correlations = make_corr_map(3, map_size)\n",
    "correlations = correlations[np.newaxis, :]\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp : -3\n",
      "correlation map : [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "pred : [[-2.9509406]]\n",
      "mp : -2\n",
      "correlation map : [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "pred : [[-1.993837]]\n",
      "mp : -1\n",
      "correlation map : [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "pred : [[-1.0495007]]\n",
      "mp : 0\n",
      "correlation map : [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "pred : [[-0.03037992]]\n",
      "mp : 1\n",
      "correlation map : [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred : [[0.77065706]]\n",
      "mp : 2\n",
      "correlation map : [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred : [[1.8314229]]\n",
      "mp : 3\n",
      "correlation map : [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred : [[2.7361767]]\n"
     ]
    }
   ],
   "source": [
    "for mp in range(-3, 4):\n",
    "    print(\"mp : {}\".format(mp))\n",
    "    correlations = make_corr_map(mp, map_size)\n",
    "    print(\"correlation map : {}\".format(correlations))\n",
    "    correlations = correlations[np.newaxis, :]\n",
    "    print(\"pred : {}\".format(regressor(correlations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "correlations = make_corr_map(0, map_size)\n",
    "correlations = correlations[np.newaxis, :]\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.04047279 -0.99722314  0.3957979 ]\n",
      "  [-0.94426554 -0.4719824   0.26582116]\n",
      "  [-0.88229877 -0.23336631  0.81563485]\n",
      "  [ 0.05481879  0.04589921  0.40782452]\n",
      "  [ 0.31365323 -0.03390867  0.88932276]\n",
      "  [ 0.26640224  1.0559425   0.1406259 ]\n",
      "  [ 0.32395542  1.0599446   0.37888575]\n",
      "  [ 0.06934863  0.7657155  -0.20486099]]], shape=(1, 8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x0 = regressor.layers[0](correlations)\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 2.4977894   0.17562427  0.98474115 -1.3203086   0.9368994 ]\n",
      "  [ 1.4291443   0.3629106   0.77300125 -0.03006107  0.72971624]\n",
      "  [ 1.917206   -0.30980498 -0.06431294 -0.19640678  0.9131112 ]\n",
      "  [ 0.02132098 -0.1921407  -0.09719075  1.0107207  -0.38287225]\n",
      "  [ 0.4644827  -0.49491793 -0.86649126  1.0966722   0.18014663]\n",
      "  [-0.2628289  -0.7291928  -1.0780644   2.4244912  -0.8860649 ]]], shape=(1, 6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x1 = regressor.layers[1](x0)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2.4977894   0.17562427  0.98474115 -1.3203086   0.9368994   1.4291443\n",
      "   0.3629106   0.77300125 -0.03006107  0.72971624  1.917206   -0.30980498\n",
      "  -0.06431294 -0.19640678  0.9131112   0.02132098 -0.1921407  -0.09719075\n",
      "   1.0107207  -0.38287225  0.4644827  -0.49491793 -0.86649126  1.0966722\n",
      "   0.18014663 -0.2628289  -0.7291928  -1.0780644   2.4244912  -0.8860649 ]], shape=(1, 30), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x2 = regressor.layers[2](x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.20266402, -0.45680767,  0.19514614],\n",
      "        [-0.32682413,  0.14888588,  0.49707907],\n",
      "        [-0.39510965,  0.25430968,  0.5517269 ],\n",
      "        [-0.20713466, -0.13433601,  0.14050928],\n",
      "        [ 0.12854865,  0.43426514,  0.22738065],\n",
      "        [ 0.03235257,  0.32786995,  0.4731219 ],\n",
      "        [-0.37103134,  0.40387267, -0.08195354],\n",
      "        [-0.43279672,  0.48427862,  0.01932186],\n",
      "        [ 0.12695353,  0.59280384,  0.41961437],\n",
      "        [ 0.19153593,  2.3380334 ,  0.4012421 ]],\n",
      "\n",
      "       [[-0.22648907, -0.47799256,  0.3361597 ],\n",
      "        [-0.08392954, -0.23352493,  0.23309393],\n",
      "        [-0.38177422, -0.30574477,  0.13176784],\n",
      "        [-0.31792524,  0.01305335,  0.46315035],\n",
      "        [-0.12842464,  0.4573812 ,  0.31687924],\n",
      "        [ 0.10124819, -0.04134237,  0.46382728],\n",
      "        [-0.08486688,  0.4090763 ,  0.19505878],\n",
      "        [ 0.30798432,  0.2589119 ,  0.593645  ],\n",
      "        [ 0.30695447, -0.09361494, -0.09638142],\n",
      "        [-0.01554332,  0.5602452 ,  0.26661444]],\n",
      "\n",
      "       [[-0.9116127 , -1.5079285 ,  1.7115024 ],\n",
      "        [-0.3765011 , -0.38916856,  0.82275784],\n",
      "        [-0.21164429, -0.16029194,  0.39582425],\n",
      "        [-0.36904985, -0.16852495,  0.06524067],\n",
      "        [-0.30264655, -0.3541308 ,  0.22902401],\n",
      "        [ 0.2569955 , -0.13054743,  0.37870243],\n",
      "        [-0.04952623, -0.28023288,  0.6263812 ],\n",
      "        [ 0.18553396,  0.46559486, -0.09928842],\n",
      "        [ 0.25361982,  0.54375863,  0.29546073],\n",
      "        [ 0.06180827,  0.5216504 ,  0.300465  ]]], dtype=float32), array([ 0.1333826 , -0.14659855, -0.4282664 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(regressor.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[-0.17078312, -0.01162177, -0.03591012,  0.19387801,\n",
      "         -0.32481307],\n",
      "        [ 0.16891924, -0.7695455 , -0.56307197,  1.39601   ,\n",
      "         -0.32923612],\n",
      "        [ 0.5288803 , -0.8341203 , -0.43261093,  0.5799557 ,\n",
      "          0.45425814]],\n",
      "\n",
      "       [[-0.33561116, -0.08569728, -0.26779363, -0.49795243,\n",
      "         -0.40750745],\n",
      "        [ 0.26649016,  0.1457009 , -0.34818015,  0.37932867,\n",
      "         -0.06315234],\n",
      "        [ 0.5503332 ,  0.18124531,  0.31436446,  0.6260062 ,\n",
      "         -0.46699268]],\n",
      "\n",
      "       [[-0.8385373 ,  0.36709997, -0.0424681 ,  0.14001994,\n",
      "          0.19423723],\n",
      "        [-0.4592173 , -0.05208109, -0.09540946,  0.35911217,\n",
      "         -0.08567636],\n",
      "        [ 1.6873633 , -0.03198437,  0.03508257, -0.4608975 ,\n",
      "          0.38273767]]], dtype=float32), array([-0.09687421,  0.01632302,  0.00679523, -0.03986726, -0.00932922],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(regressor.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.6420811 ],\n",
      "       [-0.78354824],\n",
      "       [-0.74629796],\n",
      "       [ 1.5706308 ],\n",
      "       [-0.1904305 ],\n",
      "       [ 0.18306412],\n",
      "       [-0.40958816],\n",
      "       [ 0.19426268],\n",
      "       [-0.10883781],\n",
      "       [ 0.47509104],\n",
      "       [ 0.26005706],\n",
      "       [ 0.1597869 ],\n",
      "       [-0.2711093 ],\n",
      "       [ 0.12301914],\n",
      "       [-0.32484055],\n",
      "       [-0.02798913],\n",
      "       [ 0.08929146],\n",
      "       [ 0.31533936],\n",
      "       [-0.4294908 ],\n",
      "       [ 0.430727  ],\n",
      "       [-0.2665483 ],\n",
      "       [-0.02864041],\n",
      "       [ 0.41853437],\n",
      "       [-0.3665342 ],\n",
      "       [-0.33480895],\n",
      "       [-1.8840692 ],\n",
      "       [ 0.06529546],\n",
      "       [-0.29592603],\n",
      "       [ 0.5489545 ],\n",
      "       [-0.22379196]], dtype=float32), array([0.01779994], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(regressor.layers[3].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
