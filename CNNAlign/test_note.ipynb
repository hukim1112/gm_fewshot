{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_codes import data, train, model\n",
    "import json, os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('overfit.json') as fp:\n",
    "    config = json.load(fp)\n",
    "split = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.synthesize_image_pair(config, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_examples': 1, 'learning_rate': 0.0001, 'batch_size': 1, 'epochs': 400, 'print_step': 10}\n"
     ]
    }
   ],
   "source": [
    "print(config['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_desc': '200507_regressor_filter_size_up',\n",
       " 'dataset_name': 'mini_imagenet',\n",
       " 'data_dir': '/home/files/datasets/mini_imagenet',\n",
       " 'image_shape': [64, 64, 3],\n",
       " 'model_name': 'CNNgeo',\n",
       " 'backbone': 'prototypical_network',\n",
       " 'data': {'method': 'synthesized_pair',\n",
       "  'tps_random_rate': 0.2,\n",
       "  'pad_ratio': 0.2},\n",
       " 'ckpt': {'save_type': 'latest', 'max_to_keep': 10},\n",
       " 'train': {'n_examples': 1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'batch_size': 1,\n",
       "  'epochs': 400,\n",
       "  'print_step': 10},\n",
       " 'val': {'n_examples': 1, 'batch_size': 1},\n",
       " 'test': {'n_examples': -1, 'batch_size': 128}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['train']['n_example'] = 1000\n",
    "config['train']['batch_size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['backbone'] = 'vgg16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset amount : 1\n",
      "val dataset amount : 1\n",
      "start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.5259109139442444\n",
      "Epoch 1, Loss: 0.5259109139442444, Val Loss: 0.5882648229598999\n",
      "end of epoch.\n",
      "start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.6186420917510986\n",
      "Epoch 2, Loss: 0.6186420917510986, Val Loss: 0.5396715998649597\n",
      "end of epoch.\n",
      "start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.5759605169296265\n",
      "Epoch 3, Loss: 0.5759605169296265, Val Loss: 0.5549355745315552\n",
      "end of epoch.\n",
      "start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.44434577226638794\n",
      "Epoch 4, Loss: 0.44434577226638794, Val Loss: 0.5108582973480225\n",
      "end of epoch.\n",
      "start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.5074084401130676\n",
      "Epoch 5, Loss: 0.5074084401130676, Val Loss: 0.5163044333457947\n",
      "end of epoch.\n",
      "start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.4833396077156067\n",
      "Epoch 6, Loss: 0.4833396077156067, Val Loss: 0.5339317321777344\n",
      "end of epoch.\n",
      "start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.4657882750034332\n",
      "Epoch 7, Loss: 0.4657882750034332, Val Loss: 0.4974561035633087\n",
      "end of epoch.\n",
      "start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.49416229128837585\n",
      "Epoch 8, Loss: 0.49416229128837585, Val Loss: 0.5366700887680054\n",
      "end of epoch.\n",
      "start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.45581942796707153\n",
      "Epoch 9, Loss: 0.45581942796707153, Val Loss: 0.5601924657821655\n",
      "end of epoch.\n",
      "start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.5352137684822083\n",
      "Epoch 10, Loss: 0.5352137684822083, Val Loss: 0.4870261251926422\n",
      "end of epoch.\n",
      "start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.579836368560791\n",
      "Epoch 11, Loss: 0.579836368560791, Val Loss: 0.5281113386154175\n",
      "end of epoch.\n",
      "start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.5344333648681641\n",
      "Epoch 12, Loss: 0.5344333648681641, Val Loss: 0.40391334891319275\n",
      "end of epoch.\n",
      "start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.4166235625743866\n",
      "Epoch 13, Loss: 0.4166235625743866, Val Loss: 0.5468930602073669\n",
      "end of epoch.\n",
      "start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.4091617465019226\n",
      "Epoch 14, Loss: 0.4091617465019226, Val Loss: 0.5001193881034851\n",
      "end of epoch.\n",
      "start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.44167160987854004\n",
      "Epoch 15, Loss: 0.44167160987854004, Val Loss: 0.5801717042922974\n",
      "end of epoch.\n",
      "start of epoch 16\n",
      "Training loss (for one batch) at step 0: 0.48087695240974426\n",
      "Epoch 16, Loss: 0.48087695240974426, Val Loss: 0.4289611279964447\n",
      "end of epoch.\n",
      "start of epoch 17\n",
      "Training loss (for one batch) at step 0: 0.6148805022239685\n",
      "Epoch 17, Loss: 0.6148805022239685, Val Loss: 0.6098549365997314\n",
      "end of epoch.\n",
      "start of epoch 18\n",
      "Training loss (for one batch) at step 0: 0.6137447953224182\n",
      "Epoch 18, Loss: 0.6137447953224182, Val Loss: 0.41004496812820435\n",
      "end of epoch.\n",
      "start of epoch 19\n",
      "Training loss (for one batch) at step 0: 0.5126239061355591\n",
      "Epoch 19, Loss: 0.5126239061355591, Val Loss: 0.48475906252861023\n",
      "end of epoch.\n",
      "start of epoch 20\n",
      "Training loss (for one batch) at step 0: 0.5536010265350342\n",
      "Epoch 20, Loss: 0.5536010265350342, Val Loss: 0.5072230696678162\n",
      "end of epoch.\n",
      "start of epoch 21\n",
      "Training loss (for one batch) at step 0: 0.50986248254776\n",
      "Epoch 21, Loss: 0.50986248254776, Val Loss: 0.5603395104408264\n",
      "end of epoch.\n",
      "start of epoch 22\n",
      "Training loss (for one batch) at step 0: 0.44804084300994873\n",
      "Epoch 22, Loss: 0.44804084300994873, Val Loss: 0.5389767289161682\n",
      "end of epoch.\n",
      "start of epoch 23\n",
      "Training loss (for one batch) at step 0: 0.414127916097641\n",
      "Epoch 23, Loss: 0.414127916097641, Val Loss: 0.49903419613838196\n",
      "end of epoch.\n",
      "start of epoch 24\n",
      "Training loss (for one batch) at step 0: 0.527799129486084\n",
      "Epoch 24, Loss: 0.527799129486084, Val Loss: 0.4477940797805786\n",
      "end of epoch.\n",
      "start of epoch 25\n",
      "Training loss (for one batch) at step 0: 0.41241803765296936\n",
      "Epoch 25, Loss: 0.41241803765296936, Val Loss: 0.4989902377128601\n",
      "end of epoch.\n",
      "start of epoch 26\n",
      "Training loss (for one batch) at step 0: 0.48704978823661804\n",
      "Epoch 26, Loss: 0.48704978823661804, Val Loss: 0.5025373101234436\n",
      "end of epoch.\n",
      "start of epoch 27\n",
      "Training loss (for one batch) at step 0: 0.4264254868030548\n",
      "Epoch 27, Loss: 0.4264254868030548, Val Loss: 0.48053106665611267\n",
      "end of epoch.\n",
      "start of epoch 28\n",
      "Training loss (for one batch) at step 0: 0.4084126353263855\n",
      "Epoch 28, Loss: 0.4084126353263855, Val Loss: 0.36240994930267334\n",
      "end of epoch.\n",
      "start of epoch 29\n",
      "Training loss (for one batch) at step 0: 0.4213658273220062\n",
      "Epoch 29, Loss: 0.4213658273220062, Val Loss: 0.483390212059021\n",
      "end of epoch.\n",
      "start of epoch 30\n",
      "Training loss (for one batch) at step 0: 0.4611501693725586\n",
      "Epoch 30, Loss: 0.4611501693725586, Val Loss: 0.42584228515625\n",
      "end of epoch.\n",
      "start of epoch 31\n",
      "Training loss (for one batch) at step 0: 0.46744340658187866\n",
      "Epoch 31, Loss: 0.46744340658187866, Val Loss: 0.5213100910186768\n",
      "end of epoch.\n",
      "start of epoch 32\n",
      "Training loss (for one batch) at step 0: 0.517216682434082\n",
      "Epoch 32, Loss: 0.517216682434082, Val Loss: 0.5105703473091125\n",
      "end of epoch.\n",
      "start of epoch 33\n",
      "Training loss (for one batch) at step 0: 0.5509138703346252\n",
      "Epoch 33, Loss: 0.5509138703346252, Val Loss: 0.5637243390083313\n",
      "end of epoch.\n",
      "start of epoch 34\n",
      "Training loss (for one batch) at step 0: 0.5025665163993835\n",
      "Epoch 34, Loss: 0.5025665163993835, Val Loss: 0.48601770401000977\n",
      "end of epoch.\n",
      "start of epoch 35\n",
      "Training loss (for one batch) at step 0: 0.45283442735671997\n",
      "Epoch 35, Loss: 0.45283442735671997, Val Loss: 0.41664397716522217\n",
      "end of epoch.\n",
      "start of epoch 36\n",
      "Training loss (for one batch) at step 0: 0.49424803256988525\n",
      "Epoch 36, Loss: 0.49424803256988525, Val Loss: 0.4983881115913391\n",
      "end of epoch.\n",
      "start of epoch 37\n",
      "Training loss (for one batch) at step 0: 0.5419705510139465\n",
      "Epoch 37, Loss: 0.5419705510139465, Val Loss: 0.5108944773674011\n",
      "end of epoch.\n",
      "start of epoch 38\n",
      "Training loss (for one batch) at step 0: 0.5929324626922607\n",
      "Epoch 38, Loss: 0.5929324626922607, Val Loss: 0.5410434603691101\n",
      "end of epoch.\n",
      "start of epoch 39\n",
      "Training loss (for one batch) at step 0: 0.49152567982673645\n",
      "Epoch 39, Loss: 0.49152567982673645, Val Loss: 0.45070022344589233\n",
      "end of epoch.\n",
      "start of epoch 40\n",
      "Training loss (for one batch) at step 0: 0.36393043398857117\n",
      "Epoch 40, Loss: 0.36393043398857117, Val Loss: 0.5634922981262207\n",
      "end of epoch.\n",
      "start of epoch 41\n",
      "Training loss (for one batch) at step 0: 0.5908573865890503\n",
      "Epoch 41, Loss: 0.5908573865890503, Val Loss: 0.5293613076210022\n",
      "end of epoch.\n",
      "start of epoch 42\n",
      "Training loss (for one batch) at step 0: 0.43312373757362366\n",
      "Epoch 42, Loss: 0.43312373757362366, Val Loss: 0.5057220458984375\n",
      "end of epoch.\n",
      "start of epoch 43\n",
      "Training loss (for one batch) at step 0: 0.4980464577674866\n",
      "Epoch 43, Loss: 0.4980464577674866, Val Loss: 0.49967533349990845\n",
      "end of epoch.\n",
      "start of epoch 44\n",
      "Training loss (for one batch) at step 0: 0.5783289670944214\n",
      "Epoch 44, Loss: 0.5783289670944214, Val Loss: 0.609084963798523\n",
      "end of epoch.\n",
      "start of epoch 45\n",
      "Training loss (for one batch) at step 0: 0.5453442931175232\n",
      "Epoch 45, Loss: 0.5453442931175232, Val Loss: 0.5007545948028564\n",
      "end of epoch.\n",
      "start of epoch 46\n",
      "Training loss (for one batch) at step 0: 0.5728758573532104\n",
      "Epoch 46, Loss: 0.5728758573532104, Val Loss: 0.5256050825119019\n",
      "end of epoch.\n",
      "start of epoch 47\n",
      "Training loss (for one batch) at step 0: 0.5437003970146179\n",
      "Epoch 47, Loss: 0.5437003970146179, Val Loss: 0.5128471255302429\n",
      "end of epoch.\n",
      "start of epoch 48\n",
      "Training loss (for one batch) at step 0: 0.49956050515174866\n",
      "Epoch 48, Loss: 0.49956050515174866, Val Loss: 0.4163610339164734\n",
      "end of epoch.\n",
      "start of epoch 49\n",
      "Training loss (for one batch) at step 0: 0.523560106754303\n",
      "Epoch 49, Loss: 0.523560106754303, Val Loss: 0.35926002264022827\n",
      "end of epoch.\n",
      "start of epoch 50\n",
      "Training loss (for one batch) at step 0: 0.5097792148590088\n",
      "Epoch 50, Loss: 0.5097792148590088, Val Loss: 0.4527265131473541\n",
      "end of epoch.\n",
      "start of epoch 51\n",
      "Training loss (for one batch) at step 0: 0.5936005711555481\n",
      "Epoch 51, Loss: 0.5936005711555481, Val Loss: 0.4094896614551544\n",
      "end of epoch.\n",
      "start of epoch 52\n",
      "Training loss (for one batch) at step 0: 0.5446637272834778\n",
      "Epoch 52, Loss: 0.5446637272834778, Val Loss: 0.5016561150550842\n",
      "end of epoch.\n",
      "start of epoch 53\n",
      "Training loss (for one batch) at step 0: 0.4134191572666168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, Loss: 0.4134191572666168, Val Loss: 0.5832877159118652\n",
      "end of epoch.\n",
      "start of epoch 54\n",
      "Training loss (for one batch) at step 0: 0.46378234028816223\n",
      "Epoch 54, Loss: 0.46378234028816223, Val Loss: 0.479036420583725\n",
      "end of epoch.\n",
      "start of epoch 55\n",
      "Training loss (for one batch) at step 0: 0.40235379338264465\n",
      "Epoch 55, Loss: 0.40235379338264465, Val Loss: 0.5709214806556702\n",
      "end of epoch.\n",
      "start of epoch 56\n",
      "Training loss (for one batch) at step 0: 0.47134658694267273\n",
      "Epoch 56, Loss: 0.47134658694267273, Val Loss: 0.49275630712509155\n",
      "end of epoch.\n",
      "start of epoch 57\n",
      "Training loss (for one batch) at step 0: 0.5199747085571289\n",
      "Epoch 57, Loss: 0.5199747085571289, Val Loss: 0.5157557129859924\n",
      "end of epoch.\n",
      "start of epoch 58\n",
      "Training loss (for one batch) at step 0: 0.5352789163589478\n",
      "Epoch 58, Loss: 0.5352789163589478, Val Loss: 0.4509506821632385\n",
      "end of epoch.\n",
      "start of epoch 59\n",
      "Training loss (for one batch) at step 0: 0.5416530966758728\n",
      "Epoch 59, Loss: 0.5416530966758728, Val Loss: 0.46308743953704834\n",
      "end of epoch.\n",
      "start of epoch 60\n",
      "Training loss (for one batch) at step 0: 0.4401344954967499\n",
      "Epoch 60, Loss: 0.4401344954967499, Val Loss: 0.5818665623664856\n",
      "end of epoch.\n",
      "start of epoch 61\n",
      "Training loss (for one batch) at step 0: 0.5037944912910461\n",
      "Epoch 61, Loss: 0.5037944912910461, Val Loss: 0.464003324508667\n",
      "end of epoch.\n",
      "start of epoch 62\n",
      "Training loss (for one batch) at step 0: 0.49791064858436584\n",
      "Epoch 62, Loss: 0.49791064858436584, Val Loss: 0.45086854696273804\n",
      "end of epoch.\n",
      "start of epoch 63\n",
      "Training loss (for one batch) at step 0: 0.5322582721710205\n",
      "Epoch 63, Loss: 0.5322582721710205, Val Loss: 0.5372931361198425\n",
      "end of epoch.\n",
      "start of epoch 64\n",
      "Training loss (for one batch) at step 0: 0.4893013834953308\n",
      "Epoch 64, Loss: 0.4893013834953308, Val Loss: 0.5046140551567078\n",
      "end of epoch.\n",
      "start of epoch 65\n",
      "Training loss (for one batch) at step 0: 0.5291630029678345\n",
      "Epoch 65, Loss: 0.5291630029678345, Val Loss: 0.43183013796806335\n",
      "end of epoch.\n",
      "start of epoch 66\n",
      "Training loss (for one batch) at step 0: 0.4579966068267822\n",
      "Epoch 66, Loss: 0.4579966068267822, Val Loss: 0.5281824469566345\n",
      "end of epoch.\n",
      "start of epoch 67\n",
      "Training loss (for one batch) at step 0: 0.47237715125083923\n",
      "Epoch 67, Loss: 0.47237715125083923, Val Loss: 0.48925238847732544\n",
      "end of epoch.\n",
      "start of epoch 68\n",
      "Training loss (for one batch) at step 0: 0.5796537399291992\n",
      "Epoch 68, Loss: 0.5796537399291992, Val Loss: 0.6005745530128479\n",
      "end of epoch.\n",
      "start of epoch 69\n",
      "Training loss (for one batch) at step 0: 0.44155195355415344\n",
      "Epoch 69, Loss: 0.44155195355415344, Val Loss: 0.45127183198928833\n",
      "end of epoch.\n",
      "start of epoch 70\n",
      "Training loss (for one batch) at step 0: 0.5499839186668396\n",
      "Epoch 70, Loss: 0.5499839186668396, Val Loss: 0.4581908881664276\n",
      "end of epoch.\n",
      "start of epoch 71\n",
      "Training loss (for one batch) at step 0: 0.44905999302864075\n",
      "Epoch 71, Loss: 0.44905999302864075, Val Loss: 0.4774504601955414\n",
      "end of epoch.\n",
      "start of epoch 72\n",
      "Training loss (for one batch) at step 0: 0.5205867290496826\n",
      "Epoch 72, Loss: 0.5205867290496826, Val Loss: 0.5032450556755066\n",
      "end of epoch.\n",
      "start of epoch 73\n",
      "Training loss (for one batch) at step 0: 0.49858739972114563\n",
      "Epoch 73, Loss: 0.49858739972114563, Val Loss: 0.5455206036567688\n",
      "end of epoch.\n",
      "start of epoch 74\n",
      "Training loss (for one batch) at step 0: 0.4498172700405121\n",
      "Epoch 74, Loss: 0.4498172700405121, Val Loss: 0.4660765826702118\n",
      "end of epoch.\n",
      "start of epoch 75\n",
      "Training loss (for one batch) at step 0: 0.5551567077636719\n",
      "Epoch 75, Loss: 0.5551567077636719, Val Loss: 0.5801194906234741\n",
      "end of epoch.\n",
      "start of epoch 76\n",
      "Training loss (for one batch) at step 0: 0.5490785241127014\n",
      "Epoch 76, Loss: 0.5490785241127014, Val Loss: 0.5156817436218262\n",
      "end of epoch.\n",
      "start of epoch 77\n",
      "Training loss (for one batch) at step 0: 0.4947887361049652\n",
      "Epoch 77, Loss: 0.4947887361049652, Val Loss: 0.5111813545227051\n",
      "end of epoch.\n",
      "start of epoch 78\n",
      "Training loss (for one batch) at step 0: 0.46575289964675903\n",
      "Epoch 78, Loss: 0.46575289964675903, Val Loss: 0.45965591073036194\n",
      "end of epoch.\n",
      "start of epoch 79\n",
      "Training loss (for one batch) at step 0: 0.5505128502845764\n",
      "Epoch 79, Loss: 0.5505128502845764, Val Loss: 0.4233418107032776\n",
      "end of epoch.\n",
      "start of epoch 80\n",
      "Training loss (for one batch) at step 0: 0.5749967098236084\n",
      "Epoch 80, Loss: 0.5749967098236084, Val Loss: 0.5573500990867615\n",
      "end of epoch.\n",
      "start of epoch 81\n",
      "Training loss (for one batch) at step 0: 0.5244699120521545\n",
      "Epoch 81, Loss: 0.5244699120521545, Val Loss: 0.522492527961731\n",
      "end of epoch.\n",
      "start of epoch 82\n",
      "Training loss (for one batch) at step 0: 0.3815120756626129\n",
      "Epoch 82, Loss: 0.3815120756626129, Val Loss: 0.5756731629371643\n",
      "end of epoch.\n",
      "start of epoch 83\n",
      "Training loss (for one batch) at step 0: 0.4083259403705597\n",
      "Epoch 83, Loss: 0.4083259403705597, Val Loss: 0.4627465009689331\n",
      "end of epoch.\n",
      "start of epoch 84\n",
      "Training loss (for one batch) at step 0: 0.5021539330482483\n",
      "Epoch 84, Loss: 0.5021539330482483, Val Loss: 0.5187751650810242\n",
      "end of epoch.\n",
      "start of epoch 85\n",
      "Training loss (for one batch) at step 0: 0.5036977529525757\n",
      "Epoch 85, Loss: 0.5036977529525757, Val Loss: 0.4792270064353943\n",
      "end of epoch.\n",
      "start of epoch 86\n",
      "Training loss (for one batch) at step 0: 0.4717458188533783\n",
      "Epoch 86, Loss: 0.4717458188533783, Val Loss: 0.47441813349723816\n",
      "end of epoch.\n",
      "start of epoch 87\n",
      "Training loss (for one batch) at step 0: 0.41959622502326965\n",
      "Epoch 87, Loss: 0.41959622502326965, Val Loss: 0.38594377040863037\n",
      "end of epoch.\n",
      "start of epoch 88\n",
      "Training loss (for one batch) at step 0: 0.5331229567527771\n",
      "Epoch 88, Loss: 0.5331229567527771, Val Loss: 0.516667902469635\n",
      "end of epoch.\n",
      "start of epoch 89\n",
      "Training loss (for one batch) at step 0: 0.46681836247444153\n",
      "Epoch 89, Loss: 0.46681836247444153, Val Loss: 0.4810682237148285\n",
      "end of epoch.\n",
      "start of epoch 90\n",
      "Training loss (for one batch) at step 0: 0.5010314583778381\n",
      "Epoch 90, Loss: 0.5010314583778381, Val Loss: 0.5000938773155212\n",
      "end of epoch.\n",
      "start of epoch 91\n",
      "Training loss (for one batch) at step 0: 0.535997211933136\n",
      "Epoch 91, Loss: 0.535997211933136, Val Loss: 0.46211934089660645\n",
      "end of epoch.\n",
      "start of epoch 92\n",
      "Training loss (for one batch) at step 0: 0.5618405342102051\n",
      "Epoch 92, Loss: 0.5618405342102051, Val Loss: 0.5210311412811279\n",
      "end of epoch.\n",
      "start of epoch 93\n",
      "Training loss (for one batch) at step 0: 0.5180303454399109\n",
      "Epoch 93, Loss: 0.5180303454399109, Val Loss: 0.5149676203727722\n",
      "end of epoch.\n",
      "start of epoch 94\n",
      "Training loss (for one batch) at step 0: 0.5302421450614929\n",
      "Epoch 94, Loss: 0.5302421450614929, Val Loss: 0.5818666815757751\n",
      "end of epoch.\n",
      "start of epoch 95\n",
      "Training loss (for one batch) at step 0: 0.48330655694007874\n",
      "Epoch 95, Loss: 0.48330655694007874, Val Loss: 0.5250561833381653\n",
      "end of epoch.\n",
      "start of epoch 96\n",
      "Training loss (for one batch) at step 0: 0.5874763131141663\n",
      "Epoch 96, Loss: 0.5874763131141663, Val Loss: 0.5470921397209167\n",
      "end of epoch.\n",
      "start of epoch 97\n",
      "Training loss (for one batch) at step 0: 0.5464091300964355\n",
      "Epoch 97, Loss: 0.5464091300964355, Val Loss: 0.4846591055393219\n",
      "end of epoch.\n",
      "start of epoch 98\n",
      "Training loss (for one batch) at step 0: 0.5065885186195374\n",
      "Epoch 98, Loss: 0.5065885186195374, Val Loss: 0.5051849484443665\n",
      "end of epoch.\n",
      "start of epoch 99\n",
      "Training loss (for one batch) at step 0: 0.4948667287826538\n",
      "Epoch 99, Loss: 0.4948667287826538, Val Loss: 0.47851601243019104\n",
      "end of epoch.\n",
      "start of epoch 100\n",
      "Training loss (for one batch) at step 0: 0.5326192378997803\n",
      "Epoch 100, Loss: 0.5326192378997803, Val Loss: 0.4363463819026947\n",
      "end of epoch.\n",
      "start of epoch 101\n",
      "Training loss (for one batch) at step 0: 0.4859625995159149\n",
      "Epoch 101, Loss: 0.4859625995159149, Val Loss: 0.5731387734413147\n",
      "end of epoch.\n",
      "start of epoch 102\n",
      "Training loss (for one batch) at step 0: 0.37052735686302185\n",
      "Epoch 102, Loss: 0.37052735686302185, Val Loss: 0.4376712739467621\n",
      "end of epoch.\n",
      "start of epoch 103\n",
      "Training loss (for one batch) at step 0: 0.43224379420280457\n",
      "Epoch 103, Loss: 0.43224379420280457, Val Loss: 0.5447693467140198\n",
      "end of epoch.\n",
      "start of epoch 104\n",
      "Training loss (for one batch) at step 0: 0.4269341826438904\n",
      "Epoch 104, Loss: 0.4269341826438904, Val Loss: 0.5177900195121765\n",
      "end of epoch.\n",
      "start of epoch 105\n",
      "Training loss (for one batch) at step 0: 0.4052734971046448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105, Loss: 0.4052734971046448, Val Loss: 0.5096699595451355\n",
      "end of epoch.\n",
      "start of epoch 106\n",
      "Training loss (for one batch) at step 0: 0.49541041254997253\n",
      "Epoch 106, Loss: 0.49541041254997253, Val Loss: 0.44630005955696106\n",
      "end of epoch.\n",
      "start of epoch 107\n",
      "Training loss (for one batch) at step 0: 0.49658292531967163\n",
      "Epoch 107, Loss: 0.49658292531967163, Val Loss: 0.49110811948776245\n",
      "end of epoch.\n",
      "start of epoch 108\n",
      "Training loss (for one batch) at step 0: 0.42752885818481445\n",
      "Epoch 108, Loss: 0.42752885818481445, Val Loss: 0.4902026355266571\n",
      "end of epoch.\n",
      "start of epoch 109\n",
      "Training loss (for one batch) at step 0: 0.4178595244884491\n",
      "Epoch 109, Loss: 0.4178595244884491, Val Loss: 0.5234072804450989\n",
      "end of epoch.\n",
      "start of epoch 110\n",
      "Training loss (for one batch) at step 0: 0.5142196416854858\n",
      "Epoch 110, Loss: 0.5142196416854858, Val Loss: 0.4992355704307556\n",
      "end of epoch.\n",
      "start of epoch 111\n",
      "Training loss (for one batch) at step 0: 0.5169783234596252\n",
      "Epoch 111, Loss: 0.5169783234596252, Val Loss: 0.4172971248626709\n",
      "end of epoch.\n",
      "start of epoch 112\n",
      "Training loss (for one batch) at step 0: 0.5041085481643677\n",
      "Epoch 112, Loss: 0.5041085481643677, Val Loss: 0.46182751655578613\n",
      "end of epoch.\n",
      "start of epoch 113\n",
      "Training loss (for one batch) at step 0: 0.4964927136898041\n",
      "Epoch 113, Loss: 0.4964927136898041, Val Loss: 0.5102075934410095\n",
      "end of epoch.\n",
      "start of epoch 114\n",
      "Training loss (for one batch) at step 0: 0.44641101360321045\n",
      "Epoch 114, Loss: 0.44641101360321045, Val Loss: 0.5021998286247253\n",
      "end of epoch.\n",
      "start of epoch 115\n",
      "Training loss (for one batch) at step 0: 0.4722141623497009\n",
      "Epoch 115, Loss: 0.4722141623497009, Val Loss: 0.5296007990837097\n",
      "end of epoch.\n",
      "start of epoch 116\n",
      "Training loss (for one batch) at step 0: 0.5086206793785095\n",
      "Epoch 116, Loss: 0.5086206793785095, Val Loss: 0.4800788462162018\n",
      "end of epoch.\n",
      "start of epoch 117\n",
      "Training loss (for one batch) at step 0: 0.46820855140686035\n",
      "Epoch 117, Loss: 0.46820855140686035, Val Loss: 0.4927985668182373\n",
      "end of epoch.\n",
      "start of epoch 118\n",
      "Training loss (for one batch) at step 0: 0.3956966996192932\n",
      "Epoch 118, Loss: 0.3956966996192932, Val Loss: 0.5169363021850586\n",
      "end of epoch.\n",
      "start of epoch 119\n",
      "Training loss (for one batch) at step 0: 0.49957454204559326\n",
      "Epoch 119, Loss: 0.49957454204559326, Val Loss: 0.3903478980064392\n",
      "end of epoch.\n",
      "start of epoch 120\n",
      "Training loss (for one batch) at step 0: 0.492931604385376\n",
      "Epoch 120, Loss: 0.492931604385376, Val Loss: 0.41942140460014343\n",
      "end of epoch.\n",
      "start of epoch 121\n",
      "Training loss (for one batch) at step 0: 0.5178324580192566\n",
      "Epoch 121, Loss: 0.5178324580192566, Val Loss: 0.48001599311828613\n",
      "end of epoch.\n",
      "start of epoch 122\n",
      "Training loss (for one batch) at step 0: 0.47986236214637756\n",
      "Epoch 122, Loss: 0.47986236214637756, Val Loss: 0.4179912209510803\n",
      "end of epoch.\n",
      "start of epoch 123\n",
      "Training loss (for one batch) at step 0: 0.5054015517234802\n",
      "Epoch 123, Loss: 0.5054015517234802, Val Loss: 0.4991646409034729\n",
      "end of epoch.\n",
      "start of epoch 124\n",
      "Training loss (for one batch) at step 0: 0.37812483310699463\n",
      "Epoch 124, Loss: 0.37812483310699463, Val Loss: 0.40028849244117737\n",
      "end of epoch.\n",
      "start of epoch 125\n",
      "Training loss (for one batch) at step 0: 0.44466298818588257\n",
      "Epoch 125, Loss: 0.44466298818588257, Val Loss: 0.5543782114982605\n",
      "end of epoch.\n",
      "start of epoch 126\n",
      "Training loss (for one batch) at step 0: 0.4821605384349823\n",
      "Epoch 126, Loss: 0.4821605384349823, Val Loss: 0.44377171993255615\n",
      "end of epoch.\n",
      "start of epoch 127\n",
      "Training loss (for one batch) at step 0: 0.5110556483268738\n",
      "Epoch 127, Loss: 0.5110556483268738, Val Loss: 0.44296514987945557\n",
      "end of epoch.\n",
      "start of epoch 128\n",
      "Training loss (for one batch) at step 0: 0.45988020300865173\n",
      "Epoch 128, Loss: 0.45988020300865173, Val Loss: 0.5227657556533813\n",
      "end of epoch.\n",
      "start of epoch 129\n",
      "Training loss (for one batch) at step 0: 0.4163350462913513\n",
      "Epoch 129, Loss: 0.4163350462913513, Val Loss: 0.562690258026123\n",
      "end of epoch.\n",
      "start of epoch 130\n",
      "Training loss (for one batch) at step 0: 0.566726803779602\n",
      "Epoch 130, Loss: 0.566726803779602, Val Loss: 0.4839479327201843\n",
      "end of epoch.\n",
      "start of epoch 131\n",
      "Training loss (for one batch) at step 0: 0.5170614123344421\n",
      "Epoch 131, Loss: 0.5170614123344421, Val Loss: 0.49287518858909607\n",
      "end of epoch.\n",
      "start of epoch 132\n",
      "Training loss (for one batch) at step 0: 0.4832027554512024\n",
      "Epoch 132, Loss: 0.4832027554512024, Val Loss: 0.46368297934532166\n",
      "end of epoch.\n",
      "start of epoch 133\n",
      "Training loss (for one batch) at step 0: 0.4914853870868683\n",
      "Epoch 133, Loss: 0.4914853870868683, Val Loss: 0.49866509437561035\n",
      "end of epoch.\n",
      "start of epoch 134\n",
      "Training loss (for one batch) at step 0: 0.5340689420700073\n",
      "Epoch 134, Loss: 0.5340689420700073, Val Loss: 0.5527241826057434\n",
      "end of epoch.\n",
      "start of epoch 135\n",
      "Training loss (for one batch) at step 0: 0.4691358804702759\n",
      "Epoch 135, Loss: 0.4691358804702759, Val Loss: 0.5524556040763855\n",
      "end of epoch.\n",
      "start of epoch 136\n",
      "Training loss (for one batch) at step 0: 0.544419527053833\n",
      "Epoch 136, Loss: 0.544419527053833, Val Loss: 0.5209978222846985\n",
      "end of epoch.\n",
      "start of epoch 137\n",
      "Training loss (for one batch) at step 0: 0.43422985076904297\n",
      "Epoch 137, Loss: 0.43422985076904297, Val Loss: 0.5557643175125122\n",
      "end of epoch.\n",
      "start of epoch 138\n",
      "Training loss (for one batch) at step 0: 0.6247370839118958\n",
      "Epoch 138, Loss: 0.6247370839118958, Val Loss: 0.5078300833702087\n",
      "end of epoch.\n",
      "start of epoch 139\n",
      "Training loss (for one batch) at step 0: 0.3945881128311157\n",
      "Epoch 139, Loss: 0.3945881128311157, Val Loss: 0.36220988631248474\n",
      "end of epoch.\n",
      "start of epoch 140\n",
      "Training loss (for one batch) at step 0: 0.5281266570091248\n",
      "Epoch 140, Loss: 0.5281266570091248, Val Loss: 0.37855201959609985\n",
      "end of epoch.\n",
      "start of epoch 141\n",
      "Training loss (for one batch) at step 0: 0.5226481556892395\n",
      "Epoch 141, Loss: 0.5226481556892395, Val Loss: 0.5312230587005615\n",
      "end of epoch.\n",
      "start of epoch 142\n",
      "Training loss (for one batch) at step 0: 0.4772737920284271\n",
      "Epoch 142, Loss: 0.4772737920284271, Val Loss: 0.5208809971809387\n",
      "end of epoch.\n",
      "start of epoch 143\n",
      "Training loss (for one batch) at step 0: 0.4805710017681122\n",
      "Epoch 143, Loss: 0.4805710017681122, Val Loss: 0.4517260789871216\n",
      "end of epoch.\n",
      "start of epoch 144\n",
      "Training loss (for one batch) at step 0: 0.44874536991119385\n",
      "Epoch 144, Loss: 0.44874536991119385, Val Loss: 0.4915498197078705\n",
      "end of epoch.\n",
      "start of epoch 145\n",
      "Training loss (for one batch) at step 0: 0.46275728940963745\n",
      "Epoch 145, Loss: 0.46275728940963745, Val Loss: 0.4392877519130707\n",
      "end of epoch.\n",
      "start of epoch 146\n",
      "Training loss (for one batch) at step 0: 0.5021936893463135\n",
      "Epoch 146, Loss: 0.5021936893463135, Val Loss: 0.48938363790512085\n",
      "end of epoch.\n",
      "start of epoch 147\n",
      "Training loss (for one batch) at step 0: 0.5886852145195007\n",
      "Epoch 147, Loss: 0.5886852145195007, Val Loss: 0.5282180905342102\n",
      "end of epoch.\n",
      "start of epoch 148\n",
      "Training loss (for one batch) at step 0: 0.4890093207359314\n",
      "Epoch 148, Loss: 0.4890093207359314, Val Loss: 0.5121042728424072\n",
      "end of epoch.\n",
      "start of epoch 149\n",
      "Training loss (for one batch) at step 0: 0.5150889158248901\n",
      "Epoch 149, Loss: 0.5150889158248901, Val Loss: 0.5000100135803223\n",
      "end of epoch.\n",
      "start of epoch 150\n",
      "Training loss (for one batch) at step 0: 0.5795422792434692\n",
      "Epoch 150, Loss: 0.5795422792434692, Val Loss: 0.5101777911186218\n",
      "end of epoch.\n",
      "start of epoch 151\n",
      "Training loss (for one batch) at step 0: 0.5026378035545349\n",
      "Epoch 151, Loss: 0.5026378035545349, Val Loss: 0.4851627051830292\n",
      "end of epoch.\n",
      "start of epoch 152\n",
      "Training loss (for one batch) at step 0: 0.5215135216712952\n",
      "Epoch 152, Loss: 0.5215135216712952, Val Loss: 0.4024561643600464\n",
      "end of epoch.\n",
      "start of epoch 153\n",
      "Training loss (for one batch) at step 0: 0.42697617411613464\n",
      "Epoch 153, Loss: 0.42697617411613464, Val Loss: 0.5095655918121338\n",
      "end of epoch.\n",
      "start of epoch 154\n",
      "Training loss (for one batch) at step 0: 0.5043278932571411\n",
      "Epoch 154, Loss: 0.5043278932571411, Val Loss: 0.5105284452438354\n",
      "end of epoch.\n",
      "start of epoch 155\n",
      "Training loss (for one batch) at step 0: 0.5305460691452026\n",
      "Epoch 155, Loss: 0.5305460691452026, Val Loss: 0.463594913482666\n",
      "end of epoch.\n",
      "start of epoch 156\n",
      "Training loss (for one batch) at step 0: 0.4417300522327423\n",
      "Epoch 156, Loss: 0.4417300522327423, Val Loss: 0.5003660917282104\n",
      "end of epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of epoch 157\n",
      "Training loss (for one batch) at step 0: 0.5407811403274536\n",
      "Epoch 157, Loss: 0.5407811403274536, Val Loss: 0.577216386795044\n",
      "end of epoch.\n",
      "start of epoch 158\n",
      "Training loss (for one batch) at step 0: 0.3666224479675293\n",
      "Epoch 158, Loss: 0.3666224479675293, Val Loss: 0.4047715365886688\n",
      "end of epoch.\n",
      "start of epoch 159\n",
      "Training loss (for one batch) at step 0: 0.5437006950378418\n",
      "Epoch 159, Loss: 0.5437006950378418, Val Loss: 0.4410718083381653\n",
      "end of epoch.\n",
      "start of epoch 160\n",
      "Training loss (for one batch) at step 0: 0.4490707516670227\n",
      "Epoch 160, Loss: 0.4490707516670227, Val Loss: 0.5634963512420654\n",
      "end of epoch.\n",
      "start of epoch 161\n",
      "Training loss (for one batch) at step 0: 0.4130170941352844\n",
      "Epoch 161, Loss: 0.4130170941352844, Val Loss: 0.46923500299453735\n",
      "end of epoch.\n",
      "start of epoch 162\n",
      "Training loss (for one batch) at step 0: 0.42010122537612915\n",
      "Epoch 162, Loss: 0.42010122537612915, Val Loss: 0.43034517765045166\n",
      "end of epoch.\n",
      "start of epoch 163\n",
      "Training loss (for one batch) at step 0: 0.4550358057022095\n",
      "Epoch 163, Loss: 0.4550358057022095, Val Loss: 0.5433237552642822\n",
      "end of epoch.\n",
      "start of epoch 164\n",
      "Training loss (for one batch) at step 0: 0.39617523550987244\n",
      "Epoch 164, Loss: 0.39617523550987244, Val Loss: 0.5107326507568359\n",
      "end of epoch.\n",
      "start of epoch 165\n",
      "Training loss (for one batch) at step 0: 0.4937821924686432\n",
      "Epoch 165, Loss: 0.4937821924686432, Val Loss: 0.43424054980278015\n",
      "end of epoch.\n",
      "start of epoch 166\n",
      "Training loss (for one batch) at step 0: 0.4916906952857971\n",
      "Epoch 166, Loss: 0.4916906952857971, Val Loss: 0.5610958933830261\n",
      "end of epoch.\n",
      "start of epoch 167\n",
      "Training loss (for one batch) at step 0: 0.43200138211250305\n",
      "Epoch 167, Loss: 0.43200138211250305, Val Loss: 0.4830244779586792\n",
      "end of epoch.\n",
      "start of epoch 168\n",
      "Training loss (for one batch) at step 0: 0.510102391242981\n",
      "Epoch 168, Loss: 0.510102391242981, Val Loss: 0.5640086531639099\n",
      "end of epoch.\n",
      "start of epoch 169\n",
      "Training loss (for one batch) at step 0: 0.5481343865394592\n",
      "Epoch 169, Loss: 0.5481343865394592, Val Loss: 0.4995816946029663\n",
      "end of epoch.\n",
      "start of epoch 170\n",
      "Training loss (for one batch) at step 0: 0.4745628237724304\n",
      "Epoch 170, Loss: 0.4745628237724304, Val Loss: 0.4225965738296509\n",
      "end of epoch.\n",
      "start of epoch 171\n",
      "Training loss (for one batch) at step 0: 0.5085899829864502\n",
      "Epoch 171, Loss: 0.5085899829864502, Val Loss: 0.4875052869319916\n",
      "end of epoch.\n",
      "start of epoch 172\n",
      "Training loss (for one batch) at step 0: 0.42658841609954834\n",
      "Epoch 172, Loss: 0.42658841609954834, Val Loss: 0.42738667130470276\n",
      "end of epoch.\n",
      "start of epoch 173\n",
      "Training loss (for one batch) at step 0: 0.5047186017036438\n",
      "Epoch 173, Loss: 0.5047186017036438, Val Loss: 0.43286803364753723\n",
      "end of epoch.\n",
      "start of epoch 174\n",
      "Training loss (for one batch) at step 0: 0.5218592882156372\n",
      "Epoch 174, Loss: 0.5218592882156372, Val Loss: 0.49296778440475464\n",
      "end of epoch.\n",
      "start of epoch 175\n",
      "Training loss (for one batch) at step 0: 0.505813479423523\n",
      "Epoch 175, Loss: 0.505813479423523, Val Loss: 0.5598137974739075\n",
      "end of epoch.\n",
      "start of epoch 176\n",
      "Training loss (for one batch) at step 0: 0.5153529047966003\n",
      "Epoch 176, Loss: 0.5153529047966003, Val Loss: 0.4438736140727997\n",
      "end of epoch.\n",
      "start of epoch 177\n",
      "Training loss (for one batch) at step 0: 0.5052567720413208\n",
      "Epoch 177, Loss: 0.5052567720413208, Val Loss: 0.4908146858215332\n",
      "end of epoch.\n",
      "start of epoch 178\n",
      "Training loss (for one batch) at step 0: 0.5171020030975342\n",
      "Epoch 178, Loss: 0.5171020030975342, Val Loss: 0.4305747151374817\n",
      "end of epoch.\n",
      "start of epoch 179\n",
      "Training loss (for one batch) at step 0: 0.4690544307231903\n",
      "Epoch 179, Loss: 0.4690544307231903, Val Loss: 0.4215415120124817\n",
      "end of epoch.\n",
      "start of epoch 180\n",
      "Training loss (for one batch) at step 0: 0.4795588254928589\n",
      "Epoch 180, Loss: 0.4795588254928589, Val Loss: 0.42973822355270386\n",
      "end of epoch.\n",
      "start of epoch 181\n",
      "Training loss (for one batch) at step 0: 0.40244951844215393\n",
      "Epoch 181, Loss: 0.40244951844215393, Val Loss: 0.4142696261405945\n",
      "end of epoch.\n",
      "start of epoch 182\n",
      "Training loss (for one batch) at step 0: 0.4660401940345764\n",
      "Epoch 182, Loss: 0.4660401940345764, Val Loss: 0.32594725489616394\n",
      "end of epoch.\n",
      "start of epoch 183\n",
      "Training loss (for one batch) at step 0: 0.5021734237670898\n",
      "Epoch 183, Loss: 0.5021734237670898, Val Loss: 0.5282578468322754\n",
      "end of epoch.\n",
      "start of epoch 184\n",
      "Training loss (for one batch) at step 0: 0.46691253781318665\n",
      "Epoch 184, Loss: 0.46691253781318665, Val Loss: 0.554400622844696\n",
      "end of epoch.\n",
      "start of epoch 185\n",
      "Training loss (for one batch) at step 0: 0.4898967444896698\n",
      "Epoch 185, Loss: 0.4898967444896698, Val Loss: 0.5089865922927856\n",
      "end of epoch.\n",
      "start of epoch 186\n",
      "Training loss (for one batch) at step 0: 0.4381346106529236\n",
      "Epoch 186, Loss: 0.4381346106529236, Val Loss: 0.5435816645622253\n",
      "end of epoch.\n",
      "start of epoch 187\n",
      "Training loss (for one batch) at step 0: 0.5259291529655457\n",
      "Epoch 187, Loss: 0.5259291529655457, Val Loss: 0.5087530016899109\n",
      "end of epoch.\n",
      "start of epoch 188\n",
      "Training loss (for one batch) at step 0: 0.4771871268749237\n",
      "Epoch 188, Loss: 0.4771871268749237, Val Loss: 0.5367098450660706\n",
      "end of epoch.\n",
      "start of epoch 189\n",
      "Training loss (for one batch) at step 0: 0.6290923953056335\n",
      "Epoch 189, Loss: 0.6290923953056335, Val Loss: 0.49685725569725037\n",
      "end of epoch.\n",
      "start of epoch 190\n",
      "Training loss (for one batch) at step 0: 0.4127488136291504\n",
      "Epoch 190, Loss: 0.4127488136291504, Val Loss: 0.4575834572315216\n",
      "end of epoch.\n",
      "start of epoch 191\n",
      "Training loss (for one batch) at step 0: 0.47827544808387756\n",
      "Epoch 191, Loss: 0.47827544808387756, Val Loss: 0.5299950242042542\n",
      "end of epoch.\n",
      "start of epoch 192\n",
      "Training loss (for one batch) at step 0: 0.46668967604637146\n",
      "Epoch 192, Loss: 0.46668967604637146, Val Loss: 0.47564712166786194\n",
      "end of epoch.\n",
      "start of epoch 193\n",
      "Training loss (for one batch) at step 0: 0.4706365764141083\n",
      "Epoch 193, Loss: 0.4706365764141083, Val Loss: 0.4315932095050812\n",
      "end of epoch.\n",
      "start of epoch 194\n",
      "Training loss (for one batch) at step 0: 0.4952107071876526\n",
      "Epoch 194, Loss: 0.4952107071876526, Val Loss: 0.5000783801078796\n",
      "end of epoch.\n",
      "start of epoch 195\n",
      "Training loss (for one batch) at step 0: 0.33683618903160095\n",
      "Epoch 195, Loss: 0.33683618903160095, Val Loss: 0.6179997324943542\n",
      "end of epoch.\n",
      "start of epoch 196\n",
      "Training loss (for one batch) at step 0: 0.5049463510513306\n",
      "Epoch 196, Loss: 0.5049463510513306, Val Loss: 0.5159047245979309\n",
      "end of epoch.\n",
      "start of epoch 197\n",
      "Training loss (for one batch) at step 0: 0.5232595801353455\n",
      "Epoch 197, Loss: 0.5232595801353455, Val Loss: 0.5155467391014099\n",
      "end of epoch.\n",
      "start of epoch 198\n",
      "Training loss (for one batch) at step 0: 0.579477071762085\n",
      "Epoch 198, Loss: 0.579477071762085, Val Loss: 0.5622610449790955\n",
      "end of epoch.\n",
      "start of epoch 199\n",
      "Training loss (for one batch) at step 0: 0.48869144916534424\n",
      "Epoch 199, Loss: 0.48869144916534424, Val Loss: 0.49391838908195496\n",
      "end of epoch.\n",
      "start of epoch 200\n",
      "Training loss (for one batch) at step 0: 0.5022179484367371\n",
      "Epoch 200, Loss: 0.5022179484367371, Val Loss: 0.46447819471359253\n",
      "end of epoch.\n",
      "start of epoch 201\n",
      "Training loss (for one batch) at step 0: 0.5645653605461121\n",
      "Epoch 201, Loss: 0.5645653605461121, Val Loss: 0.4392208158969879\n",
      "end of epoch.\n",
      "start of epoch 202\n",
      "Training loss (for one batch) at step 0: 0.5313870310783386\n",
      "Epoch 202, Loss: 0.5313870310783386, Val Loss: 0.43172475695610046\n",
      "end of epoch.\n",
      "start of epoch 203\n",
      "Training loss (for one batch) at step 0: 0.45488661527633667\n",
      "Epoch 203, Loss: 0.45488661527633667, Val Loss: 0.4443184435367584\n",
      "end of epoch.\n",
      "start of epoch 204\n",
      "Training loss (for one batch) at step 0: 0.49211737513542175\n",
      "Epoch 204, Loss: 0.49211737513542175, Val Loss: 0.501278817653656\n",
      "end of epoch.\n",
      "start of epoch 205\n",
      "Training loss (for one batch) at step 0: 0.5276561975479126\n",
      "Epoch 205, Loss: 0.5276561975479126, Val Loss: 0.4195898175239563\n",
      "end of epoch.\n",
      "start of epoch 206\n",
      "Training loss (for one batch) at step 0: 0.519515335559845\n",
      "Epoch 206, Loss: 0.519515335559845, Val Loss: 0.5528376698493958\n",
      "end of epoch.\n",
      "start of epoch 207\n",
      "Training loss (for one batch) at step 0: 0.4116404056549072\n",
      "Epoch 207, Loss: 0.4116404056549072, Val Loss: 0.5426075458526611\n",
      "end of epoch.\n",
      "start of epoch 208\n",
      "Training loss (for one batch) at step 0: 0.4910908043384552\n",
      "Epoch 208, Loss: 0.4910908043384552, Val Loss: 0.4531055688858032\n",
      "end of epoch.\n",
      "start of epoch 209\n",
      "Training loss (for one batch) at step 0: 0.5105918049812317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209, Loss: 0.5105918049812317, Val Loss: 0.5213540196418762\n",
      "end of epoch.\n",
      "start of epoch 210\n",
      "Training loss (for one batch) at step 0: 0.5282256007194519\n",
      "Epoch 210, Loss: 0.5282256007194519, Val Loss: 0.5264754891395569\n",
      "end of epoch.\n",
      "start of epoch 211\n",
      "Training loss (for one batch) at step 0: 0.4932553470134735\n",
      "Epoch 211, Loss: 0.4932553470134735, Val Loss: 0.4794016182422638\n",
      "end of epoch.\n",
      "start of epoch 212\n",
      "Training loss (for one batch) at step 0: 0.487151563167572\n",
      "Epoch 212, Loss: 0.487151563167572, Val Loss: 0.4932430684566498\n",
      "end of epoch.\n",
      "start of epoch 213\n",
      "Training loss (for one batch) at step 0: 0.5061758160591125\n",
      "Epoch 213, Loss: 0.5061758160591125, Val Loss: 0.4035945236682892\n",
      "end of epoch.\n",
      "start of epoch 214\n",
      "Training loss (for one batch) at step 0: 0.527052104473114\n",
      "Epoch 214, Loss: 0.527052104473114, Val Loss: 0.5517446398735046\n",
      "end of epoch.\n",
      "start of epoch 215\n",
      "Training loss (for one batch) at step 0: 0.5367366671562195\n",
      "Epoch 215, Loss: 0.5367366671562195, Val Loss: 0.5225973725318909\n",
      "end of epoch.\n",
      "start of epoch 216\n",
      "Training loss (for one batch) at step 0: 0.47059881687164307\n",
      "Epoch 216, Loss: 0.47059881687164307, Val Loss: 0.45165854692459106\n",
      "end of epoch.\n",
      "start of epoch 217\n",
      "Training loss (for one batch) at step 0: 0.5454736351966858\n",
      "Epoch 217, Loss: 0.5454736351966858, Val Loss: 0.44658178091049194\n",
      "end of epoch.\n",
      "start of epoch 218\n",
      "Training loss (for one batch) at step 0: 0.40517759323120117\n",
      "Epoch 218, Loss: 0.40517759323120117, Val Loss: 0.5245072841644287\n",
      "end of epoch.\n",
      "start of epoch 219\n",
      "Training loss (for one batch) at step 0: 0.5724174976348877\n",
      "Epoch 219, Loss: 0.5724174976348877, Val Loss: 0.4904668629169464\n",
      "end of epoch.\n",
      "start of epoch 220\n",
      "Training loss (for one batch) at step 0: 0.5001479983329773\n",
      "Epoch 220, Loss: 0.5001479983329773, Val Loss: 0.49062690138816833\n",
      "end of epoch.\n",
      "start of epoch 221\n",
      "Training loss (for one batch) at step 0: 0.5354060530662537\n",
      "Epoch 221, Loss: 0.5354060530662537, Val Loss: 0.49720707535743713\n",
      "end of epoch.\n",
      "start of epoch 222\n",
      "Training loss (for one batch) at step 0: 0.4644681215286255\n",
      "Epoch 222, Loss: 0.4644681215286255, Val Loss: 0.5582398176193237\n",
      "end of epoch.\n",
      "start of epoch 223\n",
      "Training loss (for one batch) at step 0: 0.5753210783004761\n",
      "Epoch 223, Loss: 0.5753210783004761, Val Loss: 0.4388633966445923\n",
      "end of epoch.\n",
      "start of epoch 224\n",
      "Training loss (for one batch) at step 0: 0.4825134873390198\n",
      "Epoch 224, Loss: 0.4825134873390198, Val Loss: 0.5114076733589172\n",
      "end of epoch.\n",
      "start of epoch 225\n",
      "Training loss (for one batch) at step 0: 0.493521511554718\n",
      "Epoch 225, Loss: 0.493521511554718, Val Loss: 0.4863928556442261\n",
      "end of epoch.\n",
      "start of epoch 226\n",
      "Training loss (for one batch) at step 0: 0.5109169483184814\n",
      "Epoch 226, Loss: 0.5109169483184814, Val Loss: 0.4387381374835968\n",
      "end of epoch.\n",
      "start of epoch 227\n",
      "Training loss (for one batch) at step 0: 0.5665091276168823\n",
      "Epoch 227, Loss: 0.5665091276168823, Val Loss: 0.46223199367523193\n",
      "end of epoch.\n",
      "start of epoch 228\n",
      "Training loss (for one batch) at step 0: 0.4437066912651062\n",
      "Epoch 228, Loss: 0.4437066912651062, Val Loss: 0.5623940825462341\n",
      "end of epoch.\n",
      "start of epoch 229\n",
      "Training loss (for one batch) at step 0: 0.428170382976532\n",
      "Epoch 229, Loss: 0.428170382976532, Val Loss: 0.4427231550216675\n",
      "end of epoch.\n",
      "start of epoch 230\n",
      "Training loss (for one batch) at step 0: 0.5510125756263733\n",
      "Epoch 230, Loss: 0.5510125756263733, Val Loss: 0.5157411694526672\n",
      "end of epoch.\n",
      "start of epoch 231\n",
      "Training loss (for one batch) at step 0: 0.5199134945869446\n",
      "Epoch 231, Loss: 0.5199134945869446, Val Loss: 0.5168976187705994\n",
      "end of epoch.\n",
      "start of epoch 232\n",
      "Training loss (for one batch) at step 0: 0.4572165310382843\n",
      "Epoch 232, Loss: 0.4572165310382843, Val Loss: 0.5241317749023438\n",
      "end of epoch.\n",
      "start of epoch 233\n",
      "Training loss (for one batch) at step 0: 0.4870051443576813\n",
      "Epoch 233, Loss: 0.4870051443576813, Val Loss: 0.4869808554649353\n",
      "end of epoch.\n",
      "start of epoch 234\n",
      "Training loss (for one batch) at step 0: 0.545631468296051\n",
      "Epoch 234, Loss: 0.545631468296051, Val Loss: 0.4652199447154999\n",
      "end of epoch.\n",
      "start of epoch 235\n",
      "Training loss (for one batch) at step 0: 0.47621574997901917\n",
      "Epoch 235, Loss: 0.47621574997901917, Val Loss: 0.43761366605758667\n",
      "end of epoch.\n",
      "start of epoch 236\n",
      "Training loss (for one batch) at step 0: 0.5305294394493103\n",
      "Epoch 236, Loss: 0.5305294394493103, Val Loss: 0.4857614040374756\n",
      "end of epoch.\n",
      "start of epoch 237\n",
      "Training loss (for one batch) at step 0: 0.35177841782569885\n",
      "Epoch 237, Loss: 0.35177841782569885, Val Loss: 0.5610474348068237\n",
      "end of epoch.\n",
      "start of epoch 238\n",
      "Training loss (for one batch) at step 0: 0.46730542182922363\n",
      "Epoch 238, Loss: 0.46730542182922363, Val Loss: 0.45077279210090637\n",
      "end of epoch.\n",
      "start of epoch 239\n",
      "Training loss (for one batch) at step 0: 0.5745779871940613\n",
      "Epoch 239, Loss: 0.5745779871940613, Val Loss: 0.47162437438964844\n",
      "end of epoch.\n",
      "start of epoch 240\n",
      "Training loss (for one batch) at step 0: 0.40613213181495667\n",
      "Epoch 240, Loss: 0.40613213181495667, Val Loss: 0.5060440301895142\n",
      "end of epoch.\n",
      "start of epoch 241\n",
      "Training loss (for one batch) at step 0: 0.4782446324825287\n",
      "Epoch 241, Loss: 0.4782446324825287, Val Loss: 0.4832933843135834\n",
      "end of epoch.\n",
      "start of epoch 242\n",
      "Training loss (for one batch) at step 0: 0.5340763926506042\n",
      "Epoch 242, Loss: 0.5340763926506042, Val Loss: 0.42134547233581543\n",
      "end of epoch.\n",
      "start of epoch 243\n",
      "Training loss (for one batch) at step 0: 0.5290356874465942\n",
      "Epoch 243, Loss: 0.5290356874465942, Val Loss: 0.46019646525382996\n",
      "end of epoch.\n",
      "start of epoch 244\n",
      "Training loss (for one batch) at step 0: 0.37089696526527405\n",
      "Epoch 244, Loss: 0.37089696526527405, Val Loss: 0.5712956190109253\n",
      "end of epoch.\n",
      "start of epoch 245\n",
      "Training loss (for one batch) at step 0: 0.4396137297153473\n",
      "Epoch 245, Loss: 0.4396137297153473, Val Loss: 0.48308584094047546\n",
      "end of epoch.\n",
      "start of epoch 246\n",
      "Training loss (for one batch) at step 0: 0.457227885723114\n",
      "Epoch 246, Loss: 0.457227885723114, Val Loss: 0.4920912981033325\n",
      "end of epoch.\n",
      "start of epoch 247\n",
      "Training loss (for one batch) at step 0: 0.4193027913570404\n",
      "Epoch 247, Loss: 0.4193027913570404, Val Loss: 0.5150076746940613\n",
      "end of epoch.\n",
      "start of epoch 248\n",
      "Training loss (for one batch) at step 0: 0.41547080874443054\n",
      "Epoch 248, Loss: 0.41547080874443054, Val Loss: 0.5305871963500977\n",
      "end of epoch.\n",
      "start of epoch 249\n",
      "Training loss (for one batch) at step 0: 0.44981223344802856\n",
      "Epoch 249, Loss: 0.44981223344802856, Val Loss: 0.5539806485176086\n",
      "end of epoch.\n",
      "start of epoch 250\n",
      "Training loss (for one batch) at step 0: 0.48710742592811584\n",
      "Epoch 250, Loss: 0.48710742592811584, Val Loss: 0.48952338099479675\n",
      "end of epoch.\n",
      "start of epoch 251\n",
      "Training loss (for one batch) at step 0: 0.4454239308834076\n",
      "Epoch 251, Loss: 0.4454239308834076, Val Loss: 0.4397636950016022\n",
      "end of epoch.\n",
      "start of epoch 252\n",
      "Training loss (for one batch) at step 0: 0.4677494168281555\n",
      "Epoch 252, Loss: 0.4677494168281555, Val Loss: 0.5371251702308655\n",
      "end of epoch.\n",
      "start of epoch 253\n",
      "Training loss (for one batch) at step 0: 0.554621160030365\n",
      "Epoch 253, Loss: 0.554621160030365, Val Loss: 0.4702169597148895\n",
      "end of epoch.\n",
      "start of epoch 254\n",
      "Training loss (for one batch) at step 0: 0.4335351586341858\n",
      "Epoch 254, Loss: 0.4335351586341858, Val Loss: 0.47147971391677856\n",
      "end of epoch.\n",
      "start of epoch 255\n",
      "Training loss (for one batch) at step 0: 0.5026587843894958\n",
      "Epoch 255, Loss: 0.5026587843894958, Val Loss: 0.4307750463485718\n",
      "end of epoch.\n",
      "start of epoch 256\n",
      "Training loss (for one batch) at step 0: 0.46179038286209106\n",
      "Epoch 256, Loss: 0.46179038286209106, Val Loss: 0.5629330277442932\n",
      "end of epoch.\n",
      "start of epoch 257\n",
      "Training loss (for one batch) at step 0: 0.5175867676734924\n",
      "Epoch 257, Loss: 0.5175867676734924, Val Loss: 0.506514310836792\n",
      "end of epoch.\n",
      "start of epoch 258\n",
      "Training loss (for one batch) at step 0: 0.5594835877418518\n",
      "Epoch 258, Loss: 0.5594835877418518, Val Loss: 0.47399234771728516\n",
      "end of epoch.\n",
      "start of epoch 259\n",
      "Training loss (for one batch) at step 0: 0.4924319386482239\n",
      "Epoch 259, Loss: 0.4924319386482239, Val Loss: 0.5690737962722778\n",
      "end of epoch.\n",
      "start of epoch 260\n",
      "Training loss (for one batch) at step 0: 0.47258201241493225\n",
      "Epoch 260, Loss: 0.47258201241493225, Val Loss: 0.46304869651794434\n",
      "end of epoch.\n",
      "start of epoch 261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 0.4803380072116852\n",
      "Epoch 261, Loss: 0.4803380072116852, Val Loss: 0.4216865003108978\n",
      "end of epoch.\n",
      "start of epoch 262\n",
      "Training loss (for one batch) at step 0: 0.47173985838890076\n",
      "Epoch 262, Loss: 0.47173985838890076, Val Loss: 0.48286083340644836\n",
      "end of epoch.\n",
      "start of epoch 263\n",
      "Training loss (for one batch) at step 0: 0.5843462944030762\n",
      "Epoch 263, Loss: 0.5843462944030762, Val Loss: 0.5141816139221191\n",
      "end of epoch.\n",
      "start of epoch 264\n",
      "Training loss (for one batch) at step 0: 0.5191679000854492\n",
      "Epoch 264, Loss: 0.5191679000854492, Val Loss: 0.5302677154541016\n",
      "end of epoch.\n",
      "start of epoch 265\n",
      "Training loss (for one batch) at step 0: 0.4741418659687042\n",
      "Epoch 265, Loss: 0.4741418659687042, Val Loss: 0.4328575134277344\n",
      "end of epoch.\n",
      "start of epoch 266\n",
      "Training loss (for one batch) at step 0: 0.549227237701416\n",
      "Epoch 266, Loss: 0.549227237701416, Val Loss: 0.6033914089202881\n",
      "end of epoch.\n",
      "start of epoch 267\n",
      "Training loss (for one batch) at step 0: 0.3681001663208008\n",
      "Epoch 267, Loss: 0.3681001663208008, Val Loss: 0.5049216151237488\n",
      "end of epoch.\n",
      "start of epoch 268\n",
      "Training loss (for one batch) at step 0: 0.47295767068862915\n",
      "Epoch 268, Loss: 0.47295767068862915, Val Loss: 0.46247923374176025\n",
      "end of epoch.\n",
      "start of epoch 269\n",
      "Training loss (for one batch) at step 0: 0.5096631050109863\n",
      "Epoch 269, Loss: 0.5096631050109863, Val Loss: 0.4849291741847992\n",
      "end of epoch.\n",
      "start of epoch 270\n",
      "Training loss (for one batch) at step 0: 0.47350218892097473\n",
      "Epoch 270, Loss: 0.47350218892097473, Val Loss: 0.5378960371017456\n",
      "end of epoch.\n",
      "start of epoch 271\n",
      "Training loss (for one batch) at step 0: 0.4416995346546173\n",
      "Epoch 271, Loss: 0.4416995346546173, Val Loss: 0.5177423357963562\n",
      "end of epoch.\n",
      "start of epoch 272\n",
      "Training loss (for one batch) at step 0: 0.5614392161369324\n",
      "Epoch 272, Loss: 0.5614392161369324, Val Loss: 0.611764132976532\n",
      "end of epoch.\n",
      "start of epoch 273\n",
      "Training loss (for one batch) at step 0: 0.4289494454860687\n",
      "Epoch 273, Loss: 0.4289494454860687, Val Loss: 0.4295295178890228\n",
      "end of epoch.\n",
      "start of epoch 274\n",
      "Training loss (for one batch) at step 0: 0.4332490563392639\n",
      "Epoch 274, Loss: 0.4332490563392639, Val Loss: 0.46116265654563904\n",
      "end of epoch.\n",
      "start of epoch 275\n",
      "Training loss (for one batch) at step 0: 0.5365885496139526\n",
      "Epoch 275, Loss: 0.5365885496139526, Val Loss: 0.49095794558525085\n",
      "end of epoch.\n",
      "start of epoch 276\n",
      "Training loss (for one batch) at step 0: 0.436549574136734\n",
      "Epoch 276, Loss: 0.436549574136734, Val Loss: 0.46503889560699463\n",
      "end of epoch.\n",
      "start of epoch 277\n",
      "Training loss (for one batch) at step 0: 0.464063435792923\n",
      "Epoch 277, Loss: 0.464063435792923, Val Loss: 0.4365191161632538\n",
      "end of epoch.\n",
      "start of epoch 278\n",
      "Training loss (for one batch) at step 0: 0.48052430152893066\n",
      "Epoch 278, Loss: 0.48052430152893066, Val Loss: 0.5278179049491882\n",
      "end of epoch.\n",
      "start of epoch 279\n",
      "Training loss (for one batch) at step 0: 0.5638689994812012\n",
      "Epoch 279, Loss: 0.5638689994812012, Val Loss: 0.5212627053260803\n",
      "end of epoch.\n",
      "start of epoch 280\n",
      "Training loss (for one batch) at step 0: 0.46074941754341125\n",
      "Epoch 280, Loss: 0.46074941754341125, Val Loss: 0.3966929316520691\n",
      "end of epoch.\n",
      "start of epoch 281\n",
      "Training loss (for one batch) at step 0: 0.5523570775985718\n",
      "Epoch 281, Loss: 0.5523570775985718, Val Loss: 0.4070706069469452\n",
      "end of epoch.\n",
      "start of epoch 282\n",
      "Training loss (for one batch) at step 0: 0.5210329294204712\n",
      "Epoch 282, Loss: 0.5210329294204712, Val Loss: 0.3923453092575073\n",
      "end of epoch.\n",
      "start of epoch 283\n",
      "Training loss (for one batch) at step 0: 0.5661382675170898\n",
      "Epoch 283, Loss: 0.5661382675170898, Val Loss: 0.4316636919975281\n",
      "end of epoch.\n",
      "start of epoch 284\n",
      "Training loss (for one batch) at step 0: 0.4969020485877991\n",
      "Epoch 284, Loss: 0.4969020485877991, Val Loss: 0.5218786597251892\n",
      "end of epoch.\n",
      "start of epoch 285\n",
      "Training loss (for one batch) at step 0: 0.5005989670753479\n",
      "Epoch 285, Loss: 0.5005989670753479, Val Loss: 0.501105546951294\n",
      "end of epoch.\n",
      "start of epoch 286\n",
      "Training loss (for one batch) at step 0: 0.4460364282131195\n",
      "Epoch 286, Loss: 0.4460364282131195, Val Loss: 0.5473572015762329\n",
      "end of epoch.\n",
      "start of epoch 287\n",
      "Training loss (for one batch) at step 0: 0.3249305486679077\n",
      "Epoch 287, Loss: 0.3249305486679077, Val Loss: 0.43231645226478577\n",
      "end of epoch.\n",
      "start of epoch 288\n",
      "Training loss (for one batch) at step 0: 0.5246949791908264\n",
      "Epoch 288, Loss: 0.5246949791908264, Val Loss: 0.37770915031433105\n",
      "end of epoch.\n",
      "start of epoch 289\n",
      "Training loss (for one batch) at step 0: 0.41153988242149353\n",
      "Epoch 289, Loss: 0.41153988242149353, Val Loss: 0.4169715940952301\n",
      "end of epoch.\n",
      "start of epoch 290\n",
      "Training loss (for one batch) at step 0: 0.5819879174232483\n",
      "Epoch 290, Loss: 0.5819879174232483, Val Loss: 0.3905477225780487\n",
      "end of epoch.\n",
      "start of epoch 291\n",
      "Training loss (for one batch) at step 0: 0.3806806206703186\n",
      "Epoch 291, Loss: 0.3806806206703186, Val Loss: 0.3945389986038208\n",
      "end of epoch.\n",
      "start of epoch 292\n",
      "Training loss (for one batch) at step 0: 0.5385817885398865\n",
      "Epoch 292, Loss: 0.5385817885398865, Val Loss: 0.509840726852417\n",
      "end of epoch.\n",
      "start of epoch 293\n",
      "Training loss (for one batch) at step 0: 0.6269040107727051\n",
      "Epoch 293, Loss: 0.6269040107727051, Val Loss: 0.48472732305526733\n",
      "end of epoch.\n",
      "start of epoch 294\n",
      "Training loss (for one batch) at step 0: 0.4660486578941345\n",
      "Epoch 294, Loss: 0.4660486578941345, Val Loss: 0.43957391381263733\n",
      "end of epoch.\n",
      "start of epoch 295\n",
      "Training loss (for one batch) at step 0: 0.5059760808944702\n",
      "Epoch 295, Loss: 0.5059760808944702, Val Loss: 0.5081216096878052\n",
      "end of epoch.\n",
      "start of epoch 296\n",
      "Training loss (for one batch) at step 0: 0.4498135447502136\n",
      "Epoch 296, Loss: 0.4498135447502136, Val Loss: 0.5277388691902161\n",
      "end of epoch.\n",
      "start of epoch 297\n",
      "Training loss (for one batch) at step 0: 0.4492698907852173\n",
      "Epoch 297, Loss: 0.4492698907852173, Val Loss: 0.49546217918395996\n",
      "end of epoch.\n",
      "start of epoch 298\n",
      "Training loss (for one batch) at step 0: 0.5235127806663513\n",
      "Epoch 298, Loss: 0.5235127806663513, Val Loss: 0.45873335003852844\n",
      "end of epoch.\n",
      "start of epoch 299\n",
      "Training loss (for one batch) at step 0: 0.4742864668369293\n",
      "Epoch 299, Loss: 0.4742864668369293, Val Loss: 0.49315640330314636\n",
      "end of epoch.\n",
      "start of epoch 300\n",
      "Training loss (for one batch) at step 0: 0.5127198100090027\n",
      "Epoch 300, Loss: 0.5127198100090027, Val Loss: 0.5262559652328491\n",
      "end of epoch.\n",
      "start of epoch 301\n",
      "Training loss (for one batch) at step 0: 0.46161648631095886\n",
      "Epoch 301, Loss: 0.46161648631095886, Val Loss: 0.44000595808029175\n",
      "end of epoch.\n",
      "start of epoch 302\n",
      "Training loss (for one batch) at step 0: 0.5748143792152405\n",
      "Epoch 302, Loss: 0.5748143792152405, Val Loss: 0.42511096596717834\n",
      "end of epoch.\n",
      "start of epoch 303\n",
      "Training loss (for one batch) at step 0: 0.5348228812217712\n",
      "Epoch 303, Loss: 0.5348228812217712, Val Loss: 0.5179003477096558\n",
      "end of epoch.\n",
      "start of epoch 304\n",
      "Training loss (for one batch) at step 0: 0.5181779861450195\n",
      "Epoch 304, Loss: 0.5181779861450195, Val Loss: 0.44886067509651184\n",
      "end of epoch.\n",
      "start of epoch 305\n",
      "Training loss (for one batch) at step 0: 0.3193015456199646\n",
      "Epoch 305, Loss: 0.3193015456199646, Val Loss: 0.4473351538181305\n",
      "end of epoch.\n",
      "start of epoch 306\n",
      "Training loss (for one batch) at step 0: 0.4775603711605072\n",
      "Epoch 306, Loss: 0.4775603711605072, Val Loss: 0.4780861735343933\n",
      "end of epoch.\n",
      "start of epoch 307\n",
      "Training loss (for one batch) at step 0: 0.5448906421661377\n",
      "Epoch 307, Loss: 0.5448906421661377, Val Loss: 0.4673207104206085\n",
      "end of epoch.\n",
      "start of epoch 308\n",
      "Training loss (for one batch) at step 0: 0.5076005458831787\n",
      "Epoch 308, Loss: 0.5076005458831787, Val Loss: 0.4977259933948517\n",
      "end of epoch.\n",
      "start of epoch 309\n",
      "Training loss (for one batch) at step 0: 0.479926198720932\n",
      "Epoch 309, Loss: 0.479926198720932, Val Loss: 0.5620407462120056\n",
      "end of epoch.\n",
      "start of epoch 310\n",
      "Training loss (for one batch) at step 0: 0.49644872546195984\n",
      "Epoch 310, Loss: 0.49644872546195984, Val Loss: 0.5019423365592957\n",
      "end of epoch.\n",
      "start of epoch 311\n",
      "Training loss (for one batch) at step 0: 0.49162688851356506\n",
      "Epoch 311, Loss: 0.49162688851356506, Val Loss: 0.5284411907196045\n",
      "end of epoch.\n",
      "start of epoch 312\n",
      "Training loss (for one batch) at step 0: 0.48397523164749146\n",
      "Epoch 312, Loss: 0.48397523164749146, Val Loss: 0.5361650586128235\n",
      "end of epoch.\n",
      "start of epoch 313\n",
      "Training loss (for one batch) at step 0: 0.3609238564968109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313, Loss: 0.3609238564968109, Val Loss: 0.4236752986907959\n",
      "end of epoch.\n",
      "start of epoch 314\n",
      "Training loss (for one batch) at step 0: 0.5208494067192078\n",
      "Epoch 314, Loss: 0.5208494067192078, Val Loss: 0.5418254137039185\n",
      "end of epoch.\n",
      "start of epoch 315\n",
      "Training loss (for one batch) at step 0: 0.4782419800758362\n",
      "Epoch 315, Loss: 0.4782419800758362, Val Loss: 0.4124601483345032\n",
      "end of epoch.\n",
      "start of epoch 316\n",
      "Training loss (for one batch) at step 0: 0.5009971261024475\n",
      "Epoch 316, Loss: 0.5009971261024475, Val Loss: 0.5052449703216553\n",
      "end of epoch.\n",
      "start of epoch 317\n",
      "Training loss (for one batch) at step 0: 0.5156872272491455\n",
      "Epoch 317, Loss: 0.5156872272491455, Val Loss: 0.44732579588890076\n",
      "end of epoch.\n",
      "start of epoch 318\n",
      "Training loss (for one batch) at step 0: 0.4308564364910126\n",
      "Epoch 318, Loss: 0.4308564364910126, Val Loss: 0.4725569486618042\n",
      "end of epoch.\n",
      "start of epoch 319\n",
      "Training loss (for one batch) at step 0: 0.5025884509086609\n",
      "Epoch 319, Loss: 0.5025884509086609, Val Loss: 0.4142691493034363\n",
      "end of epoch.\n",
      "start of epoch 320\n",
      "Training loss (for one batch) at step 0: 0.480624794960022\n",
      "Epoch 320, Loss: 0.480624794960022, Val Loss: 0.44739192724227905\n",
      "end of epoch.\n",
      "start of epoch 321\n",
      "Training loss (for one batch) at step 0: 0.42761796712875366\n",
      "Epoch 321, Loss: 0.42761796712875366, Val Loss: 0.41718533635139465\n",
      "end of epoch.\n",
      "start of epoch 322\n",
      "Training loss (for one batch) at step 0: 0.4555141031742096\n",
      "Epoch 322, Loss: 0.4555141031742096, Val Loss: 0.4669175446033478\n",
      "end of epoch.\n",
      "start of epoch 323\n",
      "Training loss (for one batch) at step 0: 0.5234727263450623\n",
      "Epoch 323, Loss: 0.5234727263450623, Val Loss: 0.5158118605613708\n",
      "end of epoch.\n",
      "start of epoch 324\n",
      "Training loss (for one batch) at step 0: 0.5659593343734741\n",
      "Epoch 324, Loss: 0.5659593343734741, Val Loss: 0.37974876165390015\n",
      "end of epoch.\n",
      "start of epoch 325\n",
      "Training loss (for one batch) at step 0: 0.49078378081321716\n",
      "Epoch 325, Loss: 0.49078378081321716, Val Loss: 0.5132177472114563\n",
      "end of epoch.\n",
      "start of epoch 326\n",
      "Training loss (for one batch) at step 0: 0.4524950385093689\n",
      "Epoch 326, Loss: 0.4524950385093689, Val Loss: 0.6068825125694275\n",
      "end of epoch.\n",
      "start of epoch 327\n",
      "Training loss (for one batch) at step 0: 0.4774337708950043\n",
      "Epoch 327, Loss: 0.4774337708950043, Val Loss: 0.42880865931510925\n",
      "end of epoch.\n",
      "start of epoch 328\n",
      "Training loss (for one batch) at step 0: 0.4987626373767853\n",
      "Epoch 328, Loss: 0.4987626373767853, Val Loss: 0.5358225107192993\n",
      "end of epoch.\n",
      "start of epoch 329\n",
      "Training loss (for one batch) at step 0: 0.5789517164230347\n",
      "Epoch 329, Loss: 0.5789517164230347, Val Loss: 0.4935332238674164\n",
      "end of epoch.\n",
      "start of epoch 330\n",
      "Training loss (for one batch) at step 0: 0.5367001891136169\n",
      "Epoch 330, Loss: 0.5367001891136169, Val Loss: 0.5648486614227295\n",
      "end of epoch.\n",
      "start of epoch 331\n",
      "Training loss (for one batch) at step 0: 0.48772937059402466\n",
      "Epoch 331, Loss: 0.48772937059402466, Val Loss: 0.4581516981124878\n",
      "end of epoch.\n",
      "start of epoch 332\n",
      "Training loss (for one batch) at step 0: 0.4505680799484253\n",
      "Epoch 332, Loss: 0.4505680799484253, Val Loss: 0.47641387581825256\n",
      "end of epoch.\n",
      "start of epoch 333\n",
      "Training loss (for one batch) at step 0: 0.445701003074646\n",
      "Epoch 333, Loss: 0.445701003074646, Val Loss: 0.49274539947509766\n",
      "end of epoch.\n",
      "start of epoch 334\n",
      "Training loss (for one batch) at step 0: 0.38280799984931946\n",
      "Epoch 334, Loss: 0.38280799984931946, Val Loss: 0.45763614773750305\n",
      "end of epoch.\n",
      "start of epoch 335\n",
      "Training loss (for one batch) at step 0: 0.4152071475982666\n",
      "Epoch 335, Loss: 0.4152071475982666, Val Loss: 0.39412954449653625\n",
      "end of epoch.\n",
      "start of epoch 336\n",
      "Training loss (for one batch) at step 0: 0.4177626967430115\n",
      "Epoch 336, Loss: 0.4177626967430115, Val Loss: 0.4743746221065521\n",
      "end of epoch.\n",
      "start of epoch 337\n",
      "Training loss (for one batch) at step 0: 0.5041210055351257\n",
      "Epoch 337, Loss: 0.5041210055351257, Val Loss: 0.4489782750606537\n",
      "end of epoch.\n",
      "start of epoch 338\n",
      "Training loss (for one batch) at step 0: 0.4705300033092499\n",
      "Epoch 338, Loss: 0.4705300033092499, Val Loss: 0.49052730202674866\n",
      "end of epoch.\n",
      "start of epoch 339\n",
      "Training loss (for one batch) at step 0: 0.5430898070335388\n",
      "Epoch 339, Loss: 0.5430898070335388, Val Loss: 0.4412822425365448\n",
      "end of epoch.\n",
      "start of epoch 340\n",
      "Training loss (for one batch) at step 0: 0.47188037633895874\n",
      "Epoch 340, Loss: 0.47188037633895874, Val Loss: 0.44557347893714905\n",
      "end of epoch.\n",
      "start of epoch 341\n",
      "Training loss (for one batch) at step 0: 0.5221555233001709\n",
      "Epoch 341, Loss: 0.5221555233001709, Val Loss: 0.4882088303565979\n",
      "end of epoch.\n",
      "start of epoch 342\n",
      "Training loss (for one batch) at step 0: 0.39332225918769836\n",
      "Epoch 342, Loss: 0.39332225918769836, Val Loss: 0.5649059414863586\n",
      "end of epoch.\n",
      "start of epoch 343\n",
      "Training loss (for one batch) at step 0: 0.5374900698661804\n",
      "Epoch 343, Loss: 0.5374900698661804, Val Loss: 0.45092642307281494\n",
      "end of epoch.\n",
      "start of epoch 344\n",
      "Training loss (for one batch) at step 0: 0.5238074064254761\n",
      "Epoch 344, Loss: 0.5238074064254761, Val Loss: 0.4183836281299591\n",
      "end of epoch.\n",
      "start of epoch 345\n",
      "Training loss (for one batch) at step 0: 0.4934273660182953\n",
      "Epoch 345, Loss: 0.4934273660182953, Val Loss: 0.4719032943248749\n",
      "end of epoch.\n",
      "start of epoch 346\n",
      "Training loss (for one batch) at step 0: 0.5259012579917908\n",
      "Epoch 346, Loss: 0.5259012579917908, Val Loss: 0.5368710160255432\n",
      "end of epoch.\n",
      "start of epoch 347\n",
      "Training loss (for one batch) at step 0: 0.4510873556137085\n",
      "Epoch 347, Loss: 0.4510873556137085, Val Loss: 0.43360570073127747\n",
      "end of epoch.\n",
      "start of epoch 348\n",
      "Training loss (for one batch) at step 0: 0.47688114643096924\n",
      "Epoch 348, Loss: 0.47688114643096924, Val Loss: 0.5065276026725769\n",
      "end of epoch.\n",
      "start of epoch 349\n",
      "Training loss (for one batch) at step 0: 0.4591030478477478\n",
      "Epoch 349, Loss: 0.4591030478477478, Val Loss: 0.44375747442245483\n",
      "end of epoch.\n",
      "start of epoch 350\n",
      "Training loss (for one batch) at step 0: 0.49512770771980286\n",
      "Epoch 350, Loss: 0.49512770771980286, Val Loss: 0.4069933593273163\n",
      "end of epoch.\n",
      "start of epoch 351\n",
      "Training loss (for one batch) at step 0: 0.4382565915584564\n",
      "Epoch 351, Loss: 0.4382565915584564, Val Loss: 0.5229381918907166\n",
      "end of epoch.\n",
      "start of epoch 352\n",
      "Training loss (for one batch) at step 0: 0.5016250014305115\n",
      "Epoch 352, Loss: 0.5016250014305115, Val Loss: 0.4266079366207123\n",
      "end of epoch.\n",
      "start of epoch 353\n",
      "Training loss (for one batch) at step 0: 0.5184246897697449\n",
      "Epoch 353, Loss: 0.5184246897697449, Val Loss: 0.48352867364883423\n",
      "end of epoch.\n",
      "start of epoch 354\n",
      "Training loss (for one batch) at step 0: 0.5572130680084229\n",
      "Epoch 354, Loss: 0.5572130680084229, Val Loss: 0.4693383574485779\n",
      "end of epoch.\n",
      "start of epoch 355\n",
      "Training loss (for one batch) at step 0: 0.5262990593910217\n",
      "Epoch 355, Loss: 0.5262990593910217, Val Loss: 0.440663605928421\n",
      "end of epoch.\n",
      "start of epoch 356\n",
      "Training loss (for one batch) at step 0: 0.5682850480079651\n",
      "Epoch 356, Loss: 0.5682850480079651, Val Loss: 0.5289129018783569\n",
      "end of epoch.\n",
      "start of epoch 357\n",
      "Training loss (for one batch) at step 0: 0.44371622800827026\n",
      "Epoch 357, Loss: 0.44371622800827026, Val Loss: 0.5110970139503479\n",
      "end of epoch.\n",
      "start of epoch 358\n",
      "Training loss (for one batch) at step 0: 0.5593809485435486\n",
      "Epoch 358, Loss: 0.5593809485435486, Val Loss: 0.5330043435096741\n",
      "end of epoch.\n",
      "start of epoch 359\n",
      "Training loss (for one batch) at step 0: 0.6041259169578552\n",
      "Epoch 359, Loss: 0.6041259169578552, Val Loss: 0.383049875497818\n",
      "end of epoch.\n",
      "start of epoch 360\n",
      "Training loss (for one batch) at step 0: 0.5043665766716003\n",
      "Epoch 360, Loss: 0.5043665766716003, Val Loss: 0.4755405783653259\n",
      "end of epoch.\n",
      "start of epoch 361\n",
      "Training loss (for one batch) at step 0: 0.4843336045742035\n",
      "Epoch 361, Loss: 0.4843336045742035, Val Loss: 0.5343406796455383\n",
      "end of epoch.\n",
      "start of epoch 362\n",
      "Training loss (for one batch) at step 0: 0.4254209101200104\n",
      "Epoch 362, Loss: 0.4254209101200104, Val Loss: 0.4128769636154175\n",
      "end of epoch.\n",
      "start of epoch 363\n",
      "Training loss (for one batch) at step 0: 0.48232415318489075\n",
      "Epoch 363, Loss: 0.48232415318489075, Val Loss: 0.49402233958244324\n",
      "end of epoch.\n",
      "start of epoch 364\n",
      "Training loss (for one batch) at step 0: 0.4221358001232147\n",
      "Epoch 364, Loss: 0.4221358001232147, Val Loss: 0.4345517158508301\n",
      "end of epoch.\n",
      "start of epoch 365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 0.44050654768943787\n",
      "Epoch 365, Loss: 0.44050654768943787, Val Loss: 0.4460725784301758\n",
      "end of epoch.\n",
      "start of epoch 366\n",
      "Training loss (for one batch) at step 0: 0.5860850811004639\n",
      "Epoch 366, Loss: 0.5860850811004639, Val Loss: 0.5747321844100952\n",
      "end of epoch.\n",
      "start of epoch 367\n",
      "Training loss (for one batch) at step 0: 0.4339686334133148\n",
      "Epoch 367, Loss: 0.4339686334133148, Val Loss: 0.4380626380443573\n",
      "end of epoch.\n",
      "start of epoch 368\n",
      "Training loss (for one batch) at step 0: 0.5345919728279114\n",
      "Epoch 368, Loss: 0.5345919728279114, Val Loss: 0.4789201617240906\n",
      "end of epoch.\n",
      "start of epoch 369\n",
      "Training loss (for one batch) at step 0: 0.5289421081542969\n",
      "Epoch 369, Loss: 0.5289421081542969, Val Loss: 0.4803032875061035\n",
      "end of epoch.\n",
      "start of epoch 370\n",
      "Training loss (for one batch) at step 0: 0.47426837682724\n",
      "Epoch 370, Loss: 0.47426837682724, Val Loss: 0.3872393071651459\n",
      "end of epoch.\n",
      "start of epoch 371\n",
      "Training loss (for one batch) at step 0: 0.5154631733894348\n",
      "Epoch 371, Loss: 0.5154631733894348, Val Loss: 0.3400065302848816\n",
      "end of epoch.\n",
      "start of epoch 372\n",
      "Training loss (for one batch) at step 0: 0.4838915765285492\n",
      "Epoch 372, Loss: 0.4838915765285492, Val Loss: 0.3882778286933899\n",
      "end of epoch.\n",
      "start of epoch 373\n",
      "Training loss (for one batch) at step 0: 0.5162440538406372\n",
      "Epoch 373, Loss: 0.5162440538406372, Val Loss: 0.5086480975151062\n",
      "end of epoch.\n",
      "start of epoch 374\n",
      "Training loss (for one batch) at step 0: 0.4546191096305847\n",
      "Epoch 374, Loss: 0.4546191096305847, Val Loss: 0.5932419896125793\n",
      "end of epoch.\n",
      "start of epoch 375\n",
      "Training loss (for one batch) at step 0: 0.394264817237854\n",
      "Epoch 375, Loss: 0.394264817237854, Val Loss: 0.43786346912384033\n",
      "end of epoch.\n",
      "start of epoch 376\n",
      "Training loss (for one batch) at step 0: 0.48251885175704956\n",
      "Epoch 376, Loss: 0.48251885175704956, Val Loss: 0.37347546219825745\n",
      "end of epoch.\n",
      "start of epoch 377\n",
      "Training loss (for one batch) at step 0: 0.4170590341091156\n",
      "Epoch 377, Loss: 0.4170590341091156, Val Loss: 0.5472761392593384\n",
      "end of epoch.\n",
      "start of epoch 378\n",
      "Training loss (for one batch) at step 0: 0.35949522256851196\n",
      "Epoch 378, Loss: 0.35949522256851196, Val Loss: 0.48990142345428467\n",
      "end of epoch.\n",
      "start of epoch 379\n",
      "Training loss (for one batch) at step 0: 0.411734014749527\n",
      "Epoch 379, Loss: 0.411734014749527, Val Loss: 0.5062414407730103\n",
      "end of epoch.\n",
      "start of epoch 380\n",
      "Training loss (for one batch) at step 0: 0.4550971984863281\n",
      "Epoch 380, Loss: 0.4550971984863281, Val Loss: 0.4909040331840515\n",
      "end of epoch.\n",
      "start of epoch 381\n",
      "Training loss (for one batch) at step 0: 0.48930680751800537\n",
      "Epoch 381, Loss: 0.48930680751800537, Val Loss: 0.4764922559261322\n",
      "end of epoch.\n",
      "start of epoch 382\n",
      "Training loss (for one batch) at step 0: 0.49486929178237915\n",
      "Epoch 382, Loss: 0.49486929178237915, Val Loss: 0.5328283905982971\n",
      "end of epoch.\n",
      "start of epoch 383\n",
      "Training loss (for one batch) at step 0: 0.42748966813087463\n",
      "Epoch 383, Loss: 0.42748966813087463, Val Loss: 0.5081704258918762\n",
      "end of epoch.\n",
      "start of epoch 384\n",
      "Training loss (for one batch) at step 0: 0.5097686648368835\n",
      "Epoch 384, Loss: 0.5097686648368835, Val Loss: 0.4994313418865204\n",
      "end of epoch.\n",
      "start of epoch 385\n",
      "Training loss (for one batch) at step 0: 0.5579916834831238\n",
      "Epoch 385, Loss: 0.5579916834831238, Val Loss: 0.5119727849960327\n",
      "end of epoch.\n",
      "start of epoch 386\n",
      "Training loss (for one batch) at step 0: 0.5149712562561035\n",
      "Epoch 386, Loss: 0.5149712562561035, Val Loss: 0.4435006082057953\n",
      "end of epoch.\n",
      "start of epoch 387\n",
      "Training loss (for one batch) at step 0: 0.4114534854888916\n",
      "Epoch 387, Loss: 0.4114534854888916, Val Loss: 0.45508497953414917\n",
      "end of epoch.\n",
      "start of epoch 388\n",
      "Training loss (for one batch) at step 0: 0.44639769196510315\n",
      "Epoch 388, Loss: 0.44639769196510315, Val Loss: 0.4961501955986023\n",
      "end of epoch.\n",
      "start of epoch 389\n",
      "Training loss (for one batch) at step 0: 0.5145726799964905\n",
      "Epoch 389, Loss: 0.5145726799964905, Val Loss: 0.46737727522850037\n",
      "end of epoch.\n",
      "start of epoch 390\n",
      "Training loss (for one batch) at step 0: 0.5479128956794739\n",
      "Epoch 390, Loss: 0.5479128956794739, Val Loss: 0.4794318974018097\n",
      "end of epoch.\n",
      "start of epoch 391\n",
      "Training loss (for one batch) at step 0: 0.4744409918785095\n",
      "Epoch 391, Loss: 0.4744409918785095, Val Loss: 0.5297888517379761\n",
      "end of epoch.\n",
      "start of epoch 392\n",
      "Training loss (for one batch) at step 0: 0.5273432731628418\n",
      "Epoch 392, Loss: 0.5273432731628418, Val Loss: 0.5639815926551819\n",
      "end of epoch.\n",
      "start of epoch 393\n",
      "Training loss (for one batch) at step 0: 0.5755225419998169\n",
      "Epoch 393, Loss: 0.5755225419998169, Val Loss: 0.44033902883529663\n",
      "end of epoch.\n",
      "start of epoch 394\n",
      "Training loss (for one batch) at step 0: 0.5281442999839783\n",
      "Epoch 394, Loss: 0.5281442999839783, Val Loss: 0.5188314914703369\n",
      "end of epoch.\n",
      "start of epoch 395\n",
      "Training loss (for one batch) at step 0: 0.4791141748428345\n",
      "Epoch 395, Loss: 0.4791141748428345, Val Loss: 0.4525240659713745\n",
      "end of epoch.\n",
      "start of epoch 396\n",
      "Training loss (for one batch) at step 0: 0.44427689909935\n",
      "Epoch 396, Loss: 0.44427689909935, Val Loss: 0.5196387767791748\n",
      "end of epoch.\n",
      "start of epoch 397\n",
      "Training loss (for one batch) at step 0: 0.5651757717132568\n",
      "Epoch 397, Loss: 0.5651757717132568, Val Loss: 0.5015825033187866\n",
      "end of epoch.\n",
      "start of epoch 398\n",
      "Training loss (for one batch) at step 0: 0.48619192838668823\n",
      "Epoch 398, Loss: 0.48619192838668823, Val Loss: 0.5842647552490234\n",
      "end of epoch.\n",
      "start of epoch 399\n",
      "Training loss (for one batch) at step 0: 0.4728192687034607\n",
      "Epoch 399, Loss: 0.4728192687034607, Val Loss: 0.5180776119232178\n",
      "end of epoch.\n",
      "start of epoch 400\n",
      "Training loss (for one batch) at step 0: 0.4933466911315918\n",
      "Epoch 400, Loss: 0.4933466911315918, Val Loss: 0.46840497851371765\n",
      "end of epoch.\n",
      "Checkpoint directory :  checkpoints/CNNgeo/200507_regressor_filter_size_up\n",
      "Tensorboard log directory :  logs/CNNgeo/200507_regressor_filter_size_up\n"
     ]
    }
   ],
   "source": [
    "train.overfit(config, ['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset amount : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare gt : [[[-0.1148138   0.15529494]\n",
      "  [ 0.12351213  0.12793016]\n",
      "  [-0.07588854  0.17240429]\n",
      "  [-0.06932826 -0.09832153]\n",
      "  [-0.11190281 -0.15785117]\n",
      "  [-0.05877905  0.06662889]\n",
      "  [ 0.16314201 -0.04046483]\n",
      "  [-0.11794777 -0.12232723]\n",
      "  [ 0.00325527  0.07304354]]] and pred : [[[ 0.00085992 -0.00129458]\n",
      "  [-0.00538858  0.00543234]\n",
      "  [ 0.00768351  0.0006666 ]\n",
      "  [ 0.00340796  0.00081706]\n",
      "  [-0.00772987  0.01070661]\n",
      "  [ 0.00066273  0.00141213]\n",
      "  [ 0.00027825 -0.01160984]\n",
      "  [ 0.00623972  0.00637852]\n",
      "  [ 0.00295155  0.00012533]]]\n",
      "score : [[0.07132681 0.06714977 0.06628522 0.06627319 0.06635749 0.06633465\n",
      "  0.06663328 0.06533518 0.06390215 0.0653858  0.06597848 0.06657533\n",
      "  0.06632768 0.06633456 0.06600584 0.05656511]\n",
      " [0.06565203 0.06429319 0.06322441 0.06364568 0.06398603 0.06336305\n",
      "  0.06174847 0.05896564 0.06086777 0.0616793  0.06364863 0.06268588\n",
      "  0.06315792 0.063146   0.06306393 0.05604176]\n",
      " [0.06494284 0.06416973 0.06376428 0.06310859 0.06010579 0.06058332\n",
      "  0.06157563 0.05999434 0.05998784 0.06245636 0.0632415  0.06215702\n",
      "  0.06298234 0.06302181 0.06281388 0.05536058]\n",
      " [0.06549404 0.06427926 0.06093768 0.06043133 0.06062642 0.06139922\n",
      "  0.06086992 0.06138352 0.0609889  0.06307203 0.06268447 0.06246804\n",
      "  0.06256685 0.06260156 0.06259773 0.05530919]\n",
      " [0.06468929 0.0627318  0.0608308  0.0630303  0.06265242 0.0614807\n",
      "  0.06161575 0.06215741 0.06353988 0.06280607 0.06255858 0.06288255\n",
      "  0.06277016 0.06289272 0.06272295 0.05513608]\n",
      " [0.06253611 0.06271781 0.06290135 0.06252474 0.06422728 0.06553493\n",
      "  0.06556591 0.0653099  0.06481072 0.06338427 0.06309708 0.06314144\n",
      "  0.06307265 0.06309812 0.06282652 0.05547499]\n",
      " [0.06455418 0.06228792 0.0645671  0.06449316 0.06506154 0.06327031\n",
      "  0.06330826 0.0627949  0.06288082 0.06312937 0.06297342 0.06299078\n",
      "  0.06300548 0.0629785  0.06281948 0.05543046]\n",
      " [0.06604961 0.06444977 0.06139554 0.06200631 0.06279415 0.06294227\n",
      "  0.06311615 0.06312751 0.06286228 0.06291053 0.06288105 0.06292114\n",
      "  0.06292327 0.06292997 0.06283028 0.05530534]\n",
      " [0.0628502  0.06313334 0.063645   0.06117836 0.06310354 0.06121428\n",
      "  0.06351826 0.06323443 0.06275268 0.06287812 0.06293447 0.06287299\n",
      "  0.06290881 0.06293068 0.06279644 0.0552955 ]\n",
      " [0.06451473 0.06697907 0.06627189 0.06307083 0.06199961 0.06090572\n",
      "  0.06314307 0.06324521 0.06313652 0.06287848 0.06294195 0.06292708\n",
      "  0.06290964 0.06292069 0.06275461 0.055316  ]\n",
      " [0.06425357 0.06504865 0.064574   0.06430921 0.06422067 0.06021122\n",
      "  0.06223498 0.06323575 0.0637762  0.0630453  0.06289793 0.06294491\n",
      "  0.06284416 0.06288067 0.06278638 0.05531083]\n",
      " [0.06427409 0.06404227 0.06358014 0.06318683 0.06374516 0.0632032\n",
      "  0.0595022  0.06327344 0.06341112 0.06367006 0.06295701 0.06293865\n",
      "  0.0629108  0.06289514 0.06280814 0.05525539]\n",
      " [0.06403665 0.06348024 0.06332651 0.06286911 0.06310843 0.06353033\n",
      "  0.06176374 0.05982221 0.06386384 0.06375389 0.06323253 0.06299455\n",
      "  0.0629191  0.06290864 0.06277142 0.055287  ]\n",
      " [0.0637185  0.06318316 0.06381688 0.06321418 0.06294916 0.06340061\n",
      "  0.06340875 0.06179235 0.06200616 0.06317366 0.06348408 0.0633461\n",
      "  0.06282093 0.0628976  0.06277831 0.05529143]\n",
      " [0.06394997 0.06444394 0.06425565 0.06355919 0.06370005 0.06380444\n",
      "  0.06432921 0.06441656 0.06212783 0.06169573 0.06262252 0.06392421\n",
      "  0.06396744 0.06383277 0.06388968 0.05489328]\n",
      " [0.0595746  0.05787679 0.05919888 0.05872273 0.05770352 0.05728006\n",
      "  0.05728767 0.05859318 0.05952758 0.05701039 0.05838443 0.05888706\n",
      "  0.0582942  0.05741436 0.05783859 0.04945507]]\n",
      "loss : [0.11641564]\n",
      "(20, 20) (13, 29)\n",
      "(52, 20) (59, 28)\n",
      "(84, 20) (79, 31)\n",
      "(20, 52) (16, 45)\n",
      "(52, 52) (44, 41)\n",
      "(84, 52) (80, 56)\n",
      "(20, 84) (30, 81)\n",
      "(52, 84) (44, 76)\n",
      "(84, 84) (84, 88)\n",
      "(20, 20) (20, 20)\n",
      "(52, 20) (51, 20)\n",
      "(84, 20) (84, 20)\n",
      "(20, 52) (20, 52)\n",
      "(52, 52) (51, 52)\n",
      "(84, 52) (84, 52)\n",
      "(20, 84) (20, 83)\n",
      "(52, 84) (52, 84)\n",
      "(84, 84) (84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG8AAAD8CAYAAAB5N/qNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO29d7Qc93Xn+bm/qur0ul8G8AA8JIKIBEiKApMkKosSJY1lrZNky/bYsuQgrW3Z67M+6/HR2bVnxsdn/tnxjmaH9uoojCzZxxYtB1nRkmVSJEU8EoFEzvHl2Lmqfnf/qOrwQJAgXwKb7C/YfN3VXaHr2/f3u+l3r6gqbbQmzM2+gDYWjjZ5LYw2eS2MNnktjDZ5LYw2eS2MZSFPRN4jIsdF5JSI/P5ynKMNkKW280TEAU4A7wIuAU8BH1bVI0t6ojaWRfLuAU6p6hlVrQJfAT6wDOd5zcNdhmOuBy42vb4E3PtiO/T39+vmzZuX4VJaA0NDQ+Oquurl7rcc5L0kiMjHgY8DbNy4kf3799+sS7npEJHzC9lvOYbNy8CGpteD8bZ5UNWHVXWfqu5btepl/+jaYHnIewrYJiJbRCQBfAj4+2U4z2seSz5sqmogIp8Evgk4wGdV9bmlPk8byzTnqerXga8vx7HbaKDtYWlhtMlrYbTJa2G0yWthtMlrYbQkeQc5iMUuybF8fA5zeEmOBTAc/1sJ3DT32EJxkIN8yH6YB49/gs2n34og8943Ili1iAiqNN6V6H9OaRz31PcR67NpVYZn7j7OF3c/xh8+8XuM5t8Fthh9tinYok2H0HmHM/XnDgom5JE3/Af6u1J8iS/h4CzLPaih5cjby17effwTTHxvNzN2CoBaWEtEEAUkusnSTICAMYbE3DjuD4eYy0/QcVsnOw77fPDufornnuNc361MWw9EwIKiTYRp/YeiEg1ZIgZRiyMGB4uGAbvsz/Dx9+3BrMCg1nLDpsGw+czbcKw3b7tIQwJVlflxSkEkutlJ15AILSknQTCRx5cUtzy9mrA0xVozCiLRvjHzUju2CEYExxgSYvBE8GyAG/qYoArVMgQVdgc72Mve540Iy4GWkzxg/pDWJHWqigoIkQQ2tikqgut4+MOncVxImQwz1WnW9fZSqYZ09vYznR/DpNYRInVpUyT6hWs0NIoNQRVsiNoQ1MeiGCyoBeuv2G1oTfKYTxqAtbb+XLX2v/hzInhuhvLUGPlLZ+hOpHnjLT0kZi3h2Ci7dt7KmZk5Ap1CUhZwMEg8DFvEWhwNMWpBQ1Rj4myIqkVEo3NbWMkM9JYlrxk1CWu8joRTVTHGII5DaXqck9/+It2Sp38wh2QC8jMOWQ2YOHWBRCZJ6KxBVHAFjFocLEZDDArqIxqioR9Js1WsWDSaXVGcaK5tk3dj1IfEa25WTdKUSPMUIwjCxBOP0K3TeJ5LZy5FYDxyPUlsyZIyVTolZC68woQtEpoUDhZrfYQgkriapNkQi6ACVgE10XMbD56ycmpEa5LX9AsXqM+BqvHwpbFmKIJjXMaffRyqk3jpFK6EhH6V6bkqiUSSTGcClwriGDYnQo5IlbIFjSXOEA2bYi0gWBUUJVCw6hCi2NDBqsFiCNR7oatecrQmefMUFjBNw2Q060W6nnFdgkKeqWM/IJs0IELC9Ug6hrBaomB98p5H2lU6cwmKRQjDEuI4CBbHWkQtqKJEUh4gxPKIj0uAgzUOYhwwgnpt8l4UQkSPorUXYKP5DUBUQBQNLflj/8L6NT3MzUwxMTlFiiruhlsQq+TnpkimEhTURyWglBggtApiqTlwQgUVgxXBWkPVKD4uvhosDjguxoBxHIwIrrNyt7QlyYOGt0NEsPE8Bw2bzPOS6PgFJs4dxS/P4DqGoFJl9bocga9MzkyxoT9N0oHQVsFYJjWNWgcVQY1FrYDjEahgEazjomIQcXCswURGIMaJpFpWTlcBWpQ8VY0UkfhmNUwEjQxpIOUYDv/b39Ht+JRSMDM1QybhMTtXYGYmybpsmop1yGQzVAoGzxiq7hpCk0CpzWcOoi7WOCgSSV+sVRqHhloL2Jpmu4L3oSXJoyZtCqCRm8pE3hdRi6vC1aP7yblFspkMdjakI1WlWq3Qv2aQrv4caS2T8cCvVPDDkISbYbZiqXhJLAbEEBgDGFRM7GExDdcbglpbd7sZomtaGnf5S0PLuceaYSQypI2Aq4oThpgwQMIi1UsHSWkZGwYMXx0BgU1bNpHKZBkem6FMiF8pMj49g5P0yAeGQqafsqTwTYKqSaCOB66LOA7GxE7mumYb0dRsskTa7sp9/5aUPIn9jAbBqGKsRW1kQLuiVGcu4s5dpFgJmZkcY8e2zawbHGBycoazF66wsTfFbMGSSENHKkXR95ktwHhuB4hBXBcxINZGrmmNTfF4jkMjaQet+0IFMGJWxKdZQ0uSh7VIGNlhoiFqfQgjY1rwGR/6DmPj07iex60b15Pr8Dh4+BSVcpGe7gzdfT0YAnK9CWwljwME2dXkvQ5c/EiLjSwExLlmXquLltaVpXo04zpOg+XEooZNETknIodF5ICI7I+39YrIt0XkZPy3Z2kutQFPLCkNSGoVz1YwQRljK3gS4OXHmJkaplqt0JNNsLY7zZHj54CQDWv7yOWyDE9MI+IzPlcinU1j3AQXZAfiQC0CKAI4Mm9YdBynTo7GwUKJlZaandky5MV4m6reqar74te/D3xXVbcB341fLyk89UlQwrEVjK3gEuKJknKTnH3mXwg1JOHA9k2rOHH+IpmONJsGB8hkOyjMFljb00loEmSzHRQqJebSa5no2g62ipqG2VF3NMfmh7U2Ginjf7FaGomoKmLMvNDUcmM5FJYPAJ+Pn38e+PGlPoGoX384NsRoiOsmmLt6gquXzpJwHXbeMsjo+DTTc3Pk0kkKszNUqz7pbBoVB9d1yefzFCqGy6k9VN00QkSQqo2GzGbbsXbueM6Da6SvNve1EHkKfEtEhuJVPwBrVPVq/HwYWHO9HUXk4yKyX0T2j42NveQT+vgc2vUNArcU+VgEVBzCsMLkxWfYsmkQW54lnTDMFcr0dHZQLpfp7u4iMrVDPBMQBj6lks9kzy08etcFfMJG4LWmdDSTVCcmnuPiBxI5qTVWXvY7T/I4j8fRhuXFYsl7k6reBTwEfEJE3tz8pkYTwHW/xUJXCR3jGN/c9Fec33QYKw6BcQi9DorTs1w+cQybn2bH5k0USmWSiUjFX7d+NRjBMQ7ZtEdgA0qlIn5uNY/fPcPB2/+aQna4calNju+aNNUdATTmtpq/E2LSjfLY5r/kM3xmyRKkXgyL0jZV9XL8d1REHiFaFTsiImtV9aqIrAVGl+A669jLXj72xJ9w8WzkELYYSPWQv/QdgnKJdGY1xWpA0nMolUoMrF9POtuFtVUc16FSLjMzU6Rz9VqmBx9k07k19Iy8g47ZtfNut3Ed1Go9ig4SO8EjE6H2i4xyWWrzH/z86U/zv+7YuezJR7VzLwgi0iEiudpz4EHgWaLlXL8Yf+wXga8t9iKvRefMBiwpQpPCeh1Upq5y4fRRNnV34STSFKoh+UKRrq4cV0bGGR4eIZ+fY2Z2hnKpQNlNU9zybiqZ9UhoyM6unTcU1oO79dFTG5lozclO1Iz0+DNAH7300rvUX/m6WIzkrQEeiYcTF/hLVf2GiDwF/LWIfBQ4D/z04i9zPqriUZIEKg7GcZi8fJpVfRmyxkNcB99aSsU8lcDHSyYpFSv093RQLpVxxdJz1weYzm7Ehn7TXNaEJkd3Y1hsvN2w75q10ShnaSV90wsmT1XPAHdcZ/sE8I7FXNSNYI2DJDxccQiNcOKxRxhwK2y8dRMlDUl7LrNANQjJdhjchENPZz9+6LGx36O8dgszBQchaGSL1b8ADTMA5rHWbPM1h6VqMcT4vxVDS3pYHNcjkeygNH6JztIF3n7XDnp7OsjPFenJJujKZSlXqswVimRzHUxNzPDss6fZ0J/mwOEp0sNfxt3+Bry+rYTWQTTEWhs7lxszn1wjfUA9ZhiZFFp/Xtu+kmhJ8hg/ReHJ79BtivTlEvRtXYWb8rBhJ5mkx7q16xgdH8cxLgokt93K0d7TpAKLdYRSQhjb/01K1rB27xtIDL4e3ymBT0PS6o7mKA+mxp8SJR/NmxubsYJ2XkuSF1w9wtYew7YNm1m1po98ocTw+BRz5RIjU0WOXZqmUq0SBpZkwiWRTOIEcOrMRd794H0owoWLE4yWhNHD3yRnDM7q9ZSCBOImQTWe0yJp0loW9jWGT20INcasqFushpYk7+7XbSM/08vJMxc58G9PU6n6nLkwzORUHsf65ItVOrKdFIoz+I5DxnOoFsrctv0Wvv3Yc/T3Zdm0cYAt3Rkm13by/X/+IrnX/Rj0D2K6N0Q5maYeq4/nNWnEXuclY187Z7ZT/14Uo1OzfO97T3Dx8gjDVydQ67Oqp5vLly+jVvEyA8yWA6oBaHGGEQ3xK3OkEy7WSTAw0Mvxs1fo7+vCsVUGBlbh2VFmnbU4nodftZgmDqRpwUpz+kX0WmPFZWVzNqFFyXv2yBmGDhxjdrbCuo23YEsjFGYned2uzfTc+T5673g3fgDiF5m6dIGRqyfwJ4Y5//S/Mjx6gSPHQlLpHDu3b6S7p4dgrsL9m0P6Sgc5f7kI/TsxJkrrk6ZMbGCetwWJEpQcasRJe867EUKnk/Wvfz933/42xp/7HtNHvkPecagmezGDdzE1MxPF/MTBW72ODWvWIcZh85t/Cluc4Oz3/5ZTR4Y4eOgY6VSaVCoFCWHHji3sWH+Z45dmKK+5EzeZjpKRmiTqWrNCrUXrmqa2TYUbYcP9H2Dk3BzF8gyzl49QKgckM91seedHqXhprF8hGtD8OHtagADjCF5XP9t+4lNsfPN5Jg7+K09+52+Znp3m0JGTjE7Msf3Wjey7YweHz/0buuXNmGQaG4YNYzzWQDVO7HWMqW9fabQkecVKlSCoUr58itLYaQLfsuX+91FO9hL61ehD8ShWe6GqWAvWKhIUSHatZtM7fo41e97IgX98mONPP8bU5DSjE+PM5fPcfedOhi8/TnnwfhwvSWi1nicaucUaGmZtHqy5yFYKLZqApLiOoWfuWTavX0vfqlWY1buwYYUof0vq+SbXriaK9lastZT9Cu6qDdz983/Im/7dvyeZ8Bi9OsIzB4/y6JPP0sc07vnHQS1GnEhx0YaJcK0Rv5KxPGhR8hwR7PhZeiiAFVZvuwMn1wuhE6v0teVZtah3I8u69lw1UkZCP8Aaw5YHf4n3/+qnyWU7GR8d49Ch53j86WdZ5UzhjB7BcQTjOC+skNRTIlorDWLFocbQNXOYYnGOhBeQ3PZ2wgAQWyeoeUir+ypruZ71+yuoWsLA4gdl0rfcybt/7Y/o6ullcnqKgwef5cmDR+ktnSU5fQHPdSPzL7oKaoHZ2rGoZZetEFqSvGxpEnfyFNPTs6T7tlIhiY0j4bVYW/Pvv64hNqU1zEtxiKMDYRCS23QbD/3qf6Ar18vs7BxHjxzn6MmTMHKIuZGLGMeN928cvxZht3Zltc2WJK984QeEvs/qNRkKXZuxmGhdAbGUxWPjvHmunqow/1iKorHEqiq+XyY1eBsP/dqn6cz2MDk2wcEDzzE9epneuZMQ2Nht1pjvotheJPEWXZEoOrSgtjnMMH/zM//EHWYje9z7KK2/n6qTajiO61HTpvmNpghP0zaA/c6TPLr5L/mF05+mV/pBFWPA23gvG5Of4ujTj+IlEqxbv56t21czxiTdg9sIgyB2lTWGzhDLV1f/v1QY5Hf4nWWvCNFy5AH0rlrDL/zG/8Ze9i76WI8zScgqPrl9x/Mj4O/aDvzGSz6WxVJlcNHX9FKx5KX5F4J9+/bpy6kxHRJGi0qWYIbReJhbqpyT2pD5cqRORIaa8l5fMlpS8pYyuUeQJT3eShTPaZyrjZZFm7wWRpu8FkabvBbGDckTkc+KyKiIPNu07brLuCTCf427dx0SkbuW8+Jf63gpkvc54D3XbHuhZVwPAdvix8eB/740l9nG9XBD8lT1B8DkNZtfaBnXB4AvaIQngO54vUIby4CFznkvtIzreh281l/vAAtd4tVGA4tWWF5sGdcN9ms3glokFupheaFlXC+pg9e1GBoayovI8QVeSyujHxgHNi1k54WSV1vG9SfMX8b198AnReQrRA0PZ5qG1xfD8YX49lodIrJ/Md/7huSJyJeBtwL9InIJ+DQRaddbxvV14L3AKaAI/NJCL6yNG+MVEVVY7C+wVbHY7/1K8bA8fLMv4CZhUd/7FSF5bSwMrxTJa2MBaJPXwrjp5InIe0TkeOzMXvJSVzcTy+3Uv6nkiYgD/Dcih/Zu4MMisvtmXtMS43Mso1N/Wch7GdJ0D3BKVc+oahX4CpFz+1WB5XbqLzl5L1OaXrIj+1WERTv1a1gOyXtVS9NSYqFO/RqW3M4TkZ8E3qOqvxK//nngXlX95DWf+zjwKWBdR0dH586dO5f0OloJQ0ND48BXge+r6pcBYkf9W1/MN3zT8jZV9WER+SxwYufOnZ0vJ+n21QYROc8CnPrLMWy+5LCQqgbAJ6/33msQXwfOEDn1/5yXkGe/HOQ9BWwTkS0ikgA+RPSrui5U9evLcA0th1jL/ISqblXVvap6w6FoyYdNVQ1E5JPAN4mqXHxWVZ9b6vO0sUxzXixNbYlaZtx091gbC0ebvBZGm7wWRpu8FkabvBZGS5J3kINLVnHBx+cwh5fkWBAVPBhmeMmO92JouWXNB0tP8vdf+jBfuO93GTz31riURlRaQ+pNKhqFxyQu6VGvh9lUk0xVubDlH+nd/+f846Y/I13cgmitxlitMqo0Ha+5oFncNKNetAfUWMYSf8CqrhS/dc+Xlr23QmuRF4bsffhxvrD+fYz/4DYmwqmoIIDWarDYRoOKJoKaa4Q1lxQWgfDK3RTDY9z/8Pf4wRu75rWRiT5mEKIe6BBSG6yiSlUG1DZKh1jLzx3axtt/9xdWZG16aw2b585h8gU2pH4drNOovCCKiK1XoG3u3dpcSKfezVmiOmLGS5EpCWbyfVRSynu/+1lW56+QrUxFN6Zeav/aOrfRr0MQtElCV00Nc0uHi7N774o0QWwd8s6dg69+FT71qeh13P6l3v5H4tqXMYS4s2V8G01zySoEJ5nC2DLFoa+QPvg56J5jzdUh7n/q/+Ftp/6CHTMH4pHTRqX+6ieTaJjW6L1aMypRywOP/wM/uO99K3VHWoi8P/1T+M3fhEyGeictE4mGubbNdXNl2nkHMVFz3mQKjME7u5/M3ChzczOMl2eY+KkH6R6bZfiHR9kQnsMLynH1aOrnjGp3WpCIzFrZqjWjFymlOpjt7Fve+zDv27zSUSjAgw/CO98JicS8tyTuHau2Vjs/mqGgMbgZ1wU3QaVcAr+MlPLY84eQpx/BnPgemVIJv+xx6uhlzh0/RjCQ40NT04SVKVzCesXAepnHa8pIx91kuX/oWzyx710r2tLkla+wuC68971wzz1Nd6258ClN27XRBsg4OKqUzh9mZvQKU5dOE+RHSXsumZTHqp4sXT1drN3osQ3Bzk6RmZvAbt/M9zasxvUsjihqANtUkK6Ohva59fwR5nK9THavZtMKJqC/8slLJuG3f/t5m+d1iazp6oBxPSjlmT19CH/kMPgVbKWMVy3R25umP5dkoKeD7g6Pvs4kGTckFZQpq+J5CShMQecqioleVJx5FefmF11VjBGMWnrKs/TPjOBKu0D4S0BTY0KgpkS4jkvx0hEYOU7p4lGsDQlDy/CVK2y7ZR29aRjodkhQIBO6zIyOUHFhY2+OVatzFKYCXAJ6bJ5jlVuo5JJxfzwQ1agbdK0rhg2jngpqOXrLbRzbupeEDUis4ETUmuTVNExqCoshlUgwNvQt7OVnqFRmqVZ9Zqdn6Mxl2b5lLbkOj0xSMOUSSapYSZExIZ0OzI2PURahqzNFNp2l6huC1BrS6kd2nEZaJWqxYYi1itWaaRLPr2LQsIr4lRW7Da1JXq2GNFG9aQl8zv/wb2H0OdKpBKViiUwmyaqtmxBj8IOAbNIlmYLeDkOX51Iq+/SmUqRdmKtaUh0Gz7OM0cHRxHamEgM4WsKxsYlgLWotoQ2J3AGAGFRNRKKGCCHWllfsNrQkeSKCK4KxISb0qVx4htKFZ+jqzNGRzpBKehTKVXzfB5R02iOZgDWdGdykoRSWcDwoVAtUfIvrKqlkJ6P0MeS8kZKTQ2LvSeSNsXXtNcAQqBAqkYEedzERFKc9590Yng1xqkUc9ZGJ81x46p/I52fYc+t6rg6PU0VYu6aXSqUCNmRdXyeuCcBR0mmQSkAil6Y661Lp2MhwopehzD5m1MNXB4cAg8aECCI26uwlii/gW6LSyHG7DREHUYsbVgjDdiOoF4dfwAnySHGWiwe/jeu63HbLBi5cuoyIoa+7i1KpSBD4rF/djagPtkqpmuEDX/w23373L3FgzX3YrFBxsgRuAtQDrWLUosYljLXXILY9pKl7s6OxgdxUZFxV8atCVRIvcNFLj5Ykz9OATCLB3KXzXDx3gtft2ErZ90kkXNKZDF29nVQrPkFYwfpVSm4SWXcfxc7tnB8sEPg55pxuxK31x7OoliN3WhyBsDQ6M9dL7sckRr1htU5ezTFgkmkkkVqx+/DK97DUMDcH//APUC5jjaFcrnDq6e+zeWA1nusghHR1ddLVtwqMIdSAarnEbHqQ0p6PML36jVS9fv7qxz9B98wY666cjVrUNHUjqc1rqlzTRrThVok+ow2lKZY6ay1iDI7TIpVuReSciBwWkQMisj/edt3Fg4vG8DD84i/C5CShl6VQnKNYmKG3q5epfBkNQwqFAiNXx3BQpqdnuHXrVjru+GmqXi8a9wIKvCSP3v8+Xnf4UXqmx+pek5qPMv4OtY31kN68Rr9NISZrbeSYdmIpXUGVZSl+Jm9T1TubSlK80OLBxeHWW+Hzn4cf/pDQuIxcOMdArgdJupT9gPHpWTzXpbcrCZpmTc9qqhUlmDiH5wheMlOXHt9Ncmrr7ew68XTUsiZ2ckeO7ibXW3M77Zp/s6lfrCA4Yup+z0hqV6anAizPsPlCiwcXBxF46CEYGkJm5shPXqCnv5PuXIZ7X7+HtzxwPzt37eDWrVsI/Qp9aY+R0UkqZ37Eua/9GYWjP6CjowvHjYa2k9vuYrx3LW/40bdwQhtLZpMLTGtOgEZEr9anqD62Qr3FTa0txko2g1qswqLAt0REgf+hqg/zwosH5yFe4vVxgI0bN77Eq3XxH7iPc0d+j7t23kG/m6MjnWL9mn4CC8VSBeMmSHfkOHPsNG954E7K+Sq9vR2Mnv0RXi6HbNhJqRyiGE5uu4NdX/8mI+vz9I98ELHNjXul3hYvTnR4XhNEaXquGkXxD9kn2e9P83rvvmUPyC6WvDep6mURWQ18W0SONb+pqhoT+zzERD8MUV+Fl3rCY+/aSMfYZYK9W9k9fAeXRyYYOnqekYlZpqfnmMzP4vqWnlwHc08eY+1AD3duX0th7R18/5+/TLLwfsLu9bg9g2BDHv7YDtaNPMJM9g10zq2jES2IUL/9tXZutSwLEwdkm9rRqPp0Hf0/+O7md/C6Pfe8snNYVPVy/HdURB4hWhX7QhUBlwR7vddxatcf4J4ssf/oCZ46cIrR4REKc1VmCgWQqOn8+lWdjE3PcWW0myOnLtKT9ehIp+hOh8x2dRKYiIXVl/8dM7m7yc2ua1Yq64lN83rjybw4b4NIhNWTw+w+9iPsA3/Mj+358WUnDhYx54lIh4jkas+BB4FnaVQEhPkVAZcMG4ONfOuxZzhy7DyzU9NM5/Os3vMAb/jZ3+P17/oI6/bcQ8kmefrgc/zw8SGOHTvJxEyZi5MVBrNzMHKUVDIFCAZDbm4d9UiFNLm9YJ45UGtTZCROr6jlywj80hf+iGSpwNyG+5/f1maZsBjJWwM8Ek/QLvCXqvoNEXmK61cEXDIcOXmWk6cvc/HCMD0Z4c4797D2wQ9jvQ5UYYNxCPIzzA2f5dg3PsuZk8eZmpqjK5vj9Lmr3LLecvLCM7BqJ4ZG86iaxjgvThiLm1zbL7b2Os4m/Myv/Ed8L8muVlBYVPUMcMd1tk8A71jMRd0Iw8MTTE/NseWO+7ATpykUS8wW5vBSUcqRimISHt1bb+O+X/3PdP3T/+TZR/8OxfKjZ45yv+exud/j5Gw/iZ41VEMfY+PhMT5HzWjnmlbbtWH02rX8+Vz3ivf8bR0PSxOmJ2dZtfk2Nr/tZ3EzPbjJLtKJdOQ81jBuZGjxKxUCdbjjg7/G/e/7ZQr5PGdOn+exJ54lKM2SnTpAsZiv/4JrOTHSZGvXJA2Ih8gGkXJNQouYlZM6aFHySoVZurfdR+BkSHSvolouYW0Yz1UxYuXChgHlcpFNb/5x7n3XT5Cfm+Ho8RM8uv85+swczsgRHDdRJ8VabRjlTZJU87BcD9LQcpbrK18XLUmek8mRHbwVYxysm0L9PNhS3WC+FqG1VK1l64O/zM69+ygXZjlx/DRnzl9mgwxTnRmJmvlSU1AaybnQ8GHGGyFWVJpxrRSuBFqSvHSuDyfTiR9USfVtpTNhCfNziDTmp/lSomjgE2rI7T/1ewxuuZWR0XEOHDpFpThLZvQwNgyo8yHN81ot3a/WATp2TV8jZaJgdEUz/1qUvK7eKFQahuQ27WBw7Vrs1AWUFCpNGdJxxnTNuxwGlkRXD/e8798jqly5fJH9h47RI5OEIydwE+l50QWozXm1F5EZ4YjBXCNl9X3a3ZpvACfRsLncDvzEajrLF3ETUTpCc2vta11avu/Ttf1udt/7dmbyczz73AlOnbnAmuAS5enRSCnROt/z9lWrhDYksHa+SdGMdp/0F8fs2DCORg13rQ3IbNwNlTwyOxxnVEa47s0VCMVw9wd+jbUbNjAzPcHTh44zOz3M6tJZHMA4Dk78A6i7v+L5zxEnJqj2w3i+GbFSaEnyNKzWb2qIoZoZJLSWTGXseSGdZtQjAkGIZrt40//yayS8NFdHhvnR04cJxo6SmTlHIukhTlMAVhXV2OtSWyZW98A0jn2tEv8YTggAABYcSURBVLPcaEnyXClSrVTqQ2MxlSXT00l19CTYKrUVBPD84U+I5q3Qr9K9437e/JO/TlAqc/rEafYfOkJq+ih2+ASJRPKa6SuWspoGWtt6rbS157wXR8JNUJ29SiPjwMXmdpOYOkPKBvWUE+SaoIwqm88doaM4i0qUz7nh/ndz/0M/TblQ5uiRUxw8dABv/DmmL55CpNkor2mejajDtTw1PKIrg5YkTxyB0jQYL3ZhKdW+XZFKP3YQaNx0qfknAcdaPvq5/5PtJ56OUxiiVbU73/kz7N73AHPTMww9fYgLp46RGD6IqRZxPLdpUWZDD7XWNsVjG07tlURLkoeTwBSn41T0SHGoiAO9m+jwr0ZTk61lhTWkITQOf/z7n6ejMMeWc88iRDkovpth3099gltvu52piUmeHjpIZfI8XZMHMeUSYpw4XtcID13rJqunQawggS1JXncmgUMFMXG6O4KDYVXZhZkpvNI49XlPoK4ZClRSGR6/771suHSSTRePAxJJUbKTez70KTbuuo3JsRGe/NHTjF14FnvxSWwQ1rPFrgvTlB6xgmhJ8sTA1UvnsaUCiWqJ93/9/+O9//RZ3v8v38UZPk8Y+A0XVmxkN+YqJXA9fvCmH2fj+aNsPn8ES4i1IU7nAG/5pT9i8Ja9XLo8zKM/+BEzF55j+tA30aAyL62vLn1N4SMhWoCyVGVGboSWS7odZpiLG7/GLz+S4O1PfYHA9fj2z/ws1WSaqY4uNpz8OusHkoRd3US3U3l+Lkn0urjhI6z70R9zoPeL3N71n+jVbozp5e4//e/88Kv/g2J+mr7ePrZu7SU9oJje1fh+tT5U153ZxFa9KD+0D9MzvJ6fG/idZa8I0XLkAawZWMMH/uwPcHbvBeDD897d9bKOtf/dv8Xo8c/wsT3b50XAP/b+P3vZ12WxvOVrffCjMvzYy979ZaPlyBtggN+650tL9qt+vXffkiULGQy3s5fYcbfo490ILUcesKTJPZGys4TJQr29K2aotyR5r2g88MCKnaoltc02IrTJa2HckDxZ5jZibSwcL0XyPscythFrY+G4IXnL3UasjYVjoXPekrURa2PhWLTCstA2YiLycRHZLyL7x8bGFnsZr0kslLyR2nB4zUqgl9ME6mFV3aeq+1atWrXAy3htY6FGem0l0J8wfyXQy24jBjA0NJSP+8W91tAPjAObFrR3LY37hR7Al4GrgE80h30U6CPSMk8C3wF6488KUcvR08BhYN+Njh/vt/+lfO7V9ljs917yzpULgYjs10ZBgtcMFvu92x6WFsYrhbyHb/YF3CQs6nu/IobNNhaGV4rktbEAtMlrYdx08kTkPSJyPI5ELE2pq1cIljsic1PJExGHyC58CNgNfFhEdt/Ma1pifI5ljMgsC3kvQ5ruAU6p6hlVrQJfIYpMvCqw3BGZJSfvZUrTazEKsWQRmeWQvFe1NC0lFhqRqWHJ7TwR+UngPar6K/HrnwfuVdVPXvO5jwOfAtZ1dHR07ty5c0mvo5UwNDQ0DnwV+L6qfhkgdtS/9cUc+zct9U9VHxaRzwIndu7c2bl///6bdSk3HSJyngVEZJZj2Hw5Mb0A+OT13nsN4uvAGeAU8OfAb9xoh+Ug7ylgm4hsEZEE8CGiX9V1oapfX4ZraDnEWuYnVHWrqu5V1RsORUs+bKpqICKfBL4JOMBnVfW5pT5PG8s058XS1JaoZcZNd4+1sXC0yWthtMlrYbTJa2G0yWthtCR5Bzm4ZBUXfHwOc3hJjgVRwYNhhpfseC+GllsZe5CD/P3//DBfeN0n2XDubagNUWuxYYDaEFQJwwDUYoMAVRuVMFYbFcMZP4OMHiWR8FidEd545Qr/eOeT/HPvbzI7uh3PVKNyxwrGcYlKVhnEmPivU39uHDd6LYJrDOIoF3r+L/q6Uvzvt3/pld0U42ZgL3v5/O2/ztj3dzGhE0CjrGK9oFvUKB0hQaPSN0giSXrqJMGhZ0mllVu3dDCWAT+9lq6zZU5n+/FDP9rDKhpQ75Wnqk29TqLSHcZYRAOMCA4KYUBH5pf5sT1blr2MB7QgeQbD+jMPMKn567wb3VQbd9Iy9forAgaMKp4YtAqEPrZQ5qRVtjzTRzFzlp6OKa4GqajgalxaxYjEHWPj4gO1CrpC1G67VqjOWlRDdtmd7DHbl/9G0KJzXjNqIa16/S+JSKt1HAFQiUpZuQh25hLJhIMVl+l8lf6NG/ATOfp7crjliUYpRiEuOK44CA7gqOLYECcMwK9CtYJWy4TVIjYoQlCOS0auDFpO8mpojkOKRPXDXrB5hTF4yQ6qE8OURq/Sl+vidQMpcv446eIMmwbWc3VuCuuMI7nBeIiMungJirGK0RCjFjQEFA0DRENUw6houImGWFnBPNiWJa/WH625108Ntcp7tbnQcT3y42Oc/e5X6PcKbNqSJdttKV4O6JqrEOoI4iTwU42amUYtLoqoxRDPbYRY66PWYqxixdYL0oGDItgVzGFuWfLqZeCahk1oUl5qVd4dF/V9Rn/4N3T4o5hEknQmTcXxyXSlqVbL5KqzJDMpzpaGyXdbDGGkgNgqIkEkcTaIpCy0UWV5iRpDEde0VmtRDFZWbiZqSfLU2gZpEOspTYW8a+8ZB9dxGT/6BJ4/RpjwyGaSqFqm8yVu7emCWcHNGBKq7MjOMaEV1AYoFggx1mKsBRspQaoQYgkVrDpYlEg2DVYNvl25W9qS5NXKBUfr1KDWwsdaG/c7iKROXI/ixAjjh/+Fns4sJBVjFPUrWM8yU4GeZAZNVPEcF9dAaEuxwmNxrEWIC7ISFV4NFXyEAEOAi48hFBcxBjUG9bwVuwstSl4Tmua3ejlhJeosGfj4559gcKCXqakppqZnSfZn6EjlKM9MMlUq4yP0ago36zLjO+AFiHExNrbxFKwIikuIUHWgqoZAHULjgPGioq2Og4jBdduS96LQ0Mb9EyIJtKpNxdoEjOB6ScKRU4yefhYXP6pbbKvk0lkmp2boEiXhGCBgrjSL41lm3A5EJWp+EVfKtcYhQFAcrDhYERAXsQZHoh+JMSaqdgsrWu22NcmLm+vWiqA2KyvGRIa0pwFnnvwGWakwWy5QLlbpTCXIF8sEHQkqJsTLZelwLEk3IJlMUmA1VqPi4jYmDIkkTsVgjYPFRL8PZ37ZYquKMcvfarQZLUlevWa0Rs8jnyOIGgyKq8LkmUOk7TSZXBqTECYr45SrJbKdfSQyCfpTCYLQYgyUKiFpD+aSHfh4YKOuJ6GJzHMblzsWBIypV89VVZRouDZIVKJ4BfuktyR50U2j7scUAUdpGNFhiamTT5ETH0eSDF8ZJZ1OMjCwhkwux2ylwLq0IaiWKKmSySWRVAd5k6JMEjGKGgc1ce9zY5qGxJrxb6Mee0bmOwRWcNxsSfIEcGo31iqOEkUUQh8jii2MkC1fRhyXy8OTbBwcYN36NZQrAecvj7G+0zCVMCSCKumuDPnCHMPOFspdA4RicFwHkailWr2X7Lzq7hr32NO4AHnktF7pdjSLIk9EzgFzQAgEqrpPRHqBvwI2A+eAn1bVqcVd5nzYMIBQELWRi8oGYEOwAUYCxp75FiOTeVwvQX9vJxsHV3P+6gTTU1Pkskl6enoJxSfbl6JYKdLhuUx5/VTdTjwp1glR1UjqYsmyaGSK1IfrSFmSWqtSjWzQlcJS/FTepqp3NpWkeKH1Z0sGF0tSfZJaxbMVTFjGhCUSEmBmRxi/fI5iqUhXNsH2DWs4+NwpxsbG6e/J0tmZZTJfwvEcQjdJtjuNkmUmtSHKMsVEzZ5EqPW7qY2WzT3zam6xa/2prUbetXih9WdLBsf6JKng2gqOreJqgCshScdj7MSTlIMKrijbN/Zz7sJF3ITHrh2b6ezsoJgvkk44VMIwMjESKSZTg4wnN+FQbWoGpI1eeCINf6nVRkNFBaxGsb+aBK5gN6HFkqfAt0RkKF71Ay+8/mzpYKsYrSJaxVEfowGu61GeusyZE4fp6+1l5y2DTE7PcnVyilVdWcrFIqVKQC6bIduRA+MQ2oCpyVlGeu7COi6qUcQdIt6uZ4qINPyn8/yqtdct1I7mTap6F9FCyk+IyJub33yx9WcLrfrn43PhtkcJ3FK9zaGKAbVMXzrE2oE15KdG6OxIMD41R393lkqpjOc6WBsgDtiwiu/7zE2XmMrs4sCOI1j8euNEvfaSY4WkHq2XxgMTSaXGyssB+wT7/ceff4xlwKLIU9XL8d9R4BGihZUvVBHw2n0XVPXvGMfIHvs7Lg4eQsUQiIN10pQKJS4eHcL1S+zYOEi+WCKVdAlCpXd1F57nkXAMXR0JwrCCWp9Sbh3rLxbYcuZvyGeHrxNi0rpZUhtB59X/amr8K7F6Olb6Et89/pkVaUmzYPJEpENEcrXnwIPAszQqAsL8ioBLgr3sJdHz+2y68EasSWKdJGGmn/yFYxTzRbLpNGoiVV/UMji4nu7uPlKZFNnOTsrVkJm5IqHXzfTAG/jBG36G0e7/RHZ2Xd3FpjTU/ijlQepapjHzPSu1iL1BMCrclfk0H9vzfy978hEszlRYAzwSfxEX+EtV/YaIPAX8tYh8FDgP/PTiL3M+spPrGMMQGiE0Hlqa4/zpw6xPp5FEiul8gYwnZDsyzBYr4JRwXaVSLeOXSjiJDIVND1Lq3Ibj+3RU10ZzFsSmm8ROAInV//hVIwMpcnzX5z7qtl4fffPa2iwnFkyeqp4B7rjO9gngHYu5qBshMB4liSLX4rrMnD9GX3eG7twaxPXwEaZnZ+mwGSphkUqpSleXR7FUwUNJ7HyQSu8O1I9alMbXXZ/T0JrxTSM2OE8RkXnKSz38rteZL5cRLZmAZI0LnoeTSGASKU788BHGzhwhm0kgGkAYUKxUmC2UyKQcMknDti234rkJ9uy8le5b7kYDairlvGaGtWig1DiRa4kD4na/dW00/lf3f64QWtI95nlJEskEpYlhcpUr3Lt1gFV9WynMFViTdrll8yBzhTz5Qome7ixTk3OcODlMVzLF2FSR8OpJ3NwgxksTVCuArQ97DUi9ZWmzUlIL1DcVPI2CwCvsGoMWJc+On6R04ClyUqKvM0X3tlW4SQ+7OkdHymPtwFrGxicwjkNooVq1HHnuJP3pDM+dOY9c/mtsdi121VZ6t95J6HVgbQWxQhCHm4ij9HXtMz537a88XxxfdPtyoCXJk+nzbF+dYvPataxa1UuhVGJsapZ8scTobJWxwmXKlQpioSOdpLOrm81bN3HpzAXe+963Mj5X5ejJq5QnjhHOniZzz4fxHaViYy1TNY7Pxe6xJkWlNg/OjyGaRvZam7wXx+vv3E65MM3xUxd46shpwsBy/vIoM3MlEoQUSj6ZbI5yOU/ourhisUWfrZvWc/zCJIMDfbzlnm0UKhVOHr/C6Df/K86ut2JzAzi51TRHC4heNSSqyd6reVbs84bclUFLknfu0iiPPf4EV6+OMzY2DdZnTV8v586eJ6hWEK8LMXNU/Tz4IV4iSbVcoLcry8WnDrF2oIdsLsemwX5Wrckxc/k8OSlR6EhiEx5htTJPIWn4OBsph3UvS81cQOsfWym0JHnHj5/j6LHzlIMkAxt2YfMXyc9OcOfODXTufge9t7+HatViS9OMnzzGuTNDVKZGGDr0LMVqidNnukilcwyuX0W2I0VOEmzWK+jVSUqr34SmuiEMsWidHKBuA9ZfC1gilV1VVnLEBFqUvJ5Ne9iS2s3ArrsZefqfmTp8CWtc/EQv3ub7yVfjrOZ0Jz2330vvnffhAFMXTjM3dpazj36NS5fOUihO05XrBmuQjhS7d27Gn3mcs9O34KzehhFBwxB4vjM62hhpmlLXNGVF81hakrzObfeS66gwMznGxKkhSsUKqY5utjz4MSqJLGFQARXCMIw9J0Ig0LlhCz1btjKw9y2MH3uM809+mxOHnyKby7L/gOXK6DSvv307u/rPceDcNJnNr4801jCc5/Os/0Vw4vwWmkyHlUJLkleulAnDkMrFI7iVUUK/zMYHPkg52UPo+8Ru/nmxOWuh6vsYHzCGgTvezsY9b2bL0HfY/60vcu7MScbGprh4ZZgH7r2DXYMDnLugsOluHEcIrUZLvaRmvDc0zHrqPc3nXH60JHlGBMcGrC6fJNXfT0euH1m9HRtW65ldIkQrXGPMUxatJaiUCR2HNfc+xHu238GBr36GZ595jGIhz78GATN7d7Fnh3Dx/FPIprsxIoTWxkTVrT3qmWxQ97esFFqSPM9NYcYPk/KnmQ1DerbehXb0EFYVEY2XZhGnqc83vWoCqRot0/JtiNO9mn0f+UPW7vgn/vVv/oLLFy8TVKqE1nLb9oDh0Rxmze5IQbGNCHvTsohYIldWY2lJ8sQEZEYfo1AqkU4Z7C1voRwAokhTTiU0fJXzpSM+Thw1CMpVrDFsfPNP8GCuj+988b9w8coVipUKfmi5c2+S2UIPQfcGqpVKvIyr4TKLjh37NturhF4ccvkwqcock+UC6Y2vIx86KOG8SPi8ga0ugc2i0oAKhGoplYusvvOdPJR0+cZf/GempiY5cOgI6ZTDoG+YKkOqfx2iwfMWdyoaSWULpUHcFGTmzhAEFQbW9lLp24kkUhgniXFcjIkqNERLFhq2lzSlKsyz1QCROKnIQrU8R8/uN/GeX/k03Z19jA0Ps3/oOcavnKdz+jhOGKW1G9O8f22dekziCkTRoQUlb5hhtg/9I1/ugL26j45gEp16Bi+RxktmcBJJ3GQKJ5HE81I4XgLHTUSEGoMRM4+4A/YJhuc+x92d/5E+6YuWjBkhteuD7Fqb4tBj38I4hnXrN7Dn9kHGpUR2/TZsvEbQ2hAhsvdCDXgs+Awdw/383MDvLHtFiJYjD+C7v7ueP93zB+wxexd9rP3+Wr57vIuP7Vn//Aj4PR+B3/7ISz6WxdI13L9iI+crohHUvn379OXUmA4JiZZ2LP421Ya5pco5qQ2ZL0fqRGRoIX30WlLyljK5R5AlPd5KFM9pnKuNlkWbvBZGm7wWxg3JW+42Ym0sHC9F8j7HMrYRa2PhuCF5y91GrI2FY6Fz3pK1EWtj4Vi0wrLQNmILXeLVRgMLJe+FlnG9nCZQC1ri1UYDC/Ww1JZx/Qnzl3G97DZiAENDQ/m4X9xrDf3AOLBpQXvPWyx4nQfwZeAq4BPNYR8F+oi0zJPAd4Be1XoGx38DTgOHgX03On683/6X8rlX22Ox3/sV4ZgWkf0Lccy2Ohb7vdselhbGK4W8h2/2BdwkLOp7vyKGzTYWhleK5LWxANx08kTkPSJyPHZmL3mpq5uJ5Xbq31TyRMQhMi0eAnYDHxaR3TfzmpYYn2MZnfo3W/LuAU6p6hlVrQJfIXJuvyqw3E79m03ea9GRvWRO/ZtN3msaC3Xq13CzyXvJjuxXERbt1K/hZpP3FLBNRLaISAL4EJFz+9WMF6rN9vfAL8Ra5328FKf+K8A5+17gBJEz+w9u9vUs8XdbVqd+28PSwrjZw2Ybi0CbvBZGm7wWRpu8FkabvBZGm7wWRpu8FkabvBbG/w9qKH+uR3QUcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.result_test(config, ['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn_geo import CNN_geo\n",
    "from data_loader import load_data\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_geo(config['backbone'])\n",
    "ckpt_dir = os.path.join(\n",
    "        'checkpoints', config['model_name'], config['exp_desc'])\n",
    "ckpt = os.path.join(ckpt_dir, \"{}-{}.h5\".format(config['model_name'], str(config['train']['epochs'])))\n",
    "model.load(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2](np.ones([1, 16,16,16,16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset amount : 1\n"
     ]
    }
   ],
   "source": [
    "datasets = load_data(['train'], config)\n",
    "ds = datasets['train'].batch(1)\n",
    "for image_A, image_B, parameters in ds.take(1):\n",
    "    image_A = image_A.numpy()\n",
    "    image_B = image_B.numpy()\n",
    "    parameters = parameters.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_A.shape, image_B.shape\n",
    "\n",
    "x = model.layers[0](image_A)\n",
    "y = model.layers[0](image_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1c653f9550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO19eZhcV3Xn79Srqt5bvag3qSW1ZMmWhWwJI9sYG/CCsdmTwWEITD4n4+DMTEJIJpkAWZgk3+QLSSYQJglMnEBwAhmzY4cAtnFsMGDLli3b2qzV2pfuVqv3rZY7f3T1O+fc6ve6tFUL6vy+T59O1b3vvlvvvdvvnHvO+R1yzsFgMPz0I7HQEzAYDOWBLXaDoUJgi91gqBDYYjcYKgS22A2GCoEtdoOhQnBei52I7iSi3US0j4g+cqEmZTAYLjzoXP3sRBQA2APgdgBHATwL4Oedczsv3PQMBsOFQvI8jr0OwD7n3AEAIKIHALwLQORir65vcvUtnedxSsO8ICEGfHvzfQdUt7r6hlBua29XbUNnTofy+OQ4jyEHB9A/MBzKPR0tqk0+WKeTi0P5Ug3icjGfyPvdFxfyXHHXyp/TTN/RgZOYHB2cc8Lns9iXAjgiPh8FcH3cAfUtnXjb//h7AEAioS0I+RD4DwTR2V9s/5h8Pj9nm98vbh7nOycAIFfazczHtMWdW17XZGNTKI9/9v2q3/U3vD6U7/3QB1Xbw1/9l1DeuuvZUB7NVat+n3vgu6H8if/6btXWnOf537/4nlB2Lhs5d3mPZsC/0zlx/4LoZ8cHRTV5lzAnzu2PFwRB9Pgk53j+f8hmFOb5x4t6vv/tL+6ZqzuAMmzQEdG9RLSFiLZMjg5e7NMZDIYInM+b/RiAZeJzd+E7BefcfQDuA4DFy9eW9Kcv7s0u23ztoPjNwJB9o97yPnK5nPqcSqUi5xg1X39OCfVboo+L0xvkPMbHx1VbOp3mMXLToTzZ2qH6jQ8cD+WHH/ySamtYxI/Fa65mDeDRb3xF9fvTD7C20LZmtWr70XceDOX6Rv4jP5ysU/3itCyJvNAU/F7yGvvPRNS5nK+qizZ/jLg5lvosScQ9O87lIvvJ8b1Hs6Rzn8+b/VkAa4hoJRGlAbwXwEPnMZ7BYLiIOOc3u3MuS0S/BuBhAAGAzznndlywmRkMhguK81Hj4Zz7NoBvX6C5GAyGi4jzWuznA98elvB3P6VdJNvixvDtLtk3zq6TdpI/D9lW6i6sb0vF26ViR1h08+cxPc22uLTfAf3bCNyvunqZ6rdzzzOhXNNUr9quefW6UP7R9s2hvKxDPy512d5Q/s5jA6qtuZk9Ac37+X0wtu69ql/cPZSIs5vj9kgC4uuh7pN3G0odv1RPkd8vbl9B9537GfPb/B9wsW12g8HwEwRb7AZDhWDB1Pg4t1m8yyFa7YsLiIlyvcW5WYrmLFSnhFAPp7LTql+ciy7ud0p1XfXL5Uvq5yMLnkfDmFazqzp7Qvm5Z3aptuEB/j1tDezKu6apSfVrqebrceeGK1XbZ558KpQ3bmB32zFoF+A0sVsunR9RbS7Bv1MG0vjXQ94XeAE3Mqgmzk1WamCO8663MiHEqf2nSJoTBC9IR3ZOlGYeVvsrtzCvOGXe3uwGQ4XAFrvBUCGwxW4wVAjKbrPP2shFLpKgtASAucY62+PiXClx4bjS8yH3DpLJ0i9jqaGXcYhzI6p9izxf07raRtVvfORUKPePTqm21lG2na/o4OSXM336WtXkOaklM6bzHtb1dPO5BoVd3vGKnq8wdLMJfR2j7gUl/OsWk0Ql91nEGP6TkkhEu++c6hfjts3L+6Lt8lyWr1WQ1POXffO5rPg+4fXjz35qDgUzY8Z54OzNbjBUCGyxGwwVgrKr8bNqlh8VVqpqGqeCKzXNa8tKNSomPzlOTZOQql4yqaPY5LniXHmlRmNJNRUAgmRp+dVBwPPKZfUYVTlua043q7aRoxwZt6hzUSgPBDqf/dgwZ9yd2L5NtTWsXBPKJ08wGcbaJZOq3/aJ1lDOpj31OTt37nuxuXb2eeQyP37mM8tFz0C+RLNSuNcSnhMsJbIR80VJ9jyX6kCaE9rNnJAuY9/JNpstFzNVe7MbDBUCW+wGQ4Wg7Gr8rJrp73ieK82TRFyEVKnqeRyidtKzEermeZ4tlILA232O8TpIE0Ue11Bfq/oNnz4ZylnoCMC2NuakO7nvaCivufFa1e/JLc+FsstrUorlo6ziHzzORBlv27BZ9Xsu+WYxRmkJLjIazYdPShEFf7dcHlcU2Rjz7MjnKiG9BzHnTsd4gCB244uMNfn8eT8zP+sZiXk27M1uMFQIbLEbDBUCW+wGQ4Wg/DZ7waTwubgldXIRSWOES61UIog4nA1pZU4SC4jsKopxzZwN57j8bXnhjvHHoIRw7aFBtQUBj9EhaJt7T+9V/VpFv8m6RaptYoJt+H7BG199+JTq9+Y77gjlH+3QvPRNrcwV/463XxXKJ3q1661F8Gac8dyDDnO7WbPw7pF8JrxbIUlAJNmnf01lIp2/JUBiUP9eB+KZEJc0nlQy57v95H4Bu9t81xuRjKDzsj9nx3TRz6+92Q2GCoEtdoOhQrBgEXSlVjWRx8x33ELhbOYUH1HHciDMBN+0yCSqQrkqq3njc2kur3VkCzN757LakdPaxOWagimtEuamOeLNNTBhBVXpCLrRfo60a6rX9+yllw6F8sr1HK03OT6q+lHuMJ+rZjl0I+ZEUfSYTIzx1GypxiumN09FTilyDC+yUQwS+MlL02wqSQdskRovn+F8BlFIOG6jGFMgC+3uTc5eg8gSOPZmNxgqBrbYDYYKgS12g6FCsADhsjP/+8SRykUSE55Yam2tUu3oIqKCOPKKiH4E333Hv+1cq8RKk9LP0qtKcuhr2ssUa0jwgYeP7wnl9s7LVL8Dr7ArbqMgmgCAq5cv5TmOsO1drZP70Ldjayi/8WffrtqOX86/e9dBJqyoSmm7PzNxJpSD+pWqLZefm1y06N6qOnCeTU3ieRH9fLufRJiqDFkt6uuH0go3oKPovaWsJKWIqWQL2RYTVkvkhZu76EzI8LzzdSCizxFRLxFtF9+1ENGjRLS38H9z3BgGg2HhUYoa/3kAd3rffQTAY865NQAeK3w2GAyXMOZV451zPyCiHu/rdwG4uSDfD+AJAB8u7ZQz6k2Re61E79W5ut6iSC/iovVK5YgrIhkQY/jmShxxRiTnmjePdE64iap16aYv/+Xvh/Krl7OL7mCvdtGtXtrG8pWaDz6ZGQrlZ15iTvnl3bqE1JIWdpUN79ccdDuOsgnR3M6c8qfO9Kp+Qbs4txf9FXW9i6LkYkp2JcXlJxWh6KvIfK60lxEHxxGFCe/kTt1flnNZz7UnowFjTMdsjJtZZuoFNL/a7uNcN+g6nHMnCvJJwGP+NxgMlxzOezfezfxZitxpIqJ7iWgLEW2ZHB2M6mYwGC4yznU3/hQRdTnnThBRF4DeqI7OufsA3AcAi1esdWG0k/fnIU45j9qBPxsOuijVutQddx+SPtpX1SW5BAXRiTb++GnBU5aTZA0eVbWrqQnl7/3NH6i29iompRgf5wSXrBe1VdvDKvmx07o0VK6Kf0/T6qtDuatGU04PDQ6H8uCu7aqtu40j+QaG9oVyUN2i+o328xi0Qv/OlOSFUxvi3r0VO+IJj0hEXn4ZNVftEYKQUONzOT/CLZqUIp9nFT8rxgi8J1omv5BvwgaCz1D087nqVHCg09cqTJK5CBx0DwG4uyDfDeDBcxzHYDCUCaW43v4fgKcAXEFER4noHgAfB3A7Ee0F8KbCZ4PBcAmjlN34n49ouu0Cz8VgMFxElDeCzkXbwdLu9V0OUfbx2USnlUpmUWppqDiSybgoPFm6qCgQTO5NJNilVpXW/Z75wqdCeVnquGqrrmGXWiLJ85gaGVL9UkmeV7sgmASAKkGO0b97SyivrtOxU3WNHA1H3r5FY4Kz27qauN++oQnVr9MdC+W+3HrVpiLjRMRYwo9iU0SMui2X530G8ZOVjQ5ARasl/Kw0cZ/84wLi3+2cLMvsZc4JT1ne6fFVXQDiZ71oH0sl93l7E5SKOIhhsfEGQ4XAFrvBUCEoeyLMrKrqq+DycxwvXByRRaltpfbzEcUbH1fGqZjEQPaLrtLp6ln9HDvUr/o1DjFfe66mS49BrCYPDPIYt91wjXcu5oKbmNQq+JGTTChx63WrQ7lqWvd7fjv362nTvPEZ4ZI6foxde2uWXaH67XuFXXajeJNqyyu1W7hL/XsmItcyOc1xJwPqnEhG8aPwpGvPV8HzkjPOV/+li9RF+Arh8Rd671iZ0+LEM+EnAineQ9JZSdOFe+OXCpOwN7vBUCGwxW4wVAhssRsMFYLyk1dEfC/raRUVtC0xXFbZ3/4gEeQYZ1NzLqqt1NLLPpJ+GKwI+2zJLQnl7z/6MdWvqpVDXTvTOvvp1CjbpakUj9/fpznfl7Sxa6//jHbLZUTG1t5e3gNY29yo+q1ewqGv007bl23dTIiR6+OciMkRnR9x3Vrec3jIJ3UQlzUvQlgzPme6QMIPg1WhrtEuOvXZ54YX7jCQb7MLV7B4dfqhrk6Mmct6YyREaW0xnv9bpD1eNERixj8bS+Qa2WIwGH6qYIvdYKgQLIAaP5v15kenCdnTRKJUkzi3WRHHmPicVxlIpZUJnvlibteKf64EzZ1hB2j3SQY6NK6hhskmvvrpD4byFY26X30Nq+CpKu2CSYtSyW3LOTJucGxY9asN2DW2bEWbass18uepkRGeb8orc1zDWXXBlE587B/i7LtUNR+XmdS/RfKkT+Y0wUYA6WqSpZt8whFuS2Y8bkP1OhPur4Rf81hEwhVlm8kx9fsxF0GEkvN9ezL9Lq3vWUb8nJwkJimK+uRIxKz/7BeezXwMC4y92Q2GCoEtdoOhQlB2NZ5VqWj12cVl4MuR4ipllhgJF7fz76vxqYAvl89hFjWGz4kmd1gz3tWf7uOyS4tzHHWWgK6yOjLM9Mu+mbB4MVdPnRhhtbilWlM4r1rBu/2jg8dU2/EzvGN+RRfvwB89rnf068FmR5DSEXSpGv7dbaJU67GDI6rf4JgwqWo8Cuckjy+j5nI5/ewE8jZ5WmyQnzthqbg6U3SFV8k7l/N26qVZlhXzcoG+udNZWXnXo5kGX6u82PnP+v3Es5PyOehmPUwxZCz2ZjcYKgS22A2GCoEtdoOhQrAANvuFK7kcV9q5iNAyooJPHG+8H+EmXXaJJNtMkkywGJ79l2TbuSldpdp2PPLHobxu/dpQrq7S/XpPsVvr2NE+1dbXyy4w6TLqXNej+qWTbEPmqhpUW10V29V9J5kco6NZk1eMZse436HTqq2peV0oHzzObr/F1dr1tndakGymNalDoPjbKUIGEqLEsu95ykeURfLdazk/Mk6O79hVlvH65YSbNSOy6nJ5z0UnymxLggoAyMo5SnKThL7vKRLXx/uhyYI7zyLoDAaDLXaDoVJQdjV+VtUuLq0kS/h4KpBwL0kO9UxWq30qysqLjJPKV0KM77yIqzjueelmUdF6Hof3dJqj02oy3jzS7Mp64at/rtouW7kqlHPjnIBy8MArql8gptW5RJdu6uvn3zPR+3Io19etVv2OHDvK85/W7qnLO5nz/dBJkbhC+lpNp/m3tS/RpsArx4+EcpWIkgu6dAmpiQS7FdOBjiyjvIhmlBGQHoGEdFH5SruMZMs5mbSie+bB53a+LZAQz1xGXysZseYE77+vgmfFcX7pJpkAFSiXnZ+kxW3SjCwV9mY3GCoEttgNhgqBLXaDoUJQ/qy30K4uPdRVlqr1yyPPPXZ8ueW4cNYooow45AJtW9URu9fyNTpMdXFa1PVq0uN/6bs/COUrOjn8tKZa27KSs761WhfQ3bvjiVB+3123h/LIsM56W9HdE8pPPfF91bZsEZ+7qYpDXZuT2g4dHOExJ/Oa6LGhifcmmmr5GmS8+35mStzbGt8NKolB+b0UJD2Xq7hnWT+cVdjbQYJt6kxR2WQp+/s94rlK6WsgQ19VSG9R7QM+d9IPaXVzP3PFJZvFcVHZmufjeiOiZUT0OBHtJKIdRPShwvctRPQoEe0t/N8831gGg2HhUIoanwXwW865dQBeC+BXiWgdgI8AeMw5twbAY4XPBoPhEkUptd5OADhRkEeIaBeApQDeBeDmQrf7ATwB4MPzjTerihSr6jKrKc4dNvf3/md/DIlYXncBfwwZUae5573fIhgTUh5RwY//iWtgNtfoaLL/eN3yUN68h91tSdKmQIOIZHvyiSdV210/8+ZQ3rZ9Wyh3LtYEFTIZrNqL5Buf4Gy5TMDnrqrX7rXWesFj16vLULU3tfL4Nex6GxjV12qqdiV/SHgRi1J9lq63IlMumr8wAVk6TPTz1GBVbtnLWMsLE8Ln+ldkEdJlnPefbxHl5507SaVFCsZRLIbPaszzfFYbdETUA+DVADYD6Cj8IQCAkwA6Ig4zGAyXAEpe7ERUD+BrAH7DOad2e9zMa27OPylEdC8RbSGiLZOjg3N1MRgMZUBJi51mas18DcAXnXNfL3x9ioi6Cu1dAHrnOtY5d59zbpNzblN1fdNcXQwGQxkwr81OMwbuZwHscs59QjQ9BOBuAB8v/P9gKSectS1iM9b8SQpbOSsyi/wj4mq4nQtpZaljVHvMKZk0/7alaT3Lxw++FMrHx0dVW/P1G0O5vY3DSDvbO1W/Rx95PJRvuVnXcHt+69ZQbhO2/eiUdo29vHNHKK+/apVqqxaZXacHOWx3fEyHJ9c0sM2ezXjuwUkeo3eIM/MWt+qw3XHHewnFYapz7604RIe6+uwusryzDGHN+/1S/Dmb0/dMmt/OO86RzH6Upbr9bEfxW3w2nQiC0sDjjc+J7MrSuJw0SvGz3wjgFwBsI6IXCt/9LmYW+ZeJ6B4AhwC85xzObzAYyoRSduN/iOgk9Nsu7HQMBsPFQvmz3kj/H34fU+5WkgIo/SWOcNJTdKIinXxuA5lpFXhuFhLuGUlYQaRdaLKE8I+/9g+q7TpROlkrvsCpE0z82NnBqvvxY3o75DUb1ofyK/sPqrba2tpQnkzwHN96w3Wq39Qol4EeGdfq+fImUY6omeXhSU0WmRZPT2NK37N9/Tzn16xi/vrD+zTZxtAqnmPK49HP5QWxhcwM87aasuJe54vKP7HqrkyBIpIL8cF7/uRzUFzqiz8n0zI7zsvIFHLSy3qTEZFyXllvDGnOBlFmsJFXGAwGW+wGQ4Wg7Gq8vws/i7hdcM0tJ/rF8NlR4CdLSBVORGb5FTtjKn06QX4gtcUAumxRfYK523dufVS1HU3ymNddu0G1dS/h43bvYfIHnydvVBBuBFU6uq65jVXmFd0c53Sy/4zqlxSqb256QrVl86y6J7Lcr6amRvWTfOpTXhTh0i6uztoqvAJjnZqjfookZ7rmnnd5JgEhkbCUTXjqvvwtvvlGchdf7pZHR1/6UCZgUT8eU6rufj85Rlx0pzwubh345sQFj6AzGAw/ubDFbjBUCGyxGwwVggUjnPRJI+Ii6FQ2W1ZGGMXxxkdn1cmj/L92gbT/sn6dMD63LMvs8U3iwJPfDOV1PYtVW2czR4wlPdLD3j6u73byFLuopsY18URLW0soL1+uCRw7OthOD0TU1uDomOqXGWbXW3t7vWoLath9RxOcz7Bz70HVr6uJbey2znbVNjIyFMr7xZ7Aqarlql8+wfsA097jKM1euc/i19mTtq2fsVYqAUnsMWJfp6iMt0Cc3Z8TmXp+NWeiue15v26BfL79eZTyO+3NbjBUCGyxGwwVgvKq8cSqju9+iC1bI9WjQLpPtBqcE5F2VV6UUkK4PiRnmfO4uSV/PYrmKD+LiK76VtWvbWpnKI+ltTtpaorV6azHkz44KvjpBM/a7W+7Q/Wra2DVd2Jcu/3ktdp9kAklxka0Gr+ug+flR3u9fIiPW9zI/RJVWt1f0sMJNC6j3Xf9Z9j0SDdxUs+R5ptVv2ROllbS85BkFpInPUnRaryvIkuXa1xZ8HyM61c+O6UmThWTs/B9SXguY7kWUik/rlLMsRQTwiLoDAaDLXaDoUJgi91gqBCU2fVGoV0TZ/v42WZRJZET8L8XNrXXJMsXy7DXIhtP1I/LO+16qxNZWJNNU6F84vHPq35tgiv+h08/r9raO9hV1t45rdo2P/PjUP5P735bKDct0oSQ08LmG53UY/T3sa185MiJUF65TBNO5kS8b0+z3nM4evRwKC9q4XvR7dWVO3CU68XVVHvpg6Lu2dAYE2cchWYczyeZXz6V0C5GmSuWEjZ71q+BJkOhvX0WJ+vzIcY1G0FuCWhXX1wYrDomphS4jyiiVN/ul+5qvy1cIxYuazAYbLEbDBWCsqrxBKmie+V3ZFaQHx0kZPnXSbraAM05kPfKEUkuMuSjVSUIFT/h8ZNP1vD5WibZDXXy8NOq3w4wv3p9XYtqS1ezSn7mdL9qu/t9Px/KrbU8r8ER/Vu2vczqc3/vSdVW38AmRJtQ/xc16Iy1Zcs5K21oTKvPbe1sanQKDf9In+bMa2thl9rUpB6joYnHr0/xdRtIavddFbgt4ZFX5IWNpdyDHrmEymiEhuypojb9rDERLZnw+Otl36wXVelHgvIYnrovIwA9s1SOITnx48o/+e7S2WjMWBd2ZIvBYPipgi12g6FCUN7deOfgCqq3Twyhit44r+ySin5jNSrlJcLIiDfJA+cfR0LOe7u3gVAR/USb6ipWhV/8+v2hfGDPadXv8k3dobx39zbVVtPAquqaZTpJpmsRq+C9A5wU8+9PbVX9pJq5qFFH6C3tYLOhsYlV5rEJbQoMih3yU4c1x92Gy8W86pgLr2pU7/y7JKuSTfUeb+AgR+wdbro8lMnbtc9PiPtS5KGZu9quf8+SIuos65leUYQPrqg8U/ROt4Qf4RZV9TdO3Y9LYonbtZfHFSXJFEyD2ISyyBaDwfBTBVvsBkOFwBa7wVAhKKvN7uBCWybONVEVePaUcFVIkkPyItwcZPSbl9Uk7HTpskt6POMJ6XrzHDlt1WwPt1WdCuXhtZepfo01bNcN9GsXybXXsFvuylWayGFikEkht257OZTHp/TvvObqFaHcVK9t9uwk28rSDh0Y1JzvI6P8uadF34smUYr52088E8q3bFyh+tXJ2n15bYv3H+ZsvCNLb+A5YUr1CwIRHeh0m4sgjUh4tr20031ii1REme2cZ9vLEX27N1XF+yy+vR2V9ea75FRmWzKaYEPOvyhKLq40WfgLzsP1RkTVRPQMEb1IRDuI6I8K368kos1EtI+IvkR+pQSDwXBJoRQ1fgrArc65DQA2AriTiF4L4M8AfNI5txrAGQD3XLxpGgyG80Uptd4cgNnQqVThnwNwK4D3Fb6/H8AfAvhM3FgJAHWzZ/TUKBkUlfDdZoLUQCa/OD8PxkWr+LI6kVKw8lrNlpzkTaSjzrY+cl8o953miLFdu/arfvkrrgzl1iatzl27nlX38VGtWr+wey/PQ/wdfu2161W/Je3sGhsZ1jXva1rZ1Dh5TEToZbXbbP1KnkdDY61qe2kfJ8K88XouV0XQBBi9/Ry9t3ztJtWWOczVavtb2PUWTA6ofo6Y9MInlJCEFbJCqv+Kkh+dV4E16+Z27SV8t62ArwhLFTwuQk0mbPmuMZncNT2t70VaJA1JvjvfXJGRpf782Sw5T9cbEQWFCq69AB4FsB/AoHPhlTwKYGkpYxkMhoVBSYvdOZdzzm0E0A3gOgBrSz0BEd1LRFuIaMvE6ND8BxgMhouCs3K9OecGATwO4AYATUQ0q6t0AzgWccx9zrlNzrlNNfWL5upiMBjKgHltdiJqA5Bxzg0SUQ2A2zGzOfc4gLsAPADgbgAPzjsWHJL5qVBWyMpMNN0UCDudhOxzWqiQWy8jTrripL0TwOOvr+VL0uxlaD24+bFQvummW0K5o3uV6rdzx/ZQfvubdQn7FPGk957Sms7B0/y5o4nt8kW1up7biWNMCFlVr+3tnAhpPXqUyStef+M1ql9KlHOe9upWS479bIbdYUOjOksvLdyge555UrUNb/xNHk+4A7NFLlFhh0aVIYbmUYyJCC12SZVQW/B82hTJpPgtRSWbS3TLyWvgh9zKtuI5zs8bX4qfvQvA/UQUYEYT+LJz7ltEtBPAA0T0vwBsBfDZEsYyGAwLhFJ2418C8Oo5vj+AGfvdYDD8BKDMHHQuJIdwzs9OkqqMVnNIccvJSCpvdNHms2/nZMaTMAXySa0iVwkXybbH/8mbI5/w6w9yiae33ql53S9fyZFmzXWaP27fYXZXbdt1RLXVpFltW7WUs82GBjUxxOgQZ9m1LtHccv3HOINt2WVMQhGkvAi3AY5wa12k7aHWVmasGBgW/O+eOykjyjnXtuk92+cDjipsyLDZkU/o6x1X0ki2yVJZfj+tFnv8cTFmgkQc53upbbGc7eLccfzvUcfM9VkiCObmdlTHl3RWg8HwEw9b7AZDhaD8ajxmdou9/BOlfGWzmmgBEeoLedpQQuwqZ0lHKeXFCZLiZyedDulvFSWZnjt1SrW9+2f+Qyj/xWf+OpT37nxJ9bt8xZJQ9tXnw8d4x727Q++kL1/Gqu/xE6w+T4zpXftXXc0q8+6X96m2tnamau4/yer+WFuj6kdZHr+19XLVtvNl9iZ0poXJU6Pnm0lzHNWJ7l9UbTVZjuybTvFxbjp6h5m8XWq5oy2jxwLveYgikAC0ii93vf0dcTWGX2VV2ItxanKpFWOLPAbSTJBWgRcl5yd3ScyaF0ZeYTAYbLEbDJUCW+wGQ4WgzLzxxPZWDOleMogrWyvccL7vLS9FLxpLEgQkBEFhvbZxvv3AH4ayXzKpuordaOkc25OvWtWj+nUv7QjlvYe0e016r5pbNKf8Dpk9J+a47godoSft3Pa2dtXWsIgz9fp62WZva9ahymNZdr319+vIuOZ6do8F1SxPZPQ1rb/qXaHsRzNmMmybOxUBGf1+8ct4V4nrLaPJfJs3zpXlIux5/1zS1vVnWGpEnV+KuZRjitpi+sW53uLGDI+ft4fBYPipgC12g6FCUHbX2yxfvNxgznUAAB+TSURBVK90SJdDzuP01m4Syf3tJfcLOU9egovgWk8IV1B9TYfqVzPJ42/5oXapHW4+GMrvfsubQ7naU68CEe11/JRWkTs72TQIUvryp9Ksgi/tZvW8rkHzzI2MchmmgQFNBpFKs2mQyzIxRM9S/TufPXYolKdGx1VbhygblQ1YDZ5cpLn2joNNg3xc0ob83qvQmxP6v6+KZlVJJpEgktHnkup5Efe8lOM43KT7y7NJEjERdOpcMePHuf2ixoyrdFyqm0/C3uwGQ4XAFrvBUCGwxW4wVAjKbLMLLvBorxkSvr0tGkmQTTgvrtGJGNy8Z89Ld1siybbx81//a9WPhH2cGNbkFRtENtuwIMFM1jaofif6+0K5rlFneS1uYK713ftfVm2XreEw2OZm7pfxXVI1PGZPT4+ev+DO717ObVtffFH1Oz3KIcmNVV7dsBRfA2m+9q36WdUPecFR75Fz5uW+i9yP8e1hYYsHgZ6HtHOle83vF0eyqI+bu7yyj4QfShsXgir3a/Lzu7/8ORXhHMNxw3lY1pvBYLDFbjBUCMpc/omQL6hI5Kk8UlNy3p8gp3jiJAmA596Q/rukHn86ya6s6gZ2LS2r0Wr2tv1M/jB+Rme9jYPV+EFRqSg3qUs2d3Swm6utbkK17TrAUXJXXrZStbWv4M8Tk3yCKv9aiYy+06f1uRsbRXZbjl2Mg4PaRVcnCvgsWqyj8MZyfH1GX8O1P4KMNifyWVEWibw2qVrL6XtaZk7aaNlo8gqVHedHTkKabx6xhXhepNUXS0LhQwyZKGJMEWPEcMQpdb8o8nPuaxVd4km7qgH+bXEOOXuzGwwVAlvsBkOFoMyJMECioJLnEZe84CWxKPWFVXqf7ECpaYFWz6uTrBa3Zvm4B48/pfo9t4d3rf/37e9RbbvT4gRTgmJ5THPETR9nzrUab6d7SRuTS7S066i2p5/dGspXrBHRalkd4ZauZjV78eLFqu24PDd4x30055E6iIyc+glNFjLyul8P5Zygks57XgFJIpHJRUfQlYp4brloxEWnRe2Qx0fC6c9SdffnGMUtFxuhF3nm+eiiEd1WgiPA3uwGQ4XAFrvBUCGwxW4wVAjK63pzhMz0jO3ivCg5iPK88NtEplReull8jnDRL5fQP60uzZ+/9un/GcrBqI5+e+dlXHr4cIsmrZwelyV8OCIvVa/JHKcn2cb2y093dLKb68BBTWwxKsov79t7MJQbarWd2NjMdv+e3btV22WXsa0/mhPzmtSl+HLN3aE8/rrfVm00LUozE1+3s8nCirNfo+Db+ecyhm9TR7nbzpU48lyyzQAva6+oXPnZZ9X55a1nP14QwslC2eatRPStwueVRLSZiPYR0ZeIKD3fGAaDYeFwNmr8hwDsEp//DMAnnXOrAZwBcM+cRxkMhksCJanxRNQN4G0A/gTAf6cZXeZWAO8rdLkfwB8C+Mw8AwGpGbeR84jjM0K1Ia+yqoyMywu5KI5K8Gy7wKviGnA5paY0R8a96Tf/m+rX/29fDuWXe7Uaf2UXu7n6jnOkXY3HDT8xxceNDo2otpwgtug/fUa1Xb6CVevjvZzQ0tCgI9yyGTYTVq3S/HTS9TTSy1VcVyzSnO+pO36H5zutK45OJLgv5dgt53y3E6LVzygXUqn9gOiyS3FurSI1+zwJH3yUbsrofqoia1EE4DmcO+JaXYjyT38F4HfAgYOtAAadc7O/4CiApXMdaDAYLg3Mu9iJ6O0Aep1zz53LCYjoXiLaQkRbJkaH5j/AYDBcFJSixt8I4J1E9FYA1QAaAXwKQBMRJQtv924Ax+Y62Dl3H4D7AKBj+RWlJfwaDIYLjlLqs38UwEcBgIhuBvDbzrn3E9FXANwF4AEAdwN4cN6xQMgUaqv5+fsZUaetyK0gOeXTggzRC6sl4b6rTWvu+TMvPx7KP36Ga5ltfVrvK77tDTeG8huv36DaTp7izLF0ks89PKgzz5rr2OY9NaW1mckM2/NNLc2qLSFs/6bFbOP1nzqq+v3Gr7w3lD/9hW+otuokO0UCx/b2+9/zbtXvc8OCbMK/jmLPgRLS/QXdT86dopXEWJtajoFoe1iO77wQ2GhL2Y8ilXsH3rlj7OhSONlnxoguyxwbBktz95O17gC9H1MqR72aw1kfwfgwZjbr9mHGhv/seYxlMBguMs4qqMY59wSAJwryAQDXXfgpGQyGi4GyRtDlAUwV6iyTnxUl1LSk53qTifoyqSkFreZUiSEH0po/7kf/8ntiQD7uvf/hnarf0YOv8JQ8sob+PuaWqxI8aOkGXVppaJBdamNDY6qtOs0RezVJPf74gCCsqGMOusVd3arfI09wdtzP/NKfqrbP/9Fdofymt94RyrWLtLNkfJIvZI1nUqVFpl4mpuySUkc91bRU1VeOWRT9FmEaxJkCxWNIE0IQWRR56KQLLdoF6Ef5yYw7v6SURKkuRjl//1xxv9vKPxkMhhC22A2GCkF5qaSJQAXSBF8l0ckAui2V5h3mnFTxqzR98ZggxFiT19FvRxddG8q/+YE3hXImoz2GDa0iSs7jd1u/cR2fa4y55VatWKH6jYyx6n66t0+1nTzNO/odHvFEVyeXburr5X679hxU/YbH+LeNPPwF1bbhVVeG8vef3hvKjU06Cm9VE8+5v0pH102IRJ6k2N33VeRcTpZnio5qUwkt0RvRRbp1NGmEfy6W40orRUXkxc3XH98/Lgjk+zJ6/Dg1O5ZaWiCqHNbMHOffnbc3u8FQIbDFbjBUCGyxGwwVgvISThKQnLXZi4LkhHsjqaPfZDme+hTbkPt++JDq15HhDNze7KBqu/ImUQYosTOUdx3WZJHVizg7Lknalhoe5r6LBYHEQN9x1S9Ry3sJiYT+oatX94TydFa39Q5zKeZpQfTY7ZV4euRfnw3lD/yMvlZbT/KY6TzbeJ/5i79R/Wrw56H8xnf8nGobaOb9jbarbwllP6JLlUly2u0UZV8mYyLtpr3xo8griu3f0mzx+Ow7EaWZjC5D5UMOc65JdaWSTMa5AEs6z1kfYTAYfiJhi91gqBCUVY0PEKA+ORPZdgZaza5O94Ryqv8Z1Xbihe/yh+H+UOxq0G6WZSt4jKEBzZJVK7pmsxzFtmG9dn85oUrW1ujItSZR4TWX5SSTW2+5Q/XbvPnpUB5Z3KTaWlvYveY8lXZkhIkuVvRwRdcXX9Tc9iO3M8/cVdfcotoGiftKHrvbfu79qt/UoitC+fSkNmV23P/JUK7r/lwoZxs0UcZbPsTReyd6vUqzCXY5TqSEKj3hqcQywi2IeffIfp6nKs51FaXuJrxzqUqtLjoKr0jNFuq/JPcodtHxAxgX5SdxNu7BUtR6e7MbDBUCW+wGQ4XAFrvBUCGgUrOTLgTaV13lfu5PZsgWRl54XLUNbf9mKAcJbYt3d3JWWSLJbq2Weu126mjmTLe6Vm0rb1zLoa7SLXR6VIfEnhYkkP61GRoSNdcSTKIBL0QzL8gVkkndNjrMobTktKtpcQvPOVnNIawrl2pbOVXPY37k139Ptd1118+G8tqVvDex+eWTql9/H3PWT43qWm/bJniv4rY3vD6Un/j8J1S/N960PpRPX/NB1TZOfJ+qJvjc4/kq1c+JsGbn1eqmiAJmirwRvqvMz8ybO4ssH0MgUbQmxP0sCscVx+Uy2eh+LnqMKNdenN3vY9aGf/DP/jP6Du2acxPA3uwGQ4XAFrvBUCEoq+ste+Yw+r8yUw64uU6XK67t5Ii0sQmtrjS3cIZWZzu3re3uVP1WrmR1dySnVdOdO14O5d5BdjUND2tVenyC1cpUtS77PDDEfHJ1dWxa9A30q35SzZyYmFBtyQSP2digCTa27mE++5pFbKI88LUnVb/1q7tC+dnt+1XbnW9nlfDoKI/xmiuXqH4tt90Wymd27VVtbYeZwKN39w9CedVVG1W/7/6ISTRuPPGXqq3pHczH3+/YDRec0apprmU5f8hOqTYdnVYagYTfJlVh6a6KM16LSDow9xiAVuPjIuHkZ98MiXLLxZKFnAPszW4wVAhssRsMFYLyVnGlJLJVM+p6VYNWSVrrWCVvb9c7tq+7gSmd01Ws7m/buV31e+HxH4Vy/6le1ZaZZvV8YJJVpdP9ugTT6QFW8dvadXRdcor79p5hldN5fzMbGlnF902BU8d5zr0N2mNwapCjClsW8W72/v060SYzuTqU8x5pxKJm3sV//KnnQ7k2pSvNLl3CpB3tzdocet2NHF2XH2Uz5+jxUdWvpYHP9aOnvqfaVvzdr4Xy6MZfDuWRV3S/le/6A/FJq7cJ4s+q6m8RRxzLRSkykdkp0ck0PiRfYjx3XWkRbr46Ls0QSYful9vKC89CEJNQFAV7sxsMFQJb7AZDhcAWu8FQISirzV6bTmPD8h4AQHubzkq74QaOcHPQdu6+Q1z+aPe+LaF88pR2eQ2Nsptrcky7vNauZrfc/uPsWho+o7PvBvrZLt27/7Bq66rjiLQ9gkO+2Yv4a+9iN9fRY5rQMlHDvzsb6HLO6XrOqlvVvSyUa5yOfjt8nMkoybM1mxfxnsb2l9il1tysSSWHRni/YE+VJsU80s9u0bYWdt9dv1G73rpWstusb//Lqm3gJN+bG2++KZQnN61R/U7m+HpPpPUzkZ8WnPUyEy26KvMcJaqiMtbO1a1VGkOFv1cQ51KT9rzcjsjmtM0uIwWL5+u8/4tRan32gwBGAOQAZJ1zm4ioBcCXAPQAOAjgPc65M1FjGAyGhcXZqPG3OOc2Ouc2FT5/BMBjzrk1AB4rfDYYDJcozkeNfxeAmwvy/ZipAffhuAPqalK4fv2MirhaJKYAwIs7mBdu7xHtauodZFWvt5ej2EZHtStoRKjxxw6fUm0vbtsTylWCTOEtt2nyh3/456+EMlVptXLPwR2hnKpmEorxrFbj9wie91RKuxEnJlgFr8lplWsqx26uh3bzGDVNLarf5BGexxVrdJLM3/3950M5SLE5lPPcOPX1HL13vF+bE4ePvBTKzW1tobz5uVdUvxeeZVfnJ//wo6rtnnt+K5Q3vfAvobxzTL9fqjYwqUZDoLn+x8X1oFw0b7xWzmNcaDQ30QSg6xb4Yyj3mj9mhJkQl8QSR0ohyxunPS482S9bxKE3O35cldzS4AA8QkTPEdG9he86nHMnCvJJAB1zH2owGC4FlPpmv8k5d4yI2gE8SkRqN8Y558ivhldA4Y/DvQDQ3to8VxeDwVAGlPRmd84dK/zfC+AbmCnVfIqIugCg8H9vxLH3Oec2Oec2LfISPwwGQ/kw75udiOoAJJxzIwX5zQD+GMBDAO4G8PHC/w/ON1aegKlCeOejP9AkiieFLb7/uLa3+0+dCOWREbbjxid0ltT4FNvsExM6m42IP9eImnDf/74m0bjx2qtCOVWjbfGJDNdRe/tv/N9QPjzm8dzLenR5z3bLsEtq+JUDqm3nj38cyoeOsT08cnyf6peu5tt26Ii2ow8f5j2CXBBth8o9jO4lupzzmSF2R64SmWgjXm29jdffGsrf+cE2b47s2nvda9llV/v8D1W/J/71j0O5452/q9oyYs8hQ+JeZzXZgyKlKLFuWpz7q1Tueb8tSi4cGdmmxhREmBmfmBLyfnqjl+A5LEWN7wDwjcIEkwD+xTn3XSJ6FsCXiegeAIcAvKeEsQwGwwJh3sXunDsAYMMc358GcFvxEQaD4VJEWSPopianse/lmai0bV5kWXaU1Zfefh0xlhQlnw4NsIpfl9Hqc00HOwT+y6//mmrb+SxbGUPjHPszsO+I6jdZze6fRWPabdb4uveF8vZe5pJDRruM4lRJEtlb1Nmj2q64i91oa9wvhHIuqV2AHYK77qmHPq/a+rZ+O5T3irLP01Pjqt/wGd5ieWFIE30EQmW+bDVnxLV6ey57D7IpcDTQ7sHFy5lw5PGnuFzV2tW6vPVb67mU9CPf/XPV1rL+naFMLZzpl0vr6MjqKX6MR9NaxU9N60y6KEiuunw+WieOK5Ucl/UWF+Unx8zIMlqebp4SmW5Z5z9j8+vxFhtvMFQIbLEbDBUCW+wGQ4WgrLzxrYsXu7e8810AgEPbT6i2sTxnmE0Pa6NmUnC0f+B32K4brV+t+k0v4qy0F/7Pr6i2Za1s9z76PXa3rd94mepXNcSZZ3tJz/GWX/4iz1eyhmS0DSnttWIbLzpsUvcTYwTaZkfAexVBoPcVcuC+1QHbq9nNX1T9vv0PTBDZ7zHETOfZ/s46donW1jaofl31zORz8NRR1XbNeg6H7lrK47Ut7lL9NmxkG37ZYr0n8PD3OXbrcC27RFetv1n1GyfO2ktMavdgFtoFG4X4dSCfx9Lqr/n3Pa5Nnjsn9w48F2PaywqcC9/8+C8Zb7zBUOmwxW4wVAjKqsbX19a7qy6fcdlPjh9UbVNpVqfv/aNPqbb940y8kEuwyplI6wi3rgS7iZ78u59TbTVpofqK0kT7DmsX4JWXcb+ut/6zapsMOMvOZdh9NzUd7Y6JU9n8SCoVqQXJce5rZXwNkh7hZEr8/R6vYZOkAdpVM5bmqLnGY4+oti/+6cd4TpPsluuf0nQFOWFOrFyyXLVV17A7rF6o/7X1njuzoTWUr9u0VrW95nIm8HjyGSbq/P4p7XJd+0Y22YJAu20TORFRKIgd492jMfclhg8+LgqvVBDJaECvLJS81/m5zQlT4w0Ggy12g6FSUFY1vrau1q1dO7ODfv17/0S3LeWd9amsVrGyGd5RDYiTI1LVut/TX/h4KC+v08QWE+OsEu3dxYklPV16h/ONH/ynUD4wNqbaEnlBBuHEDrwXpBWn9vmc51HIi2Qav5opyWqnCW+8JLcFYje3lnSU32Cad8XTySHVVuuYsGLREEe/ffr3/7vql3E85viEjsKrauJrlc6z2bRkSbvqVy14/Rpr9b3o6WEuv1+4645QfvLJ51S/fz/I0YFX3fkh1TY9PXdV3lwuOgItllzCPyrCLDsbDjp1LnHqvB8VJ5JkKD+3OWFVXA0Ggy12g6FSYIvdYKgQlDXrrbFzFd704a8CAMbHdRbWyCiTHhL57irhgkiynVgX6Fps+V4mrRxq1HXUzghyysVtXIut4+prVL/DWXbrJDI6YiwHScwozSI/wynGxiuREFHypBfZf05sEnimZyIrSgMLe3IE2h5OTnNGHKb1+FMBE2z0NVwdyh/49HdVv33f+ftQ/tY3H9DzGGZbeZj4uuUO64i2ZIofwYTvlmvle/i3n2Ui0Ntuvlb1u1UQM/7we3+v2ta8mTMVp6b5YiWdjnrMyqw07x4V2c4CJN1hoptv92u3XDR5Rd5Fl5/23W1qHiXQ2dub3WCoENhiNxgqBGV1vbX3rHN3fewLAIojmDLCvRbHD5ZIcsTVkSe+qvqtruISxVu271dtExPs/rnhSk64aL3jk6rfCTrNH8a1lZOLUrEi3CDAHHxjMepWnEsmavw4xI0nr79f/lceVy049jOBTlSRpCJdVZp7/nP3Mj9d7xi7EcdIu+iSju9LdVpHxnUuYRdg93KO+Fvepd13r7+GI+8GR7SL8bEDfM9W3czMaVkveSmXFeWh/csrVPW4yLtEqZz1MQ+BjJoLguh3cdQzYBF0BoPBFrvBUCmwxW4wVAjK6npzziGXmbGNfNtH2ju+NRKI7KpkDbvsmqafVv1ufPWrQvnYgI5hHTnF2VC7TrMNeaUXRhpMsLsnkzqo2ijDJAyx9b/i+MNjzPJSQyrjEHVcnCsojqtwRFzGZF6HIAtuDBybbFVtd/7Nk6F85B+ZD/57j31P9culBNf/pHbL9fayC3Bimtv6B7yacNP8+Q2btCv15iU8xsNPfCOUX3X7XXoek0yyOTWl7XknQ2u9a5VISFs8eq9GZtwlEtHLTo4nj5lpK43bPnLseXsYDIafCthiNxgqBGVV4wFWVeOUDvIihxKCty3dx5FZl3ucaNuOMaf8oYP9qu0db+jh8W74vVA+PDyo+qWE6jTm1ug2x2PGl/phFJEdCDk2++kc1fgL4UqVY6QgVMmcxw0oovJcjb7erePct/Pu+0L51970qOr36Y9xaeeppJ778CibbLKkVqPT5cH6WvgxfnrrdtV2/TWXh/J1YzzHnc/rsl9L1t4QysmkJkVRPPIx1zcXU7I5TgXXbfx9XOZcUXRdCSjpCCJqIqKvEtHLRLSLiG4gohYiepSI9hb+txKtBsMljFL/PHwKwHedc2sxUwpqF4CPAHjMObcGwGOFzwaD4RJFKVVcFwF4A4BfBADn3DSAaSJ6F4CbC93uB/AEgA/PN96sClMUpCTUkiqf/reKE1cOPfyZUH7HhmWq278981Iov/vW16q2Jzdz9dCea5iUIpHXUVvTch7utGpDhCp2Npxlasddj648Eprjwr9a0Sp+gqLVRYW4JvXbgsh+JMpQJSb0nIbEx2CSef4mlrxG9fvtT3OCy9998O2q7UTQGMrjI6yC75nQSVQHB/g+XXmlfnYmxc76nTffyOMdPKz65SbWh3K2vlG1Ic9juozeIc+KGxWUaHpREYHJ3FF5Z2MKIHFhduNXAugD8I9EtJWI/qFQurnDOTdLrH4SM9VeDQbDJYpSFnsSwDUAPuOcezWAMXgqu5v5MzPnu4KI7iWiLUS0ZWL0zFxdDAZDGVDKYj8K4KhzbnPh81cxs/hPEVEXABT+753rYOfcfc65Tc65TTX1todnMCwUSqnPfpKIjhDRFc653Zipyb6z8O9uAB8v/P9gzDAhSonwygWdqq1NkCo21bO9tv+E1hSOCDtsQ4+O6Lr23j8N5f4JttPz+SnVL4oHHNAkBqVyifvQv79UN1npbrhzcb3FRtfF9Ivbt8hm5y6VPO2RlgRpjlj8xc88ptr+5j+/KZTHkkxskfZsXhLZiK/s36tPKNxm33uGsyJvv2Gj6vbjLf8WykvuuFu19Y+Lkkw5r4yy5KIX38fdh7haArKtKGPyPN2qpfrZPwjgi0SUBnAAwC9hRiv4MhHdA+AQgPfEHG8wGBYYJS1259wLADbN0XTbhZ2OwWC4WCh7BN0sfBUlCNjFM1mr1fMD3/yDUF6/mt1tg6e1G+Q9P8uum5de1NziyevZWZDKS/eMVuNVVc4iznfMCV8t8xMYJOLMBHlJoqqDnjui3XelRmrFRfVlclptTwpeuKwgZEgG+tpIxvqJjOYN/MgXnwrlT/0Ku1IHtCWAfJ7Pncno+3l6kJ+lxEF+xn6Q0C66165nl+Bjj+qyX6te//5QHtWeWmTktYox7eJKT8lrHPfsxCdYWSKMwWAowBa7wVAhsMVuMFQIykxeAeQKrpBU4Nvs/HldRhMbjizuDuXhkxwa+ePNe1S/2+7kcMiGpTeptsQo23UjAYcEBNk61U/ZRZ6dK2142ZJz2gYjRUDg2WclZshFyf74iYSfoSWJFlxkv1JdQ6XuHcg9lyLI35LXj1z1pCjBndSkEUcznFX363/7cCh/4pffofqNiWuc90gae08d57Yck1z43sXuzr5QflWX5q//zr9+MZQ3veP9qi2RFrb4NMtx7szivY/ofRzVK+aeuRL2dezNbjBUCGyxGwwVgrLyxhNRH2YCcBYD6J+n+8XGpTAHwObhw+ahcbbzWOGcqLktUNbFHp6UaItzbq4gnYqag83D5lHOeZgabzBUCGyxGwwVgoVa7PfN3+Wi41KYA2Dz8GHz0Lhg81gQm91gMJQfpsYbDBWCsi52IrqTiHYT0T4iKhsbLRF9joh6iWi7+K7sVNhEtIyIHieinUS0g4g+tBBzIaJqInqGiF4szOOPCt+vJKLNhfvzpQJ/wUUHEQUFfsNvLdQ8iOggEW0joheIaEvhu4V4Ri4abXvZFjvN0JT+LYC3AFgH4OeJaF2ZTv95AHd63y0EFXYWwG8559YBeC2AXy1cg3LPZQrArc65DQA2AriTiF4L4M8AfNI5txrAGQD3XOR5zOJDmKEnn8VCzeMW59xG4epaiGfk4tG2O+fK8g/ADQAeFp8/CuCjZTx/D4Dt4vNuAF0FuQvA7nLNRczhQQC3L+RcANQCeB7A9ZgJ3kjOdb8u4vm7Cw/wrQC+hZlE+4WYx0EAi73vynpfACwC8AoKe2kXeh7lVOOXAjgiPh8tfLdQWFAqbCLqAfBqAJsXYi4F1fkFzBCFPgpgP4BB59xsZkq57s9fAfgdMIVb6wLNwwF4hIieI6J7C9+V+75cVNp226BDPBX2xQAR1QP4GoDfcM4NL8RcnHM559xGzLxZrwOw9mKf0wcRvR1Ar3PuuXk7X3zc5Jy7BjNm5q8S0RtkY5nuy3nRts+Hci72YwBkCZfuwncLhZKosC80iCiFmYX+Refc1xdyLgDgnBsE8Dhm1OUmIprNQS3H/bkRwDuJ6CCABzCjyn9qAeYB59yxwv+9AL6BmT+A5b4v50XbPh/KudifBbCmsNOaBvBeAA+V8fw+HsIMBTZwFlTY5wOaSWT+LIBdzrlPLNRciKiNiJoKcg1m9g12YWbR31WueTjnPuqc63bO9WDmefh359z7yz0PIqojooZZGcCbAWxHme+Lc+4kgCNEdEXhq1na9gszj4u98eFtNLwVwB7M2Ie/V8bz/j8AJwBkMPPX8x7M2IaPAdgL4HsAWsowj5swo4K9BOCFwr+3lnsuAK4GsLUwj+0APlb4fhWAZwDsA/AVAFVlvEc3A/jWQsyjcL4XC/92zD6bC/SMbASwpXBvvgmg+ULNwyLoDIYKgW3QGQwVAlvsBkOFwBa7wVAhsMVuMFQIbLEbDBUCW+wGQ4XAFrvBUCGwxW4wVAj+PykuiVqPmlE3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1c652f0240>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO19a5RcV3Xmt29VVz/UUrfUklqthy3ZliXLxrawMCbY4EAAgzOwssKQBDLjYTzxJItkkUUIj2Qmk+cKrJkJYbIyZDyBxMkkMYRHbAjDy9gMD2Nbtmyshy3Lej9brX6p1c+qe+ZHVd+z9751j263pGqR2t9avXpXnXPPPXXrnrp7n733t8k5B4PB8C8f0UJPwGAwNAa22A2GJoEtdoOhSWCL3WBoEthiNxiaBLbYDYYmwQUtdiK6i4heJKJ9RPSRizUpg8Fw8UHz9bMTUQHAXgBvAnAUwFMAfsE5t/viTc9gMFwsFC/g2FsB7HPO7QcAInoQwDsAZC72ts5u17ls1QWc8scDFBUS2Z09I9sq44lcgEQBlUQusTFAst907H+guxa1ibbhcksiT7YtS+QoLp933nOFC7zik1bTvyxBlD1LCjwQRYvop44JPlQzjlNzInkyNUa179mhfkycG637YS5ksa8BcIS9Pgrg1aEDOpetwk9/6C8BAFqj4Bc7d1v2/ZX68hxbILwppdlQVLefhpiTmkixrSuRZx79G9HWNrwjkbujWLQtdmOJvK7Dj0EtFdHv6LhfuG959bWi7cv9fYm8+4Z3+7HPDYp+4vYK3Ihx7OdIBWn1xc63pb4z9mUU2DVNX+/MU2cuJFIH8e8i9b2z4xxrikh+lkLgyy6yz0mx/M5idl859oPq4hk5SOy/Q30m53xfBz9GIZJzFD86sb7e1YfD5//sg3U+QRWXfIOOiO4jou1EtH1ybPhSn85gMGTgQp7sxwCsY6/X1t4TcM7dD+B+AFh+xeZcGwRBlYq1VWL5xBO/1oGnfljFZL/UKU3MvxGJX13ZMWJP9uLIcdE2VVycyD1b5VP50FM/SOTOVq/u7xguiX7P7no5kd96+ytE21VL/Ve6f8pfH/k8kp879DREoF/EH5XqSUkRaxNalb7iTEMifV5+HNO4tHIQeLIXmDYSseMip64If2Krz+8q7DpW5BOba3URe0JHTppNLqAF8c9ZYNctgv5e+KuUfoDz4UKe7E8B2EhEG4ioBODnATx8AeMZDIZLiHk/2Z1zZSL6VQBfR3Wv6TPOuV0XbWYGg+Gi4kLUeDjnvgrgqxdpLgaD4RLighb7hUDbLdwGjtWOZ5YNP98xhO1N2pJhNruygzLnoXZGXeTttbHF8piburyr7P33vVe0fXt1ZyLvPnjAN5zeI/r96i++M5FPtchzv3RoZyJ3Ob8nMNYr9wfiqclEjrRFz65JVGQuwNQOMPdISMhrnO1pceC2rJpGxvUW+wEASNjN6p7g+wXsY+r7w8V8HureYTvrBSgXJuvLx6zoPQE+nrrlhHeB7YPEIa+UujSz+1f6nuWwcFmDoUlgi91gaBI0VI13SKtPsxDBG6FoFoZCQcagcRUx7xi6G9ecUiOwzvzckzPSHdM6PZHI8fJ1om3w1JOJ/A9//SnRdu3GNf7FAT/+pqUqkGPkRCJv75dqZXebn8uKJVOJ/MOyjFws0OFEJiddmMKdxHVOfa24Wp9ym2Wo4Np9x8ZXVoIyBdgx6hYqMJ1Wu9RkcItvi0ip6sKckNeU+JjK3cs/Nx+T9Dy4pi5HgANX3dkcI3l/xzE3h5RbjmaXcvZ9b092g6FJYIvdYGgS2GI3GJoEDbXZCd5mC4Vo6ra89jw3GyOVRDBTKWe2ZUG7giJmOJbLfryWlhbRjyc2LF4kbeWTZ84lcnHXXtFWivzXMTHan8ivX7lE9FvPXHTjnV2i7buHj/oxKkN+Hh0HRb/ypP9wZbX3gUr90M5UoGuU7VLjBriIqg0kKKk8G0TsC3XseqfCZXmiCqRNXRBtfow4lajCbO/05gE/mTyM2/p8TqT3k7isXGoRT77y90ClotyU2RG9yacOJdfZk91gaBLYYjcYmgQLFkGnwVXrUD57EDlz4rkcB1Q2nU8cM/U2KmTPl6tfJeVq7O7xav2el/bL49CayK9a7zPdhk9NiH5n+kcSuazcRMsXdSTy0dFTiXzDsnHR73sTSxO5LZ4SbfL6sPlH2Sp4FPC8CTVefy/cvaa9sszNxQ0lIvmZC8RyxbXLqzKdyBWeb65OVchO4EtF1HEQ+3AUs3siZfRwl5psqbDrWKkwU1HRm4gxlc1TqZmAcSoi1MOe7AZDk8AWu8HQJLhsEmFCBAR855tHroXU+0pF7cq2+I8qd/f1kdwU0FFW9dVbvbvvmLpf6OiRw0/4traC3Ek/c+RkIi9f3Z3IA60dot93X/bqf+e03O0fif3n7F7CIvnKG0W/zlY/r7KmpxPbyl7UZApcrSe9c8wjy7hppCO/+HAqOq0Irp7PMFlFuCHObIvYDjw/s4vlPATXhoquE7wWKoAuZkvIsfugrD5LzK5HXFEqOHnzLS748Ujr+yyiLtb8dBleLnF4ZovBYPgXBVvsBkOTwBa7wdAkaLjNPmtTaJs9ZANnEU+k7JNQpBY7jp9Lu9cEbbe2xZkcBW0jNv4iGf1WitmegyKNWNGzPJGP7vGRcGtv3Sr6VboWJfLojLyON2+6JpFnmCvo+dOnRb9iq4/QKzN+eQCA8+4qvk+hKZw54UOkXEFF7lLjhq6TkWsl5m+LSNm5nNxRED1q91021TNx9yDfj1FfexwgLeG+MleQS6bCBiqz6cck+fwrbKlVIjkGuyVE1KCOwuP7Tpr6evaV2ewGg8EWu8HQLGi4Gj+rTofUjbm45bKO0yp4XOEVOUKlXrLnIe0EngAhE2Fa2KkXt0nO95lJH9XWo1T88XGvPvezyK+lE9Kd1Nfjjxs9LstL7Tvoeeo72pk6Hq8R/WLuykLApOJ87aofTzIplqV6XqD6CSgFHSbHo9pc9vji3JqAjSWP6DA8l8ELVy4rk4HzFyr1eYZbIereidm5y0yOlVlDTHXXpBQF1DdhBf8f1L0/d9p4e7IbDM0CW+wGQ5PAFrvB0CRYMNdb3veBbJJKTTjJQ2S1+0TYSYGKoOI4zcfA7EFBhqhCNMsFb1Of3vPPoq2jw4fBllqWirZlrd7GXt/tbf226THRDye82+ytr71DNP1w90E/x0k/xijJfYV4UXsi6+vY4vzriNchU2WfW1iqG6nw5Dhm+wXMpeZUP25iR9oUF/XR+D2gMxWZqMNx2Ry5m4y7JQFpp5eVX67Cro8juWQcy0xjCWuI1DUtRLpAN+sr7Hl2bp1lyCvS6mzNHDjvk52IPkNE/US0k723jIi+SUQv1f4vDY1hMBgWHnnU+L8GcJd67yMAHnHObQTwSO21wWC4jHFeNd459/+IaL16+x0A7qzJDwB4DMCH85wwqxxwCHnLP4WPqa+6h9x8qfI7IgWMZyfJM/WySLj9Jw6ItnLJq/EbVGmoO6/3HPPPPPYt37Bacs/3LfGK1PPfeUa0FVu9ujjdwlTp6W7Rr9TuCTDiVukCLDJ1vchcaEV1STmPW1lFxnFttCC4+FXkFzcFdEpZhrmV4ihk45fV+FwF5y60sia8K/qIt1iRRnByiaJSx7naXeD2hFLBZZmx+hlrVWSr6pzvTl/HYrFYZyyJ+W7Q9TrnZisVnATQO89xDAZDg3DBu/Gu+mjMfMQS0X1EtJ2Itk+ODV/o6QwGwzwx3934U0TU55w7QUR9APqzOjrn7gdwPwD0XLHZzarNOtklbyKMGjtzgmlVj40fUOPl+Drayx9XZFNcsnyD6PbQX3wika+Vm+CYYarZsjWdom1g3CertK7bnMgrOiZFv0FGM93ZLhMurlzsX085f/Kp8YOi39jUlb6NpJnAvQsxi+Sb1sQQ7BqUdIVXoYLHdd8HFD0yJDhBg4xckxeVz2paj19hO+lMpde75Y7viKvdeB4Rqe8rHqlZFKp7gCo9sMvOoddBxJ7N+phZgpfQ/TzfJ/vDAO6pyfcAeGie4xgMhgYhj+vtHwA8DmATER0lonsBfAzAm4joJQA/VXttMBguY+TZjf+FjKY3XuS5GAyGS4iGl3/KKr0UKsmURVgxJzeeiLLK5gHnSNtnzP3DXDfH9r8oDzzxeCKeWy5JJVtbvP+nUJJOjLYe3/fcfl/a2S2V5BLDg17uXiptz9MjPtpuSfviRL5+lbRzhxf7/YIXps+KtpJjPPLM9VZQWyf8zKnot4yoR6eUyQqPcFM+zJjbqOxsTrnGROXooo5U4xFpbAzlQhMVnlLEEIJ1U7bxvmwMHfXJA/visi77zEpOczlwf2tC1Ty1FSw23mBoEthiNxiaBAvGG6/VDq6WpNSoeZBXaGSZCekkm4A7j7l82ju9ivzcg/9T9Fu7zHPEldWcrlrXx15JVfLZ3S8n8m2v8O68tkk5x+Xd3h3WskS671oif+7pce+UkhQawOgUi66LFPEEC5Xj/OqKTh2FmBN4qBJYTPWtsMi4WOv7XLVWLq+Y6b48qs0pDjcwN1oq+o09z/icFP0fpDaeKhPrT6WILbj+z8uDaXIJft/qxKO8rmX+Wo+RtAS0eXuyGwxNAlvsBkOTwBa7wdAkWDDyilQtNm53ZbhtdFvItg/ZOyG7n5j7J0Upz9K34mk/xuLRnaIfrVifyENnBkVbe2l1Io+OnRNtrYyccu9JX6ftptXS9Vbo8Pb2ZFmWc17LasSdHfC2+KkJeU1HeRYZI80AIOJPRRSs/lpC/IecODHynytWPcs8Y00RT1Dk90gcs5Urqh+37V2qlgC3lRl5ZpS9ZxSl/GvsXNqlFrgfRb+Avc0RuvfF/a3Ck5MZB7zR9mQ3GJoEttgNhibBgvHGz4fEIu/YQL6IovpjcPeJ4mZjpX8GnvtOIserrxX92pi6v3rNCtF26IQvy9xzhWTz4rzmba1+/geOnBD9MOEj3JZ0S6fakdM+Q66bjTdUUKWjC95lV1Tli3mGmQhqK0oVk5M6aLcZv7XKrIy05nCLOS99SY4xw07Hz6xVcBGBpmbBI9IqrIyyDqIUKrLShbnKrO+qTPNTexg5L702BTifIesXck/r8mOz5BhW/slgMNhiNxiaBQ1V4x28ChNKfEkdl6Hyh4kn8pWQSu3ai4MUiUGrJ4Z48cmvJvLGVXK3PGahZotapCnQVfQ7zMuXySSZAwdZpdXBoUTs65Hq/gyN+/lOyN34s+d8ealC36pEPjkhSS6mW1mSCclrwFXrWNA5qyQWxyqTajVelCrypoZKARFRbZodmV9+vkMeqqAbqg4sEpnmUn6MexY0fxyTQ/d0zE0eXYWWmyG8Yqwuh8V38TM+56UgrzAYDD9msMVuMDQJbLEbDE2CxpNXzNon83SNCc73gO0TiqDj0DYY7xcrW7a1xdvm0ejRRJ7slsQQZ1jU3Og5aVO/8ab1iXx6aEC0XdmzMpH7z3gm3pL6nNMzPuKt0NYh2q5Y47PqJqb9cWMdm0U/V/Dln3QFZH5dY8cj15RrjLuTCrosEitf7HhGoyITZVFthVC2I2+IA/ssGmLvYO4lxgC5R5A7MjP1Wfh48hrwMbh7MPXB+PXQ+xv1Jq5gT3aDoUlgi91gaBIsQCJM9fclLw9c9Zj6alRKBeeRTirKSrhg5OiZ52pRrpRTe3zU3E3btiXy9NSU6Dcx5UsrjQ7JCqyFgp/H6Kg8rlL2xy1e5N1yPd3SteeYyry0TbrURobPJPKyxWsS+Uy0UvTjySNlyDHKTH+MWDJKRV0PQXmhVFNRFolFImqXl6hMGlKRRfkn0S2oPoeq8maOEYALRL8VAtFvCJgCAnH2fCN2T6ei8HIo8vZkNxiaBLbYDYYmgS12g6FJ0Pist5ohol0HIvwv509QKqxRuOWy+3L3T8q7wY8ptou28WPPJ3I3WhP5xBFZlnlln3d/TY+OirYzw56jfdX6NaKtMurJLAYmWdZYRdnDsQ8/PT4wJNquu8qTVwwc9K69oQ1yjBL5rLeKquHG9wSmmb2tXXQ8fFO7k7LKLaczvrys7VxRk4+4O1BnpWUTSOQNyw653sR4avxCVH8/IrU3kblnpO5jse+UPXftwjyPAxJAvvJP64joUSLaTUS7iOj9tfeXEdE3ieil2v+l5xvLYDAsHPL87JUB/IZzbguA2wC8j4i2APgIgEeccxsBPFJ7bTAYLlPkqfV2AsCJmnyWiPYAWAPgHQDurHV7AMBjAD583vFmBe2Z4K6V8w2ix5o9jql62rPHVU7uootUGaDpgncoLeqQLq/Jw55r7plB7yZb2ikv43e/80Qiv+l1N4i2E6c8EcWqRfK39vorPMHE0KSPpBoZk+WZWrnavWyJaOvp8iWlzvR5E2IqkvzysfOmAKlrwHnYuSaZKqXNrmNKPecZZuAmVEqJZaL60vS8MlAsZt/GWZyFeh5ijICJqbkTy+y74PdYiB9Rj5HpvlPX24nrmDnFTMxpg46I1gPYCuAJAL21HwIAOAmgN+Mwg8FwGSD3YieiTgBfAPDrzjmx6+SqP011f1uI6D4i2k5E2yfHhut1MRgMDUCuxU5ELagu9L9zzn2x9vYpIuqrtfcB6K93rHPufufcNufctrbO7npdDAZDA3Bem52qxsanAexxzv0Ja3oYwD0APlb7/1CuM9aMi7RrItuuE2SDAWJABEIqBXjYoRqjldVzczOS133NmqsSeXp8eyIXWyTp48ZrPMnk8ztlOefXve6WRF7UJm3U8qQPn21j85qakbXYKhE7rixDbl88cNjLK97l5xgr+6/gM+eigpx/qeSvQZllrOlsM2F7qvDkSPDvM1enpqNx3KZWc+T7AGyMYlGXWw7sHXBjNsP9Wn2dL7Mtb2nxFDLYaADp0hQ0/WlWTHauAIl/BvL42V8L4N8AeJ6Inq2991uoLvLPEdG9AA4BeFfG8QaD4TJAnt347yH7Z+ONF3c6BoPhUmHBSjaHPAUB7UWoQOVYRX4FCAW5eifUKB2Fxwga4impxn//qWcS+ZbrvPMhVi6iI8M+qm3pcum+K5V85N3g2IhoW9vr2+is3wM9PCBLSL3qhk2JfPD4ftE2U/Iq+YGi57OPIAk2WiKfjaczpmaY2SASytQWT8TUaR3VxsFVd+12CvqMBOGDl/NGu1WH4DpyBkmEQsrFmDNjTRynzdRUyChr4y41BNR9ce65k79YbLzB0CSwxW4wNAkazBvvEhVMRz3FQpVRfNk8GSNAdhCqtpkVwVRU3Gk8Kmz92rWi7YfwavfYhI9cOzeuSCjYTu+q1TLZZWLGj39uUKrxh1lppJWLvDq+fIU0BfadOJ7I7Z0ygq6rw6vrJwuL/fuxTMgB9zqo8qwFsXvO2oq6emp2ZdKs5JdU6SZkm15Z33VuVVoOL8zD1CwC0XXzQV4OxBSE1ZG/RFryua38k8FgsMVuMDQJbLEbDE2CBrveKLGHQu4TbXZwN0mUQRZQGz2RdfRRph2mbMgWFqz2wnNPirZX3uqj31oY3eLTT+8R/UqsjtqqXmlvP7vn5UTeslxGrnGSilKHb5uclL/Ji1v99ehQBBv90ZWJXGxjdu64yiBzfv76WsnoN+Z20vsgcbYrK8sGnkuNtSxO9tAYqTbBjsHFbELI0N5BCEE2+9Ccs94P2d8ZbcHA0UCbwWD4FwRb7AZDk6Dx5Z8yEglkYr6OYGL9XDbfWF5Vj88h0qVv+TzaF4u2J773eCLf8Vqv0o+OSBfa3Xff6ccryyi/cywyrvPajXJebCqr+rz6f+jYcdFv8RKv4g8fkMQWe69/fSKXKj5KjpdZAmROS0RKxWfauuCdm5PLq37kmlaJQyWKKcMtNxfu+SwuvJTSHUimCd5XWe7BFDlLNkKc+Jmj6BJYzko2GwyGGmyxGwxNAlvsBkOTYMGy3uYbksgzqApFbYeG7BXmQuIkiqmaXN7GLsXy8ly/0bu1JliE7OrVkn5v+QpPHPmDH/5ItC3r9PZ2x+Iu0XbmiM9ge3y3t7dfuUGydBe7vLtt+tALou30Ul+auXDO2/qV1P5GIPOPyXElOySWm+kpooWMTC49hrCPA/Pgr0IuurTdXx+hu28uYapZCJVUTruM2bkDJcnF9cn4nOZ6MxgMttgNhmbBwpFXBNStinJ9CPdMBpEFALiyj+JKeWBE9BSTNScaw1j/XvG6xMgaxqc8h9uGK/pEv8NHTyfyiRMnRdstN3tCiT0Hj4q2m/o8t/s4i+QbnZwU/Qoz/vXYtW8VbcVxT5xRDqjqcYC8ocCO4ypnKOqxoFx7lQzVWpNXcNNLu0Edd3llnlkipCILrTjgQku5EaPAPDJun7mYqXwI7lpOWxNsjllmmWW9GQwGW+wGQ5PgMlLjs7m34gzWgbLmM+PjKW0mZmFhRfYbR2oXOa54/bnnymtF2/avefV8xVq/I75cJbv88Dmv/vet7BFtra2eZ25g4Ixom17tyTJWr2hL5NFzx0S/tnP+uCOv+A+irRT7iLqY/Biktoe5+lwpK9WahfJxIouQGq9Nr7zIJMpAnWSV2X763hHqfkCNny8nRYCDLrTLPh8IlV6TuPBIuyA/XX3Yk91gaBLYYjcYmgS22A2GJkGDCSe93acjqXgWT8p2i7JtJjE+s78j0llYzA7lfAYp3ng/xtBRWbopZmMODfkilStulLb9oSPfTuS33HmraJue9C67Nctl7bu44O35sYkJP19Vnml8iS8DPVWQmXmVAnM/Tvn9h0hfb2YPllrl+Nw9xu30UMaahvRyZfg9FQpRqKxTwO2Uk049iwwjPUdNrMmj/OSYeTnlc9vzPBsxQLCROqx27gvKeiOiNiJ6koieI6JdRPR7tfc3ENETRLSPiD5LRKXzjWUwGBYOedT4KQBvcM7dBOBmAHcR0W0APg7gE865awAMAbj30k3TYDBcKPLUenMAZrMyWmp/DsAbALy79v4DAH4XwKdCY3HyipQLg3PBVZQqwlWbkCsoUClTRB/xEk+xrJBamPb9OlqmRRu1drBTMfVNlYlqbfMur/a2VtHW3+/dZls2XSnapsZ98stoxbvQFkVyjqMrvBo/rRQqN+WJNHiyi75WofJHImouYBpxN1co6lG4w+Jst5mGy1Bp0/UC6hNl6L55uet0Uo8LRK5ljaHnyE2jUHmp0PhiThnuxyBpS56BiahQq+DaD+CbAF4GMOycm00ROwpgTdbxBoNh4ZFrsTvnKs65mwGsBXArgM3nOSQBEd1HRNuJaPvk2PD5DzAYDJcEc3K9OeeGATwK4DUAuomSekVrARzLOOZ+59w259y2ts7uel0MBkMDcF6bnYhWAJhxzg0TUTuAN6G6OfcogHcCeBDAPQAemsuJQyGPGlHALcIh6sdps47ZigSeHSftsxnyGWXdretEW9fiRYl8/DgjgaxI237d2lWJPK1CUXltuYlpaYuXmC13qt9rQWs6pV3etdaXbD42LTPiRNhxgdmhah+EX6sUGYQgCMm+RYSrU7mJItEvX522WNnzUaou3OwgmUOkIO8rblOrczObOkUWGbLTM+Y4F9LKLFs/GBY8D277PH72PgAPEFEB1e/wc865rxDRbgAPEtEfAtgB4NM5xjIYDAuEPLvxPwKwtc77+1G13w0Gw48BGp71NqtuaDVEkBik1Cau2vh3Q6qM5gCDY1FhfBDlguLRdadP7RNt3PU0eHowkc+MyHLIBTb+mf5+0dbR6Qkq+oflhuXKFq/6dS3xkXFxW4vod2Laz1ln7XEzh6vFKdWRmQxlxW0v1MVA9KLksUMmdESkaAu6xrjamjNMTo+QqcZnR2kGOd4DKr0oTR24NzWBR95SWXlV/CxYbLzB0CSwxW4wNAkarsZnRfpwVU8rfRHqqzaaOpprejoKqsh3PBlBBVWmRL8o8sddvflG0bbriS8lcmXaR7jNqMvYwn5CpycnZFub31kfOiXLRvWu865JYlvCcUkmiIwUvFpPsTKHmIbII780EYIrh6ro1lcJtfopkpk0x13G7nBaNc2cRu5+XKUNRaeFKwfnTbbKjtALIZigwqMNc84jbWyYGm8wGGqwxW4wNAlssRsMTYIFs9m1/Rcq3cvtvwKzaQrK/uOZVykLhpMfgJ9bzYO56Cozsu3YPk9mcfXVVyfy6Ni46NdW4vNtE22cvMKVZQTdsuXLE/ngoUOJ3FGQWXUzizwxZcvZQdEmbPOcpYbDmWfZ/YSr6TxFieuNlx4zr8s1+0x57fI58brnvFYh5L3GoWNC9nyeadmT3WBoEthiNxiaBJdNBF2QHyyDxIA0zxyyI8uKxMgDmEpfVmpfzE42oCqk3vXmuxL59PEjiTw0qiLhVnge+eHBMdHW2e0rsp48dUS0ldq9yl+u+HlNdm6U/SYYN7xyvVFON05+Fw8/SL9k30XINSacqVo1lT3FK/ZSl5eS/ULj5+OeD0WnZbnG6vXNej8vcUaesattOtHm/M9te7IbDE0CW+wGQ5PAFrvB0CRYMJs9ZRex0MsUbzwPeeTEfch2PxS1zS5e+yyvUkHZS+zcK/quEU3ffvwLibx2eVciD49Iu/zsWf96ybIu2Tbm23pXrhBth474Es6ldp8dd65NElM6FkqrCR70HoQ/JmBDZnu85DEBzvQ0WwgXQ/b23MNNtY0rQ2TzZaVpzMem1gh9llSdhIx5hUJ/5fhzDzu2J7vB0CSwxW4wNAkarsbPqiZRUao1lZlyqs8seIkmYipPRanq/LhSQbYVWLSaKCvkZJRcC/l5DZ3cLdqefPqpRO6941V+bFW2aOSsz3TrWblSzpFx0C3qkNF1Z0e8C6+DzX9y7atFP8c47/KqyCmzKUA8ke0Ny85U1MiKVsubvVante54gKSip5T7rv41CJWumgtpxHzGmMtxedvywJ7sBkOTwBa7wdAkaKgaT0Qo1tTHiqJRLkRsKqokk/hFYqpMe2on3ZsChVgnuPgxYxbRFemySEVPLjE4dFa0vfLmmxL5mvXrE/mJnVLd71nmo+TaW6WqXmpnL5Rqx3fSz7L5t7R2in7EOOPyklAE1cPMFh3hNl/uNw5tTrB+SrXO2j1Pl39ixwRosUPJVqJNnQOF8B4AABrKSURBVC/0qcU43FwJmBPRPJ+xIY67PLAnu8HQJLDFbjA0CWyxGwxNgsba7M6hrWY76ywdx6LanPoJIpYBFhG3t5V15RgxhLLZSWTE8VI/OjvOX5LRgR2irbfLE0LGbM8hUhlZI0MDfrxN16nx/blHRiXhJMW+lNM4vK3fVZC88W6G7WkEyheHMtscD4dTpaEoYNvKU114ZJnL9vPJz8KbsspCAXAV5Y7NyAbTc+Lc+SHSSqc/M+fVD7gio9B3wcto5bz2GlmRqWIOeQerlW3eQURfqb3eQERPENE+IvoskSoUbjAYLivMRY1/P4A97PXHAXzCOXcNgCEA917MiRkMhouLXGo8Ea0FcDeAPwLwAarqNW8A8O5alwcA/C6ATwXHgUNUU9eV9gzHI9lUI09+4T9PseoX8Yg6J0sacf9JxOyEsk6qWOwTV/p37xRta/tWJ/IUPEHFkkXtol9fT18iz6iSoPGUj64bPHNGTrHgx+nefKef44x0RUK4YGRTlrqYcjVxXjhNSsFdWTF/fy5Oqfrj6QnHXHXXWjz3agWSXXiNgJRpwV7mNUmcMg9dxKL3VEJLzKv0svGLqvqt+C60+5FFYM43WWdW/Q/2yWyR+FMAH4Kv39ADYNi5ZEUdBbAm51gGg2EBcN7FTkQ/DaDfOff0fE5ARPcR0XYi2j4xNnz+AwwGwyVBHjX+tQDeTkRvA9AGYAmATwLoJqJi7em+FsCxegc75+4HcD8A9F6xKf/2osFguKjIU5/9owA+CgBEdCeADzrn3kNE/wjgnQAeBHAPgIfOOxZixOWJ2YFTbbOIIO2iYqF+GGJBJ/czd5vOROPHVZhZUyHp1hof87XfWnulZXLl6lWJPMzKLS9pkyGxy1im295DR0Vb9xI/rxlli1cYscWGq29L5FPjmvOdydoNlfVzGiCeCIV25uVoD3Gcczdr2OoPZKwxO9oF2C3nQmSaPUj2mKTdvQzaTufgjuAQOaRsm4Pb8xKXbP4wqpt1+1C14T99AWMZDIZLjDkF1TjnHgPwWE3eD+DWiz8lg8FwKdDgCDogqukzlIpSKjJZRXSBc3Qx1bGi1RxGSqH5w5kS45hZUHFSjW9jEW4nX94v2gbXeDV+aHQ0kbsXd4h+hw4fT+QzpydFW2dHTyK3t0v1f2TMl3Ja1NGayIVJ5YpEthuKF7zmqp7mqisyF9KUjjbMiMKLK9otlF26SZoJHqGMNa2KZlaGCqahBZpyut5CdQv0CTifPXcjzoWkQ9ABzofPH2lTrB4sNt5gaBLYYjcYmgQN56CbVcF01Bbngkv/Ann1scBU8FgN4lgpJD2+g1fXY8Yz54pSBR/Z+3gir+5cItomp/w8Sp0+0m5w8JTod27G9+tRO/q3bF6fyE++KI+79RZfxTUuss+yaLHoF41nlztyLBkoZmYOpYg+WJvmY8upSnJ6ZB35JSPG2Pua/puXkNL6OTc9ApFweZ9Y867iys8dqGTLoxf1ZwlHxtXfgZ/LHENegmR+uUczGAw/1rDFbjA0CWyxGwxNgoba7A6eVDFK1RLi9p86kNk0lTIrn6szygR5oXSpxSwqr8x+4zpXbRD9jnz/rxJ5ZZ/kfO9iPO9HTng32bnylOjXs9SXdWopyrZDh04k8g1bf0q07X3mnxN50xnfr31cRtrRqlck8sSIypwr+r2E2J1O5LKKFIxFZJn8zY8y+mnEymXHkXlYwAwNuY/4d50agifOhcgbQqQXIVcZz7TUpcmYXADfT5JjXAyiD1GOW7vvcoxtT3aDoUlgi91gaBI0XI2fqkXKaRU8Ym4zatEqOFM52ZSjWCa7zES8NJRyz7R4FTxmnG7jhyTn++f/3of4/+77fkm07TzoE/sqU97FNTkmVfXOXh/9tqavT7QdPOz56ch9S7QdPXwykZ/6zbcn8h3vlCRAux/9YiIv2XCLaFtz89sSeQJepS8pU4NfK+22EbVCQ9xpvB8k8kaCiUSVgDIqi87q9JlsdxVluOxCiTt6GmVmrgiyCkgSkLjInp36owh6+fmVcRJl0OaRP2pPdoOhSWCL3WBoEthiNxiaBA212WOKMF2YDU+VdksLC72sqOwqnqXG7bWoRdldjLCiJZIfLWrxdnR7l3ep/d/f/tei3x9/9IOJPHD8uGi7+up1iXztNVcn8r69+0S/1k4f3jo6eFq0vfCSd9m9/rorRNvOinex3fbv/yCRD+zdJfqt7PL7D273w6KtdO2WRC4svyqRZ/pfFP3K0VL2Srn2eKYb31sJuK5C9raAsjWji+GSCoyRZaeHku8qyqUo+ODV/gYVODFHdtYbH3/e5ZtF2K5scjmuvz3ZDYYmgS12g6FJ0FjyChBailW3ly6xw1WnYqt0vXE3ndAkW+UYbe0+w2zs8OOibeiAV2P7xp5I5Hf/8m2i3+H+oURe0bNMtK3u9VlwXYu8yXDjFhmF19ruM+nWrnmdaOvs9e62aVok2t7+ttcm8u6j3iW4eIlU0a6+8dWJvP3L0nW45tG/TeTSG72Jsv/p74p+y2/9+UQm6Eg4RhYifFfIRF5VWqvteY/jCJVnCmWXhTjzhPtOqcTFAlsmkXI/itJQbE453Y315pJ5nOD616XPzn+8PdkNhiaBLXaDoUnQWPIKokQP1+pWianknW09ou1sxavWLXu+k8gTp+Ru+b5Dvo7Fli2bRdvGbr+D3bPp5kS+ZcsrRL8jpw8n8tSEjDqbLnvVfXzG603nJqQaXJz0NNNTTs7xJ7Z6FfzbX/ueaLvrLX5eN9/2+kTes0NWkz0yxEySK+Xn3PHSM4nc+zU/37blK0S/1ha/Az+hbgMq++hAwQcYoqOe5w5zqEQVR6i6aYWVB7sYVNKpyq9sjFTqj1Dd675dHUJE+enh880xixCk3vnqwZ7sBkOTwBa7wdAksMVuMDQJGut6iyK0tFfdV52LpNtp58N/lsgdozJibHmPtzcLJR/5tXWzJGJc/6afTeTNm28Sbc8+78svDw56wocHv/Go6Hfm9LlEbm2XLsDpsreMRsfOJnJlWu4/zDA34sDQoGhbzPYq/tXdbxJtjz15KJFXr/LZceuv6RX91pb99Xj1rdJ1+PHf+mEi97K9iWVbbhD9jrb461gaGxJtlQq3xXkEHSTibBsyG4EST/PcE8hTyliPETLfnSbPDJFAcjs9lAbo5r6/kUIGv3zq3BnIW5/9IICzqO5PlJ1z24hoGYDPAlgP4CCAdznnhrLGMBgMC4u5qPE/6Zy72Tm3rfb6IwAecc5tBPBI7bXBYLhMcSFq/DsA3FmTH0C1BtyHQwdE5wbQuv1/AwBODLwk2q5a5N1tW14lS8itXe1VzrZO70IbGBgX/R55wkeT/ePXZTn5kVGvdi/t8CbEyYEB0W//wf5EXt5WEm0nBrz6v7zXl4I6Oy4VmjIbH8OSI27PaT+PX/mP94i2rzJSijVr/PU42j8i+l230RNivLz/BdH2k7f7a3f0mHfD7ZqSHPUtV92dyCtWSIKNkRHf1zGyhqK6XSZafFvLjIpcy9CT87rXar0z+2YdlyLYyDif05kkok3xzLGwTT1HMb4gyshmrwi5B0MIu+WyIwdnkffJ7gB8g4ieJqL7au/1OudmWRFPAuitf6jBYLgckPfJfrtz7hgRrQTwTSISjxPnnCNdjbGG2o/DfQCwuHNxvS4Gg6EByPVkd84dq/3vB/AlVEs1nyKiPgCo/e/POPZ+59w259y29vb2izNrg8EwZ5z3yU5EiwBEzrmzNfnNAH4fwMMA7gHwsdr/h84/FlAqVRWAN75mq2i7aeurEvnkwKho+/ojPmPr2PGJRD47dFT0a23z2Wbbtz8r2lasudIfN+x/lzauk2GkHez36MSwtOenqDuRX9x1IJEnpyVBRVvRD+IWLxdty7q9dvO3f/9Poq1U8jbZDLP5Hn9SuiJfeNkTXw4el2Wle9s6E/md73lnIj+7S+5hHNnrs+/OxreLtsIif03K5L+LaEbajAXHbh+aFm1ZbjNtnnIbOB1Gysbg7rA52LzSzs1wKULavHq0KKcLMO88co+huSsCpaPzII8a3wvgS7XJFgH8vXPua0T0FIDPEdG9AA4BeNecz24wGBqG8y5259x+ADfVef8MgDdeikkZDIaLj4ZG0HUtbsfdd1QjuXbsPCTa/uuffzaRd+/cKdqo4qPaNt7hCRle9ZZ/K/qddJ5c4pc+sEW0Pf03/zmRKyeeS+SoLNXPG1Z4frre3/yMaGvtXpvIbSXPaXf2RZmV9q3PfzKRD23/umg7PeHVxW98X5ors8QeAHDVlZ4QY3pmQvRra/HZbBtUpOAzj3pijnNxOZFv2HKt6De63WfOjR+WLsbhbj/mFVf540bHpdlUmmZlsBXnHy8NFcrqkiptNqGEIo6XcBn9oNXnzG5iUD3HcLnl+u6wICefOia/mZBdqtvIKwwGQwJb7AZDk8AWu8HQJKB5Z+DMAz09Pe6ut7wFAHBo717RtuyGOxN568/8qmgbGPa27fTUZCJru4jH9Rzf9QPR1nbom4k8ftYfV56Q4aarftbXeisslpl5UcXbwAVWL67Q2ir6tXV4F90KFUj0jf/2K4m8XZFAxuM+Q24C3oV24/VrRL+r16xP5JeOnBBtS4p+Ltte4/ctrtmwTvTbsNrP67uPS075vWW/N1FeeX0ir7tGZs7FLATXKbdchWfEiXssYJOmqXDq9kt97wFjXDaFnm18virrLcCmw212WS8um/M9PX8vx4Gw1zxMO//0sffi9KE9dTvak91gaBLYYjcYmgQNdb2Nj09jx/NVtfOnPvAXoi2e8Sry0VMyU0y6x5iGUpC/VVPnfL9Vg5LM8akDPhpuxVIf4XbbR54U/U6dOJjIlRmZVQfnXV7liiejjJl6DwDlibFEPnu2TbTd+Cv/I5Fvn5JklF/84M8k8tFx72770Y+kyXP0kFf3xxiJBgBs2+pV7Rde8O7NwWHpvovhXWq3v+p60UbPerV+x0Hv9hzq6BD9lvd6dX/GjYk2mvGElsR46ON0cee6IgA4ZgpI1Tdb3U+7yeofl1bp555hpyFG0Fp8gH8/mxxDn4ubDDq77+JlvRkMhh9z2GI3GJoEjY2gW70eb/ud6m736OmTos2RV5ERT6k2r74UWfmdqFWqlat3fSmRZ5ZJ7vkVHV5lvunn/jiRj5/cI8/FNPIo9FvImmKnqn7yclUT0hSYnvEq+YjKAnzzX/4okR/9fZ9qcGafTIQ5esYnwmxYLyPjTg56FX/JtDdrzslAQZSn/BuVW2S04atv2JjIQz/wBBgn9j0l+nUu8fOn1k7RVmTXZGaG8bqnsky8mNJEHVdb2dtz4qPL2NFPqcHz2HGHStBhJBeps4a49sS5ooyW1ICBtvqwJ7vB0CSwxW4wNAlssRsMTYKG2uzlchkDp8/UTqz9D9kZPQXm7phc2pXI6/dI8odiwdurrQVpQ954ty9RPAzvFio7eQkKguBA225suiIyS0VEMVmbodOx/yzRmHRXnS17e/7u33wgkXd9/g9Fv/jRLyfygYMvi7Y2RnZZ7vGkmCNnpettouz3EiKWRQcA4xt9tN0tm3z23a6DkrRy3498WewrX6mynYs+k66F7cfMTMvNA2HzKkKJYlTfZq3MwWbP4ooP2eWFgrwewXp07DiRvabmxV2CUdovh3y4sGhXe7IbDE0CW+wGQ5OgseWfABRR9W1pDrCIuy1UdBN1Lkvka/Z5cobK6edEv1KX5z8/Ni5dXpWVd/gXZe/aK8rgN+EbShEQBJIZcoOpgZPq8rdMehV3oOKJIra+57dFvxmWDDT19QdF2/CkNw1ODTAOUJKqabHg53+kJMtccdV6yY3XJPKW9atEv8rLPgln9/bviLbrXvMG/yLmvOszop9jUYn6isZMbY3ZdYu0qi7Gy1bPQ8hbOloji1sujtU8Ml+E5hQ6r3w9ayYEefnzndZgMPy4wxa7wdAksMVuMDQJGmqzc6Szh1jmUouc1vJJH+o6s/8LXm6XGWXlIe8aomt/WbRNsyw1bgyRdu8wk0fPscJJFPnM9WcJECzKbipkk41ajL2r7JiqR/f69/9JIg8dk+QbL+30obUDjAyjVFwi+g30e677WLl0xiZY7TR2fW7dskn0u2aND1ceOywz+I7u8Tz1azf5mgCuoG85xtdejjNasnLXaq8vSq20fN9Z3uMKUbYLMD0vXquOHyOvB9/Xmg/pjD3ZDYYmgS12g6FJ0FA13jmXuAh0lBL3JRRaukVT+5N/kMjjzGXUXZJj7Bj3kV9di2RGXFzxbi2uDulorEJUYMfIbDYRIcWrEaXUcR5JlZ1dlTJlWNcJ5q5qnZIloV9ilHE/8RuSBGTR//pPifzE9z3v3pgqqTzGXJOloXOirYtFIp7s93N8vlVy/W/Z4Av3brlCcu0dOOfHqIx4MpIla9eKfiOsxJZ2qRU5jx1X5JWKzKPTwhlrCIB/F3IM/j1pcoysjLhQ6ei0mZDFnR+asG6LMt5P9wiCiLqJ6PNE9AIR7SGi1xDRMiL6JhG9VPu/9PwjGQyGhUJeNf6TAL7mnNuMaimoPQA+AuAR59xGAI/UXhsMhssUeaq4dgF4HYB/BwDOuWkA00T0DgB31ro9AOAxAB8+z1iJ+q7VoWInm8qzsiDs/kEfdbW0wyfCHBnsFf1W3PFziTxdlokfYCoh05DTUVtsx11HQYkov8DOqKxUFCBT0LlAbJwWFslXrsgIN7R7tb48Ka/BHb/23/3827z589hXPy36TTt/3NiwrLa98+xIIi8b8Rx3o8NS3S+wnfUtm6R63jfu6b/7T3uCkOkO5WlZ6qMeh1TV3DhjxzkYJRbiiAtsYPOdbz3GfMo/hRAi2ODnSiVYsSjI9D0X1/5nnzfPk30DgNMA/oqIdhDRX9ZKN/c652bjJU+iWu3VYDBcpsiz2IsAXgngU865rQDOQansrvozU/d3k4juI6LtRLR9cmz4QudrMBjmiTyL/SiAo8652QyUz6O6+E8RUR8A1P731zvYOXe/c26bc25bW2d3vS4Gg6EByFOf/SQRHSGiTc65F1Gtyb679ncPgI/V/j8UGIaPBwCIFOf7ZD/LUjv1LdG29W2/lcgnv/yJRB6+aaPoV5r2NqVTtjJluE+KRXkJysxm19F1Wa6VublZeEf5Uo7Pr49MzaNJb7tNkPyNPeb8D+obftGX0Ro6uV/02/HMo36MVulIKbHYtVFmR7cW5d7Bc3v9mFSQJKHXrbnSn3vQ9zuxe1L0O7nyukReddV60TbFNUHmOkx9t+KlVjDrX39th3M7PRztpt5wdUVoM59HyUWRvI58n0geE4rCm3t2X14/+68B+DsiKgHYD+C9qGoFnyOiewEcAvCuwPEGg2GBkWuxO+eeBbCtTtMb67xnMBguQzQ2gg4+sUKrSm3dPuJt0V2Sc+3w9v+TyNFKrx629t4m+lWmmSkQUGuiIuNEU6WbuDqnI7pkpBY7VUrdZyaDisKTQ+bjBU8nfvAxpTnkJpjLq9Vz57/unv8i+o2PevX8xX3Pi7Zyi4+G45/53KR0vQ2e9mM8H8nr2N3pz71qmefF279bVozt7PDEJC3T0n03Q/6eiOFJOcIJLfJ1KKpNHpddGkpEQbrs71qUqwqUfwqVauL3X9r1ll3manb6IfeixcYbDE0CW+wGQ5PAFrvB0CRoLHmFc4kNq7PeuPthNG4VbR1TPnxzcvPPJjKVpQ3JXTJ5gxi1XSRtIU2KWd99l+IIZzZZIW28Zc4lO7tKuVnYtauoPYEiG2N82NvvPVdcJ/r9zAc+lsif+dB7RdvwuHePRYz/fXREZt+B7R20KFfq40/uSOSfeM2tifyKtV2i39DEwUQeOb5CtLWsuiqRiY2vswwD0azZRmyksi5DmW05M9EKhblnx4WmmO6Xnd2Xh1PenuwGQ5PAFrvB0CSg+XBZzftkRKdRDcBZDmDgPN0vNS6HOQA2Dw2bh8Rc53Glc25FvYaGLvbkpETbnXP1gnSaag42D5tHI+dharzB0CSwxW4wNAkWarHfv0Dn5bgc5gDYPDRsHhIXbR4LYrMbDIbGw9R4g6FJ0NDFTkR3EdGLRLSPiBrGRktEnyGifiLayd5rOBU2Ea0jokeJaDcR7SKi9y/EXIiojYieJKLnavP4vdr7G4joidr389kaf8ElBxEVavyGX1moeRDRQSJ6noieJaLttfcW4h65ZLTtDVvsVKXG/HMAbwWwBcAvENGWBp3+rwHcpd5bCCrsMoDfcM5tAXAbgPfVrkGj5zIF4A3OuZsA3AzgLiK6DcDHAXzCOXcNgCEA917ieczi/ajSk89ioebxk865m5mrayHukUtH2+6ca8gfgNcA+Dp7/VEAH23g+dcD2Mlevwigryb3AXixUXNhc3gIwJsWci4AOgA8A+DVqAZvFOt9X5fw/GtrN/AbAHwF1eDzhZjHQQDL1XsN/V4AdAE4gNpe2sWeRyPV+DUAjrDXR2vvLRQWlAqbiNYD2ArgiYWYS011fhZVotBvAngZwLBzbpaFolHfz58C+BB80daeBZqHA/ANInqaiO6rvdfo7+WS0rbbBh3CVNiXAkTUCeALAH7dOTfK2xo1F+dcxTl3M6pP1lsBbL7U59Qgop8G0O+ce/q8nS89bnfOvRJVM/N9RPQ63tig7+WCaNvPh0Yu9mMA1rHXa2vvLRRyUWFfbBBRC6oL/e+cc19cyLkAgHNuGMCjqKrL3UQ0m/bciO/ntQDeTkQHATyIqir/yQWYB5xzx2r/+wF8CdUfwEZ/LxdE234+NHKxPwVgY22ntQTg5wE83MDzazyMKgU2MAcq7AsBVROUPw1gj3PuTxZqLkS0goi6a3I7qvsGe1Bd9O9s1Dyccx91zq11zq1H9X74tnPuPY2eBxEtIqLFszKANwPYiQZ/L865kwCOENGm2luztO0XZx6XeuNDbTS8DcBeVO3D327gef8BwAkAM6j+et6Lqm34CICXAHwLwLIGzON2VFWwHwF4tvb3tkbPBcCNAHbU5rETwO/U3r8KwJMA9gH4RwCtDfyO7gTwlYWYR+18z9X+ds3emwt0j9wMYHvtu/knAEsv1jwsgs5gaBLYBp3B0CSwxW4wNAlssRsMTQJb7AZDk8AWu8HQJLDFbjA0CWyxGwxNAlvsBkOT4P8DCO4e4Gn1RL8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_B[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.layers[1](x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_formatter = \"{:.3f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08631023\n"
     ]
    }
   ],
   "source": [
    "print(np.max(score[0, 0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.060 0.062 0.059 0.062 0.061 0.064 0.068 0.069 0.060 0.058 0.058 0.058\n",
      "  0.057 0.056 0.056 0.046]\n",
      " [0.058 0.066 0.063 0.065 0.064 0.064 0.071 0.070 0.064 0.062 0.062 0.061\n",
      "  0.061 0.060 0.061 0.052]\n",
      " [0.056 0.063 0.061 0.065 0.063 0.061 0.067 0.067 0.062 0.060 0.060 0.059\n",
      "  0.059 0.058 0.060 0.051]\n",
      " [0.056 0.063 0.062 0.065 0.061 0.057 0.066 0.069 0.063 0.060 0.059 0.059\n",
      "  0.059 0.060 0.061 0.051]\n",
      " [0.055 0.063 0.063 0.063 0.054 0.057 0.066 0.070 0.062 0.059 0.059 0.060\n",
      "  0.060 0.060 0.061 0.051]\n",
      " [0.056 0.063 0.063 0.062 0.055 0.060 0.070 0.070 0.061 0.060 0.060 0.060\n",
      "  0.060 0.060 0.061 0.051]\n",
      " [0.056 0.063 0.064 0.063 0.052 0.059 0.070 0.068 0.060 0.060 0.060 0.060\n",
      "  0.060 0.060 0.061 0.051]\n",
      " [0.056 0.063 0.062 0.060 0.055 0.060 0.068 0.063 0.060 0.060 0.060 0.060\n",
      "  0.060 0.060 0.061 0.051]\n",
      " [0.056 0.064 0.062 0.060 0.053 0.062 0.065 0.061 0.060 0.060 0.060 0.060\n",
      "  0.060 0.060 0.061 0.051]\n",
      " [0.056 0.064 0.064 0.061 0.056 0.064 0.066 0.061 0.060 0.060 0.060 0.060\n",
      "  0.060 0.060 0.062 0.051]\n",
      " [0.057 0.068 0.070 0.067 0.066 0.073 0.074 0.066 0.061 0.060 0.060 0.060\n",
      "  0.060 0.060 0.062 0.051]\n",
      " [0.065 0.079 0.079 0.076 0.071 0.076 0.077 0.072 0.064 0.060 0.060 0.060\n",
      "  0.060 0.060 0.062 0.052]\n",
      " [0.071 0.086 0.076 0.079 0.078 0.078 0.072 0.074 0.067 0.063 0.060 0.060\n",
      "  0.060 0.060 0.062 0.052]\n",
      " [0.069 0.077 0.077 0.077 0.065 0.065 0.066 0.069 0.071 0.067 0.062 0.060\n",
      "  0.060 0.060 0.062 0.052]\n",
      " [0.066 0.079 0.075 0.070 0.075 0.071 0.059 0.065 0.066 0.072 0.067 0.066\n",
      "  0.065 0.065 0.067 0.055]\n",
      " [0.055 0.065 0.061 0.057 0.058 0.064 0.060 0.047 0.048 0.058 0.059 0.057\n",
      "  0.056 0.056 0.058 0.051]], shape=(16, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(score[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2](score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG16(weights='imagenet', input_shape=(64, 64, 3), include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=vgg.layers[0].input, outputs=vgg.layers[9].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
